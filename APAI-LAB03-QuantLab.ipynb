{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "781ec3b1",
   "metadata": {},
   "source": [
    "# Quantised neural networks\n",
    "\n",
    "This notebook is a companion to the material presented during the lecture held on Thursday, 28th October 2021, in the scope of the \"Architecture and Platforms for Artificial Intelligence (APAI)\" course offered by the University of Bologna.\n",
    "\n",
    "The notebook is structured into three parts:\n",
    "* [**introduction to PyTorch**](#sec:pytorch); this section is a brief tutorial on using the PyTorch deep learning framework to describe and train DNNs;\n",
    "* [**training QNNs with the PACT algorithm**](#sec:float2fake); this section shows how to train mixed-precision QNNs using the `quantlib` Python package, an add-on for PyTorch developed to support training and manipulation of QNNs;\n",
    "* [**exporting trained QNNs to integerised ONNX IRs**](#sec:fake2true); this section shows how to use `quantlib` to convert a trained PyTorch QNN into an ONNX intermediate representation that can be further optimised before being deployed to various hardware platforms, including embedded and edge devices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d2a822",
   "metadata": {},
   "source": [
    "To facilitate the understanding of the code, it is useful to annotate functions with the types of their inputs and outputs.\n",
    "Therefore, in this notebook we will extensively use the `typing` Python package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e48db6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "from typing import Union, Tuple, List, Dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5979aab3",
   "metadata": {},
   "source": [
    "<a id='sec:pytorch'></a>\n",
    "## Introduction to PyTorch\n",
    "\n",
    "In this section, we will describe and train a simple deep convolutional neural network using PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2cbb37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "_CONFIGS = {\n",
    "    'VGG8':  ['M', 256, 256, 'M', 512, 512, 'M'],\n",
    "    'VGG9':  [128, 'M', 256, 256, 'M', 512, 512, 'M'],\n",
    "    'VGG11': [128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "\n",
    "class VGG(nn.Module):\n",
    "\n",
    "    def __init__(self, config: str) -> None:\n",
    "\n",
    "        super(VGG, self).__init__()\n",
    "\n",
    "        self.pilot      = self._make_pilot(config)\n",
    "        self.features   = self._make_features(config)\n",
    "        self.avgpool    = self._make_avgpool(config)\n",
    "        self.classifier = self._make_classifier(config)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_pilot(config: str) -> nn.Sequential:\n",
    "\n",
    "        out_channels = 128\n",
    "        modules = []\n",
    "\n",
    "        modules += [nn.Conv2d(3, out_channels, kernel_size=3, padding=1, bias=False)]\n",
    "        modules += [nn.BatchNorm2d(out_channels)]\n",
    "        modules += [nn.ReLU(inplace=True)]\n",
    "\n",
    "        return nn.Sequential(*modules)\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_features(config: str) -> nn.Sequential:\n",
    "\n",
    "        in_channels = 128\n",
    "        modules = []\n",
    "\n",
    "        for v in _CONFIGS[config]:\n",
    "            if v == 'M':\n",
    "                modules += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                out_channels = v\n",
    "                modules += [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)]\n",
    "                modules += [nn.BatchNorm2d(out_channels)]\n",
    "                modules += [nn.ReLU(inplace=True)]\n",
    "                in_channels = out_channels\n",
    "\n",
    "        return nn.Sequential(*modules)\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_avgpool(config: str):\n",
    "        return nn.AdaptiveAvgPool2d((4, 4))\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_classifier(config: str) -> nn.Sequential:\n",
    "\n",
    "        modules = []\n",
    "\n",
    "        modules += [nn.Linear(512 * 4 * 4, 1024, bias=False)]\n",
    "        modules += [nn.BatchNorm1d(1024)]\n",
    "        modules += [nn.ReLU(inplace=True)]\n",
    "        modules += [nn.Linear(1024, 1024, bias=False)]\n",
    "        modules += [nn.BatchNorm1d(1024)]\n",
    "        modules += [nn.ReLU(inplace=True)]\n",
    "        modules += [nn.Linear(1024, 10)]\n",
    "\n",
    "        return nn.Sequential(*modules)\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "\n",
    "        for m in self.modules():\n",
    "\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        x = self.pilot(x)\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)  # https://stackoverflow.com/questions/57234095/what-is-the-difference-of-flatten-and-view-1-in-pytorch\n",
    "\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca1b935a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (pilot): Sequential(\n",
      "    (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (12): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(4, 4))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (4): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking arugment for argument weight in method wrapper_thnn_conv2d_forward)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5e6c731829ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# REMEMBER: before evaluating a network, freeze the batch-normalisation and dropout parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdummy_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdummy_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"`dummy_x` shape: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/quantlab/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-0eebd7ae6711>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpilot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/quantlab/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/quantlab/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/quantlab/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/quantlab/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/quantlab/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    437\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 439\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    440\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking arugment for argument weight in method wrapper_thnn_conv2d_forward)"
     ]
    }
   ],
   "source": [
    "device = torch.device(torch.cuda.current_device() if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# create the network\n",
    "network = VGG('VGG9')\n",
    "network = network.to(device=device)  # REMEMBER: place the parameters of the 'Module' on the device that guarantees the best performance\n",
    "print(network)\n",
    "print()\n",
    "\n",
    "# verify that it can process tensor data\n",
    "network.eval()  # REMEMBER: before evaluating a network, freeze the batch-normalisation and dropout parameters\n",
    "dummy_x = torch.randn(1, 3, 32, 32)\n",
    "dummy_y = network(dummy_x)\n",
    "\n",
    "print(\"`dummy_x` shape: {}\".format(dummy_x.shape))\n",
    "print(\"`dummy_y` shape: {}\".format(dummy_y.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd24d0f",
   "metadata": {},
   "source": [
    "### The CIFAR-10 data set\n",
    "\n",
    "The ten-classes Canadian institute for advanced research (CIFAR-10) data set contains $60000$ images depicting objects belonging to ten different classes.\n",
    "Each image has a resolution of $32 \\times 32$ pixels, and is encoded using the RGB color model.\n",
    "\n",
    "Each class is represented by exactly $60000 / 10 = 6000$ images, $5000$ of which belong to the training set and $1000$ of which belong to the validation set.\n",
    "Overall, the training set contains $50000$ data points, whereas the validation set contains $10000$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58c5937f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "def create_cifar10_dir_data() -> os.PathLike:\n",
    "    \"\"\"Create a directory where `torchvision` should store CIFAR-10 data.\"\"\"\n",
    "    dir_data = os.path.join(os.curdir, 'data_cifar10')\n",
    "    if not os.path.isdir(dir_data):\n",
    "        os.makedirs(dir_data, exist_ok=True)\n",
    "\n",
    "    return dir_data\n",
    "    \n",
    "\n",
    "def load_cifar10_data_set(dir_data: os.PathLike, train: bool) -> torch.utils.data.Dataset:\n",
    "    \n",
    "    # define the pre-processing transform that will be applied to each data point\n",
    "    transform_list = []\n",
    "    if train:\n",
    "        transform_list += [transforms.RandomHorizontalFlip()]\n",
    "        transform_list += [transforms.ToTensor()]\n",
    "    else:\n",
    "        transform_list += [transforms.ToTensor()]\n",
    "    transform = transforms.Compose(transform_list)\n",
    "    \n",
    "    # load the data into `Dataset` objects (PyTorch exposes specialised loader objects for CIFAR-10: https://pytorch.org/vision/stable/datasets.html#torchvision.datasets.CIFAR10)\n",
    "    data_set = torchvision.datasets.CIFAR10(root=dir_data, train=train, download=True, transform=transform)\n",
    "\n",
    "    return data_set\n",
    "\n",
    "\n",
    "def load_cifar10_int2label(dir_data: os.PathLike) -> Dict[int, str]:\n",
    "\n",
    "    # see https://www.cs.toronto.edu/~kriz/cifar.html to understand how the CIFAR-10 data is organised\n",
    "    labels_file = os.path.join(dir_data, 'cifar-10-batches-py', 'batches.meta')\n",
    "    with open(labels_file, 'rb') as fp:\n",
    "        labels = {i: name.capitalize() for i, name in enumerate(pickle.load(fp)['label_names'])}\n",
    "    \n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17aaf26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Training data set contains 50000 data points.\n",
      "Validation data set contains 10000 data points.\n",
      "\n",
      "CIFAR-10 classes: \n",
      "0: Airplane\n",
      "1: Automobile\n",
      "2: Bird\n",
      "3: Cat\n",
      "4: Deer\n",
      "5: Dog\n",
      "6: Frog\n",
      "7: Horse\n",
      "8: Ship\n",
      "9: Truck\n"
     ]
    }
   ],
   "source": [
    "dir_data   = create_cifar10_dir_data()\n",
    "train_data = load_cifar10_data_set(dir_data, train=True)\n",
    "valid_data = load_cifar10_data_set(dir_data, train=False)\n",
    "int2label  = load_cifar10_int2label(dir_data)\n",
    "\n",
    "print()\n",
    "print(\"Training data set contains {} data points.\".format(len(train_data)))\n",
    "print(\"Validation data set contains {} data points.\".format(len(valid_data)))\n",
    "print()\n",
    "print(\"CIFAR-10 classes: \")\n",
    "for i, name in int2label.items():\n",
    "    print(\"{}: {}\".format(i, name))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15ec0a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def show_random_cifar10_image(data_set: torch.utils.data.Dataset, int2label: Dict[int, str]) -> None:\n",
    "    x, y = data_set.__getitem__(random.randrange(0, len(data_set)))  # https://docs.python.org/3/library/random.html#random.randrange\n",
    "    plt.imshow(x.permute(1, 2, 0))\n",
    "    plt.title(int2label[y])  # https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.title.html\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a130ed22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# REPLAY: this cell should show different CIFAR-10 data points\n",
    "show_random_cifar10_image(valid_data, int2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfec5fd",
   "metadata": {},
   "source": [
    "The cost of training DNNs on real hardware can be optimised by means of mini-batching.\n",
    "Indeed, since each data point is processed independently of the others we can rewrite the collection of vector-matrix products\n",
    "$$(\\mathbf{x}^{(i-1, 1)} W^{(i)}, \\dots, \\mathbf{x}^{(i, K)} W^{(i)})$$\n",
    "as a single matrix-matrix product\n",
    "$$X^{(i-1)} W^{(i)} \\,,$$\n",
    "where $X^{(i-1)}_{(k)} := \\mathbf{x}^{(i-1, k)}, k = 1, \\dots, K$.\n",
    "This rewriting allows minimising the number of memory transactions on host-device computing systems (e.g., those using GPGPUs) and maximising the utilisation of data movement components, as well as minimising the number of idle processing elements on parallel hardware (e.g., the CUDA cores of NVidia GPGPUs).\n",
    "\n",
    "The PyTorch mechanism to create mini-batches of data points consists of two components:\n",
    "* a `Sampler`, which implements the policy to split the complete data set into mini-batches; for instance, at training time it is important that the distribution of data inside a mini-batch is as heterogeneous as possible, to avoid overfitting the model's parameters to just a few classes;\n",
    "* a `DataLoader`, which queries the `Sampler` for the indices of the data points that will compose the next batch, reads the associated data points from the `Dataset` (which applies the `transforms` set at its creation), and composes them into batches ready to be fed to the PyTorch DNN that we want to train.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44c2b774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(data_set: torch.utils.data.Dataset, train: bool, batch_size: int) -> torch.utils.data.DataLoader:\n",
    "    \n",
    "    if train:\n",
    "        sampler = torch.utils.data.RandomSampler(data_set)\n",
    "    else:\n",
    "        sampler = torch.utils.data.SequentialSampler(data_set)\n",
    "        \n",
    "    loader = torch.utils.data.DataLoader(data_set, batch_size=batch_size, sampler=sampler)\n",
    "    \n",
    "    return loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a516b252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input `torch.Tensor`: torch.Size([64, 3, 32, 32])\n",
      "Shape of label array: 64\n"
     ]
    }
   ],
   "source": [
    "bs = 64\n",
    "train_loader = create_data_loader(train_data, True,  bs)\n",
    "valid_loader = create_data_loader(valid_data, False, bs)\n",
    "\n",
    "\n",
    "bx, by = next(iter(train_loader))\n",
    "print(\"Shape of input `torch.Tensor`: {}\".format(bx.shape))\n",
    "print(\"Shape of label array: {}\".format(len(by)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cfefce",
   "metadata": {},
   "source": [
    "### Training DNNs\n",
    "\n",
    "\n",
    "#### DNN training as an optimisation problem.\n",
    "\n",
    "In principle, the optimal DNN $f^{*}$ minimises the loss functional with respect to the true data distribution $\\mu$:\n",
    "$$f^{*} := \\underset{f \\in \\mathcal{F}}{\\arg\\min} \\int_{X \\times Y} \\ell(f(x), y) \\, d\\mu(x, y) \\,.$$\n",
    "Here, $\\ell : Y \\times Y \\to \\mathbb{R}^{+}_{0}$ is the loss function, a function which should satisfy $\\ell(f(x), y) = 0 \\iff f(x) = y$.\n",
    "\n",
    "Since the DNN $f$ is parametrised by $\\theta \\in \\Theta$, we can rewrite this functional optimisation problem as the minimisation of an integral equation that depends on $\\theta$:\n",
    "$$\\theta^{*} := \\underset{\\theta \\in \\Theta}{\\arg\\min} \\int_{X \\times Y} \\ell(f(\\theta, x), y) \\, d\\mu(x, y) \\,.$$\n",
    "\n",
    "One of the standard strategies to minimise differentiable functions is using gradient descent, where the gradient\n",
    "$$g^{t} := \\nabla_{\\theta} \\left( \\int_{X \\times Y} \\ell(f(\\theta^{t}, x), y) \\, d\\mu(x, y) \\right)$$\n",
    "plays the role of the learning signal.\n",
    "<!--- Under certain hypothesis, exchangeability theorems (e.g., Fubini's theorem) can be applied to bring the gradient under the integral sign:\n",
    "$$g^{t} = \\eta \\int_{X \\times Y} \\left( \\nabla_{\\theta} \\ell(f(\\theta^{t}, x), y) \\right) \\, d\\mu(x, y) \\,.$$ --->\n",
    "\n",
    "Apart from technicalities, the most important issue of this formalism lies in the fact that the true data distribution $\\mu$ is unknown.\n",
    "In practical cases, the true measure is approximated by the empirical measure associated with an available, finite data set $\\mathcal{D} = ((x^{(1)}, y^{(1)}), \\dots, (x^{(N)}, y^{(N)}))$:\n",
    "$$\\mu \\approx m := \\frac{1}{N} \\sum_{n=1}^{N} \\delta_{(x^{(n)}, y^{(n)})} \\,.$$\n",
    "\n",
    "When plugged into the loss functional, this empirical measure yields the following learning signal:\n",
    "\\begin{align*}\n",
    "    g^{t}\n",
    "    &= \\nabla_{\\theta} \\left( \\frac{1}{N} \\sum_{n=1}^{N} \\ell(f(\\theta^{t}, x^{(n)}), y^{(n)}) \\right) \\\\\n",
    "    &= \\frac{1}{N} \\sum_{n=1}^{N} \\left( \\nabla_{\\theta} \\ell(f(\\theta^{t}, x^{(n)}), y^{(n)}) \\right) \\,.\n",
    "\\end{align*}\n",
    "\n",
    "Note that when the data set $\\mathcal{D}$ is fixed, the value $g^{t}$ of the gradient computed with respect to a specific value $\\theta^{t}$ of the variable is deterministic; however, the values of $g$ computed for different values of $\\theta$ can still differ.\n",
    "\n",
    "\n",
    "#### Learning rules for DNNs\n",
    "\n",
    "Gradient descent optimisation algorithms are usually iterative, meaning that the target variable is optimised from an initial condition $\\theta^{0}$ to a final condition $\\theta^{T}$ with the hope that $\\theta^{T} \\approx \\theta^{*}$.\n",
    "In the scope of machine learning, the update rule that describes how to go from $\\theta^{t}$ to $\\theta^{t+1}$ (i.e., the one-step evolution of the system), is called the learning rule.\n",
    "\n",
    "Several gradient-based learning rules have been proposed to train DNNs.\n",
    "The simplest one is stochastic gradient descent (SGD).\n",
    "In this case, the idea is to approximate $\\mu$ with a different empirical distribution at each iteration:\n",
    "$$\\mu \\approx m^{t} := \\frac{1}{B} \\sum_{b=1}^{B} \\delta_{(x^{(t, b)}, y^{(t, b)})} \\,;$$\n",
    "the collection $((x^{(t, 1)}, y^{(t, 1)}), \\dots, (x^{(t, B)}, y^{(t, B)}))$ is called the $t$-th mini-batch, and it is sampled from the complete data set $\\mathcal{D}$ ($1 \\leq B \\ll N$).\n",
    "Due to this mini-batch sampling, $g$ can take on different values even when computed for the same value of the variable $\\theta$; in other words, the dependency of $g^{t}$ on $\\theta^{t}$ is stochastic, not deterministic, hence the method's name.\n",
    "The update rule then is as simple as\n",
    "$$\\theta^{t+1} = \\theta^{t} - \\eta g^{t} \\,;$$\n",
    "accordingly, the update is $\\Delta \\theta^{t} := \\theta^{t+1} - \\theta^{t} = - \\eta g^{t}$.\n",
    "\n",
    "Note that gradients and updates are two different concepts: updates are computed using gradients, but gradients do not define updates by themselves.\n",
    "\n",
    "\n",
    "#### Optimisation in Pytorch\n",
    "\n",
    "PyTorch implements loss functions as specific `Module`s.\n",
    "Some loss `Module`s automatically compute the average value of the loss function with respect to all the data points in a mini-batch; others do not do it automatically.\n",
    "\n",
    "It is possible to compute gradients using reverse-mode automatic differentiation (also known as back-propagation) with a simple call to the `backward` method of the `Tensor` holding the aggregated loss value.\n",
    "The gradients of the parameter and feature `Tensor`s are stored in specific `grad` attributes attached to the corresponding `Tensor` objects.\n",
    "\n",
    "The `torch.optim` module implements `Optimizer` objects associated with the most popular SGD variants (vanilla SGD, RMSProp, Adam, ...).\n",
    "Remember: `Optimizer`s do not compute gradients; `Optimizer`s have a `step` method that uses gradients $g^{t}$ to compute updates $\\Delta \\theta^{t}$ and apply them to the parameter `Tensor`s.\n",
    "\n",
    "Multiple calls to `backward` on the `Tensor` holding the loss value accumulate gradients in the `grad` attributes of parameter and feature `Tensor`s.\n",
    "Therefore, it is important to remember to zero-out the gradients stored in `grad` attributes after every call to the `step` method of the chosen `Optimizer`, otherwise the \"gradient\" used by the `Optimizer` will not correspond to the correct gradient that it is supposed to use.\n",
    "\n",
    "<!--- Sometimes, forgetting to reset the gradients to zero might prevent the network from learning: indeed, if we suppose that gradients follow a multi-dimensional normal distribution, their sum will eventually zero out, leading to vanishing gradients in the span of a few training iterations. --->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "084edbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "loss_fn   = nn.CrossEntropyLoss()\n",
    "optimiser = optim.Adam(network.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6989b3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(device:       torch.device,\n",
    "        train_loader: torch.utils.data.DataLoader,\n",
    "        valid_loader: torch.utils.data.DataLoader,\n",
    "        network:      nn.Module,\n",
    "        loss_fn:      nn.Module,\n",
    "        optimiser:    torch.optim.Optimizer,\n",
    "        n_epochs:     int) -> None:\n",
    "\n",
    "    for i_epoch in range(0, n_epochs):\n",
    "    \n",
    "        # training pass\n",
    "        network.train()  # REMEMBER: before training a network, release the batch-normalisation and dropout parameters\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        for i_batch, (x, y_gt_int) in enumerate(train_loader):\n",
    "\n",
    "            x        = x.to(device=device)\n",
    "            y_gt_int = y_gt_int.to(device=device)\n",
    "\n",
    "            y_pr       = network(x)\n",
    "            loss_value = loss_fn(y_pr, y_gt_int)\n",
    "\n",
    "            loss_value.backward()  # back-propagation using torch's `autograd`\n",
    "            optimiser.step()\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "            print(\"Epoch[{:02d}/{:02d}] | Iteration[{:04d}/{:04d}] - Loss value: {}\".format(i_epoch, n_epochs, i_batch, len(train_loader), loss_value.item()))\n",
    "\n",
    "        # validation pass\n",
    "        network.eval()  # REMEMBER: before evaluating a network, freeze the batch-normalisation and dropout parameters\n",
    "        correct = 0\n",
    "\n",
    "        for x, y_gt_int in valid_loader:\n",
    "            \n",
    "            x        = x.to(device=device)\n",
    "            y_gt_int = y_gt_int.to(device=device)\n",
    "\n",
    "            y_pr     = network(x)\n",
    "            y_pr_int = y_pr.argmax(axis=1)  # the position of the neuron with the highest score encodes the predicted class\n",
    "\n",
    "            correct += torch.sum(y_pr_int == y_gt_int).item()\n",
    "\n",
    "        print(\"Epoch[{:02d}/{:02d}] - Validation accuracy: {:6.2f}%\".format(i_epoch, n_epochs, 100.0 * correct / len(valid_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b4c6e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spmatteo/anaconda3/envs/quantlab/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448234945/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[00/01] | Iteration[0000/0782] - Loss value: 2.338247299194336\n",
      "Epoch[00/01] | Iteration[0001/0782] - Loss value: 2.1103904247283936\n",
      "Epoch[00/01] | Iteration[0002/0782] - Loss value: 2.101158380508423\n",
      "Epoch[00/01] | Iteration[0003/0782] - Loss value: 2.1150431632995605\n",
      "Epoch[00/01] | Iteration[0004/0782] - Loss value: 1.7555603981018066\n",
      "Epoch[00/01] | Iteration[0005/0782] - Loss value: 2.040335178375244\n",
      "Epoch[00/01] | Iteration[0006/0782] - Loss value: 1.9222793579101562\n",
      "Epoch[00/01] | Iteration[0007/0782] - Loss value: 1.7212440967559814\n",
      "Epoch[00/01] | Iteration[0008/0782] - Loss value: 1.8490089178085327\n",
      "Epoch[00/01] | Iteration[0009/0782] - Loss value: 2.0042903423309326\n",
      "Epoch[00/01] | Iteration[0010/0782] - Loss value: 1.8731038570404053\n",
      "Epoch[00/01] | Iteration[0011/0782] - Loss value: 1.6842401027679443\n",
      "Epoch[00/01] | Iteration[0012/0782] - Loss value: 1.8215972185134888\n",
      "Epoch[00/01] | Iteration[0013/0782] - Loss value: 1.7042959928512573\n",
      "Epoch[00/01] | Iteration[0014/0782] - Loss value: 1.8143712282180786\n",
      "Epoch[00/01] | Iteration[0015/0782] - Loss value: 1.83595609664917\n",
      "Epoch[00/01] | Iteration[0016/0782] - Loss value: 1.5471872091293335\n",
      "Epoch[00/01] | Iteration[0017/0782] - Loss value: 1.748749017715454\n",
      "Epoch[00/01] | Iteration[0018/0782] - Loss value: 1.8598803281784058\n",
      "Epoch[00/01] | Iteration[0019/0782] - Loss value: 1.826826572418213\n",
      "Epoch[00/01] | Iteration[0020/0782] - Loss value: 1.6962882280349731\n",
      "Epoch[00/01] | Iteration[0021/0782] - Loss value: 1.6452858448028564\n",
      "Epoch[00/01] | Iteration[0022/0782] - Loss value: 1.7950587272644043\n",
      "Epoch[00/01] | Iteration[0023/0782] - Loss value: 1.89335298538208\n",
      "Epoch[00/01] | Iteration[0024/0782] - Loss value: 1.6270487308502197\n",
      "Epoch[00/01] | Iteration[0025/0782] - Loss value: 1.711114764213562\n",
      "Epoch[00/01] | Iteration[0026/0782] - Loss value: 1.6730178594589233\n",
      "Epoch[00/01] | Iteration[0027/0782] - Loss value: 1.6175868511199951\n",
      "Epoch[00/01] | Iteration[0028/0782] - Loss value: 1.9148082733154297\n",
      "Epoch[00/01] | Iteration[0029/0782] - Loss value: 1.647987723350525\n",
      "Epoch[00/01] | Iteration[0030/0782] - Loss value: 1.609648585319519\n",
      "Epoch[00/01] | Iteration[0031/0782] - Loss value: 1.6071228981018066\n",
      "Epoch[00/01] | Iteration[0032/0782] - Loss value: 1.734897255897522\n",
      "Epoch[00/01] | Iteration[0033/0782] - Loss value: 1.4944572448730469\n",
      "Epoch[00/01] | Iteration[0034/0782] - Loss value: 1.7979564666748047\n",
      "Epoch[00/01] | Iteration[0035/0782] - Loss value: 1.7499830722808838\n",
      "Epoch[00/01] | Iteration[0036/0782] - Loss value: 1.655507206916809\n",
      "Epoch[00/01] | Iteration[0037/0782] - Loss value: 1.6101948022842407\n",
      "Epoch[00/01] | Iteration[0038/0782] - Loss value: 1.8848515748977661\n",
      "Epoch[00/01] | Iteration[0039/0782] - Loss value: 1.5852713584899902\n",
      "Epoch[00/01] | Iteration[0040/0782] - Loss value: 1.6009725332260132\n",
      "Epoch[00/01] | Iteration[0041/0782] - Loss value: 1.684872031211853\n",
      "Epoch[00/01] | Iteration[0042/0782] - Loss value: 1.7257113456726074\n",
      "Epoch[00/01] | Iteration[0043/0782] - Loss value: 1.5393362045288086\n",
      "Epoch[00/01] | Iteration[0044/0782] - Loss value: 1.361911654472351\n",
      "Epoch[00/01] | Iteration[0045/0782] - Loss value: 1.6415694952011108\n",
      "Epoch[00/01] | Iteration[0046/0782] - Loss value: 1.5740711688995361\n",
      "Epoch[00/01] | Iteration[0047/0782] - Loss value: 1.592132568359375\n",
      "Epoch[00/01] | Iteration[0048/0782] - Loss value: 1.2969598770141602\n",
      "Epoch[00/01] | Iteration[0049/0782] - Loss value: 1.46720552444458\n",
      "Epoch[00/01] | Iteration[0050/0782] - Loss value: 1.7529411315917969\n",
      "Epoch[00/01] | Iteration[0051/0782] - Loss value: 1.4646281003952026\n",
      "Epoch[00/01] | Iteration[0052/0782] - Loss value: 1.353550672531128\n",
      "Epoch[00/01] | Iteration[0053/0782] - Loss value: 1.7182899713516235\n",
      "Epoch[00/01] | Iteration[0054/0782] - Loss value: 1.513840913772583\n",
      "Epoch[00/01] | Iteration[0055/0782] - Loss value: 1.2219964265823364\n",
      "Epoch[00/01] | Iteration[0056/0782] - Loss value: 1.3179033994674683\n",
      "Epoch[00/01] | Iteration[0057/0782] - Loss value: 1.44349205493927\n",
      "Epoch[00/01] | Iteration[0058/0782] - Loss value: 1.5928829908370972\n",
      "Epoch[00/01] | Iteration[0059/0782] - Loss value: 1.2289235591888428\n",
      "Epoch[00/01] | Iteration[0060/0782] - Loss value: 1.6074825525283813\n",
      "Epoch[00/01] | Iteration[0061/0782] - Loss value: 1.4846687316894531\n",
      "Epoch[00/01] | Iteration[0062/0782] - Loss value: 1.5687698125839233\n",
      "Epoch[00/01] | Iteration[0063/0782] - Loss value: 1.62702476978302\n",
      "Epoch[00/01] | Iteration[0064/0782] - Loss value: 1.9616844654083252\n",
      "Epoch[00/01] | Iteration[0065/0782] - Loss value: 1.5303442478179932\n",
      "Epoch[00/01] | Iteration[0066/0782] - Loss value: 1.703113317489624\n",
      "Epoch[00/01] | Iteration[0067/0782] - Loss value: 1.255737543106079\n",
      "Epoch[00/01] | Iteration[0068/0782] - Loss value: 1.4229793548583984\n",
      "Epoch[00/01] | Iteration[0069/0782] - Loss value: 1.78483247756958\n",
      "Epoch[00/01] | Iteration[0070/0782] - Loss value: 1.5180310010910034\n",
      "Epoch[00/01] | Iteration[0071/0782] - Loss value: 1.5870503187179565\n",
      "Epoch[00/01] | Iteration[0072/0782] - Loss value: 1.333250641822815\n",
      "Epoch[00/01] | Iteration[0073/0782] - Loss value: 1.589665174484253\n",
      "Epoch[00/01] | Iteration[0074/0782] - Loss value: 1.7228436470031738\n",
      "Epoch[00/01] | Iteration[0075/0782] - Loss value: 1.4717261791229248\n",
      "Epoch[00/01] | Iteration[0076/0782] - Loss value: 1.4762684106826782\n",
      "Epoch[00/01] | Iteration[0077/0782] - Loss value: 1.5006660223007202\n",
      "Epoch[00/01] | Iteration[0078/0782] - Loss value: 1.403167963027954\n",
      "Epoch[00/01] | Iteration[0079/0782] - Loss value: 1.5349375009536743\n",
      "Epoch[00/01] | Iteration[0080/0782] - Loss value: 1.535308599472046\n",
      "Epoch[00/01] | Iteration[0081/0782] - Loss value: 1.366287112236023\n",
      "Epoch[00/01] | Iteration[0082/0782] - Loss value: 1.563886284828186\n",
      "Epoch[00/01] | Iteration[0083/0782] - Loss value: 1.6544463634490967\n",
      "Epoch[00/01] | Iteration[0084/0782] - Loss value: 1.5083783864974976\n",
      "Epoch[00/01] | Iteration[0085/0782] - Loss value: 1.7090481519699097\n",
      "Epoch[00/01] | Iteration[0086/0782] - Loss value: 1.5766571760177612\n",
      "Epoch[00/01] | Iteration[0087/0782] - Loss value: 1.3797893524169922\n",
      "Epoch[00/01] | Iteration[0088/0782] - Loss value: 1.2346521615982056\n",
      "Epoch[00/01] | Iteration[0089/0782] - Loss value: 1.8136404752731323\n",
      "Epoch[00/01] | Iteration[0090/0782] - Loss value: 1.3893370628356934\n",
      "Epoch[00/01] | Iteration[0091/0782] - Loss value: 1.2338942289352417\n",
      "Epoch[00/01] | Iteration[0092/0782] - Loss value: 1.4775668382644653\n",
      "Epoch[00/01] | Iteration[0093/0782] - Loss value: 1.2897429466247559\n",
      "Epoch[00/01] | Iteration[0094/0782] - Loss value: 1.4935635328292847\n",
      "Epoch[00/01] | Iteration[0095/0782] - Loss value: 1.372010350227356\n",
      "Epoch[00/01] | Iteration[0096/0782] - Loss value: 1.3287670612335205\n",
      "Epoch[00/01] | Iteration[0097/0782] - Loss value: 1.4330750703811646\n",
      "Epoch[00/01] | Iteration[0098/0782] - Loss value: 1.371483325958252\n",
      "Epoch[00/01] | Iteration[0099/0782] - Loss value: 1.6003093719482422\n",
      "Epoch[00/01] | Iteration[0100/0782] - Loss value: 1.454196810722351\n",
      "Epoch[00/01] | Iteration[0101/0782] - Loss value: 1.232469916343689\n",
      "Epoch[00/01] | Iteration[0102/0782] - Loss value: 1.566462755203247\n",
      "Epoch[00/01] | Iteration[0103/0782] - Loss value: 1.4939937591552734\n",
      "Epoch[00/01] | Iteration[0104/0782] - Loss value: 1.5173585414886475\n",
      "Epoch[00/01] | Iteration[0105/0782] - Loss value: 1.6737799644470215\n",
      "Epoch[00/01] | Iteration[0106/0782] - Loss value: 1.3889517784118652\n",
      "Epoch[00/01] | Iteration[0107/0782] - Loss value: 1.4799554347991943\n",
      "Epoch[00/01] | Iteration[0108/0782] - Loss value: 1.272568941116333\n",
      "Epoch[00/01] | Iteration[0109/0782] - Loss value: 1.3181071281433105\n",
      "Epoch[00/01] | Iteration[0110/0782] - Loss value: 1.3924046754837036\n",
      "Epoch[00/01] | Iteration[0111/0782] - Loss value: 1.474396824836731\n",
      "Epoch[00/01] | Iteration[0112/0782] - Loss value: 1.3862953186035156\n",
      "Epoch[00/01] | Iteration[0113/0782] - Loss value: 1.3449747562408447\n",
      "Epoch[00/01] | Iteration[0114/0782] - Loss value: 1.4494554996490479\n",
      "Epoch[00/01] | Iteration[0115/0782] - Loss value: 1.3787156343460083\n",
      "Epoch[00/01] | Iteration[0116/0782] - Loss value: 1.35776686668396\n",
      "Epoch[00/01] | Iteration[0117/0782] - Loss value: 1.5532243251800537\n",
      "Epoch[00/01] | Iteration[0118/0782] - Loss value: 1.2019630670547485\n",
      "Epoch[00/01] | Iteration[0119/0782] - Loss value: 1.5429412126541138\n",
      "Epoch[00/01] | Iteration[0120/0782] - Loss value: 1.2659815549850464\n",
      "Epoch[00/01] | Iteration[0121/0782] - Loss value: 1.4192404747009277\n",
      "Epoch[00/01] | Iteration[0122/0782] - Loss value: 1.3275887966156006\n",
      "Epoch[00/01] | Iteration[0123/0782] - Loss value: 1.1684927940368652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[00/01] | Iteration[0124/0782] - Loss value: 1.2951730489730835\n",
      "Epoch[00/01] | Iteration[0125/0782] - Loss value: 1.5703880786895752\n",
      "Epoch[00/01] | Iteration[0126/0782] - Loss value: 1.2514410018920898\n",
      "Epoch[00/01] | Iteration[0127/0782] - Loss value: 1.190774917602539\n",
      "Epoch[00/01] | Iteration[0128/0782] - Loss value: 1.3223602771759033\n",
      "Epoch[00/01] | Iteration[0129/0782] - Loss value: 1.7839947938919067\n",
      "Epoch[00/01] | Iteration[0130/0782] - Loss value: 1.2271956205368042\n",
      "Epoch[00/01] | Iteration[0131/0782] - Loss value: 1.4590815305709839\n",
      "Epoch[00/01] | Iteration[0132/0782] - Loss value: 1.4354921579360962\n",
      "Epoch[00/01] | Iteration[0133/0782] - Loss value: 1.4357213973999023\n",
      "Epoch[00/01] | Iteration[0134/0782] - Loss value: 1.300621747970581\n",
      "Epoch[00/01] | Iteration[0135/0782] - Loss value: 1.3154948949813843\n",
      "Epoch[00/01] | Iteration[0136/0782] - Loss value: 1.466486930847168\n",
      "Epoch[00/01] | Iteration[0137/0782] - Loss value: 1.40294349193573\n",
      "Epoch[00/01] | Iteration[0138/0782] - Loss value: 1.2661653757095337\n",
      "Epoch[00/01] | Iteration[0139/0782] - Loss value: 1.3468114137649536\n",
      "Epoch[00/01] | Iteration[0140/0782] - Loss value: 1.4259179830551147\n",
      "Epoch[00/01] | Iteration[0141/0782] - Loss value: 1.1625897884368896\n",
      "Epoch[00/01] | Iteration[0142/0782] - Loss value: 1.338775634765625\n",
      "Epoch[00/01] | Iteration[0143/0782] - Loss value: 1.2045120000839233\n",
      "Epoch[00/01] | Iteration[0144/0782] - Loss value: 1.1133946180343628\n",
      "Epoch[00/01] | Iteration[0145/0782] - Loss value: 1.6416511535644531\n",
      "Epoch[00/01] | Iteration[0146/0782] - Loss value: 1.2184619903564453\n",
      "Epoch[00/01] | Iteration[0147/0782] - Loss value: 1.252545714378357\n",
      "Epoch[00/01] | Iteration[0148/0782] - Loss value: 1.2429028749465942\n",
      "Epoch[00/01] | Iteration[0149/0782] - Loss value: 1.1482802629470825\n",
      "Epoch[00/01] | Iteration[0150/0782] - Loss value: 1.2576978206634521\n",
      "Epoch[00/01] | Iteration[0151/0782] - Loss value: 1.1994091272354126\n",
      "Epoch[00/01] | Iteration[0152/0782] - Loss value: 1.4984130859375\n",
      "Epoch[00/01] | Iteration[0153/0782] - Loss value: 1.2718830108642578\n",
      "Epoch[00/01] | Iteration[0154/0782] - Loss value: 1.3355131149291992\n",
      "Epoch[00/01] | Iteration[0155/0782] - Loss value: 1.3363631963729858\n",
      "Epoch[00/01] | Iteration[0156/0782] - Loss value: 1.3187401294708252\n",
      "Epoch[00/01] | Iteration[0157/0782] - Loss value: 1.4717358350753784\n",
      "Epoch[00/01] | Iteration[0158/0782] - Loss value: 1.2759851217269897\n",
      "Epoch[00/01] | Iteration[0159/0782] - Loss value: 1.2806015014648438\n",
      "Epoch[00/01] | Iteration[0160/0782] - Loss value: 1.2851743698120117\n",
      "Epoch[00/01] | Iteration[0161/0782] - Loss value: 1.2911137342453003\n",
      "Epoch[00/01] | Iteration[0162/0782] - Loss value: 1.159569501876831\n",
      "Epoch[00/01] | Iteration[0163/0782] - Loss value: 0.996364176273346\n",
      "Epoch[00/01] | Iteration[0164/0782] - Loss value: 1.4719936847686768\n",
      "Epoch[00/01] | Iteration[0165/0782] - Loss value: 1.416982889175415\n",
      "Epoch[00/01] | Iteration[0166/0782] - Loss value: 1.5404599905014038\n",
      "Epoch[00/01] | Iteration[0167/0782] - Loss value: 1.1058056354522705\n",
      "Epoch[00/01] | Iteration[0168/0782] - Loss value: 1.2107702493667603\n",
      "Epoch[00/01] | Iteration[0169/0782] - Loss value: 1.447137475013733\n",
      "Epoch[00/01] | Iteration[0170/0782] - Loss value: 1.1722744703292847\n",
      "Epoch[00/01] | Iteration[0171/0782] - Loss value: 1.3120551109313965\n",
      "Epoch[00/01] | Iteration[0172/0782] - Loss value: 1.1977285146713257\n",
      "Epoch[00/01] | Iteration[0173/0782] - Loss value: 1.116590976715088\n",
      "Epoch[00/01] | Iteration[0174/0782] - Loss value: 1.3104639053344727\n",
      "Epoch[00/01] | Iteration[0175/0782] - Loss value: 1.308302879333496\n",
      "Epoch[00/01] | Iteration[0176/0782] - Loss value: 1.2890214920043945\n",
      "Epoch[00/01] | Iteration[0177/0782] - Loss value: 1.459633231163025\n",
      "Epoch[00/01] | Iteration[0178/0782] - Loss value: 1.2129493951797485\n",
      "Epoch[00/01] | Iteration[0179/0782] - Loss value: 1.0674148797988892\n",
      "Epoch[00/01] | Iteration[0180/0782] - Loss value: 0.9317601919174194\n",
      "Epoch[00/01] | Iteration[0181/0782] - Loss value: 1.187292456626892\n",
      "Epoch[00/01] | Iteration[0182/0782] - Loss value: 1.2620790004730225\n",
      "Epoch[00/01] | Iteration[0183/0782] - Loss value: 1.40451979637146\n",
      "Epoch[00/01] | Iteration[0184/0782] - Loss value: 1.430330753326416\n",
      "Epoch[00/01] | Iteration[0185/0782] - Loss value: 1.5775691270828247\n",
      "Epoch[00/01] | Iteration[0186/0782] - Loss value: 1.282618522644043\n",
      "Epoch[00/01] | Iteration[0187/0782] - Loss value: 1.2443963289260864\n",
      "Epoch[00/01] | Iteration[0188/0782] - Loss value: 1.2231160402297974\n",
      "Epoch[00/01] | Iteration[0189/0782] - Loss value: 1.1925243139266968\n",
      "Epoch[00/01] | Iteration[0190/0782] - Loss value: 1.4540244340896606\n",
      "Epoch[00/01] | Iteration[0191/0782] - Loss value: 1.2739373445510864\n",
      "Epoch[00/01] | Iteration[0192/0782] - Loss value: 1.268921971321106\n",
      "Epoch[00/01] | Iteration[0193/0782] - Loss value: 1.2127184867858887\n",
      "Epoch[00/01] | Iteration[0194/0782] - Loss value: 1.3266043663024902\n",
      "Epoch[00/01] | Iteration[0195/0782] - Loss value: 1.1947270631790161\n",
      "Epoch[00/01] | Iteration[0196/0782] - Loss value: 1.1342079639434814\n",
      "Epoch[00/01] | Iteration[0197/0782] - Loss value: 1.0746071338653564\n",
      "Epoch[00/01] | Iteration[0198/0782] - Loss value: 1.5342226028442383\n",
      "Epoch[00/01] | Iteration[0199/0782] - Loss value: 1.1056104898452759\n",
      "Epoch[00/01] | Iteration[0200/0782] - Loss value: 1.1717174053192139\n",
      "Epoch[00/01] | Iteration[0201/0782] - Loss value: 1.3071143627166748\n",
      "Epoch[00/01] | Iteration[0202/0782] - Loss value: 1.1130261421203613\n",
      "Epoch[00/01] | Iteration[0203/0782] - Loss value: 1.3223295211791992\n",
      "Epoch[00/01] | Iteration[0204/0782] - Loss value: 1.2012758255004883\n",
      "Epoch[00/01] | Iteration[0205/0782] - Loss value: 1.31981360912323\n",
      "Epoch[00/01] | Iteration[0206/0782] - Loss value: 1.0941946506500244\n",
      "Epoch[00/01] | Iteration[0207/0782] - Loss value: 1.0182340145111084\n",
      "Epoch[00/01] | Iteration[0208/0782] - Loss value: 1.116132140159607\n",
      "Epoch[00/01] | Iteration[0209/0782] - Loss value: 1.170485258102417\n",
      "Epoch[00/01] | Iteration[0210/0782] - Loss value: 0.922429084777832\n",
      "Epoch[00/01] | Iteration[0211/0782] - Loss value: 1.4449316263198853\n",
      "Epoch[00/01] | Iteration[0212/0782] - Loss value: 1.0689119100570679\n",
      "Epoch[00/01] | Iteration[0213/0782] - Loss value: 1.4961016178131104\n",
      "Epoch[00/01] | Iteration[0214/0782] - Loss value: 0.9390669465065002\n",
      "Epoch[00/01] | Iteration[0215/0782] - Loss value: 1.217703938484192\n",
      "Epoch[00/01] | Iteration[0216/0782] - Loss value: 1.3740239143371582\n",
      "Epoch[00/01] | Iteration[0217/0782] - Loss value: 1.3557374477386475\n",
      "Epoch[00/01] | Iteration[0218/0782] - Loss value: 1.0006078481674194\n",
      "Epoch[00/01] | Iteration[0219/0782] - Loss value: 1.0427813529968262\n",
      "Epoch[00/01] | Iteration[0220/0782] - Loss value: 1.0874824523925781\n",
      "Epoch[00/01] | Iteration[0221/0782] - Loss value: 1.1023471355438232\n",
      "Epoch[00/01] | Iteration[0222/0782] - Loss value: 1.303735375404358\n",
      "Epoch[00/01] | Iteration[0223/0782] - Loss value: 1.3048450946807861\n",
      "Epoch[00/01] | Iteration[0224/0782] - Loss value: 1.2569003105163574\n",
      "Epoch[00/01] | Iteration[0225/0782] - Loss value: 1.0148695707321167\n",
      "Epoch[00/01] | Iteration[0226/0782] - Loss value: 1.2757065296173096\n",
      "Epoch[00/01] | Iteration[0227/0782] - Loss value: 0.9904124736785889\n",
      "Epoch[00/01] | Iteration[0228/0782] - Loss value: 1.146308422088623\n",
      "Epoch[00/01] | Iteration[0229/0782] - Loss value: 1.0908442735671997\n",
      "Epoch[00/01] | Iteration[0230/0782] - Loss value: 0.9511938095092773\n",
      "Epoch[00/01] | Iteration[0231/0782] - Loss value: 1.3438140153884888\n",
      "Epoch[00/01] | Iteration[0232/0782] - Loss value: 1.1761581897735596\n",
      "Epoch[00/01] | Iteration[0233/0782] - Loss value: 1.2011432647705078\n",
      "Epoch[00/01] | Iteration[0234/0782] - Loss value: 1.1450140476226807\n",
      "Epoch[00/01] | Iteration[0235/0782] - Loss value: 0.8510680794715881\n",
      "Epoch[00/01] | Iteration[0236/0782] - Loss value: 1.3108481168746948\n",
      "Epoch[00/01] | Iteration[0237/0782] - Loss value: 1.0946012735366821\n",
      "Epoch[00/01] | Iteration[0238/0782] - Loss value: 1.2151793241500854\n",
      "Epoch[00/01] | Iteration[0239/0782] - Loss value: 0.8641886711120605\n",
      "Epoch[00/01] | Iteration[0240/0782] - Loss value: 1.1762933731079102\n",
      "Epoch[00/01] | Iteration[0241/0782] - Loss value: 1.1454205513000488\n",
      "Epoch[00/01] | Iteration[0242/0782] - Loss value: 1.2347698211669922\n",
      "Epoch[00/01] | Iteration[0243/0782] - Loss value: 1.3506882190704346\n",
      "Epoch[00/01] | Iteration[0244/0782] - Loss value: 1.1512280702590942\n",
      "Epoch[00/01] | Iteration[0245/0782] - Loss value: 1.1590088605880737\n",
      "Epoch[00/01] | Iteration[0246/0782] - Loss value: 1.0071099996566772\n",
      "Epoch[00/01] | Iteration[0247/0782] - Loss value: 1.1518555879592896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[00/01] | Iteration[0248/0782] - Loss value: 1.0973291397094727\n",
      "Epoch[00/01] | Iteration[0249/0782] - Loss value: 1.3761926889419556\n",
      "Epoch[00/01] | Iteration[0250/0782] - Loss value: 1.290653109550476\n",
      "Epoch[00/01] | Iteration[0251/0782] - Loss value: 1.2036532163619995\n",
      "Epoch[00/01] | Iteration[0252/0782] - Loss value: 1.159255027770996\n",
      "Epoch[00/01] | Iteration[0253/0782] - Loss value: 1.0755980014801025\n",
      "Epoch[00/01] | Iteration[0254/0782] - Loss value: 1.2042651176452637\n",
      "Epoch[00/01] | Iteration[0255/0782] - Loss value: 1.2798129320144653\n",
      "Epoch[00/01] | Iteration[0256/0782] - Loss value: 1.0002436637878418\n",
      "Epoch[00/01] | Iteration[0257/0782] - Loss value: 1.1597046852111816\n",
      "Epoch[00/01] | Iteration[0258/0782] - Loss value: 1.325532078742981\n",
      "Epoch[00/01] | Iteration[0259/0782] - Loss value: 1.2897629737854004\n",
      "Epoch[00/01] | Iteration[0260/0782] - Loss value: 1.1506073474884033\n",
      "Epoch[00/01] | Iteration[0261/0782] - Loss value: 0.9068703651428223\n",
      "Epoch[00/01] | Iteration[0262/0782] - Loss value: 1.101233959197998\n",
      "Epoch[00/01] | Iteration[0263/0782] - Loss value: 1.1704469919204712\n",
      "Epoch[00/01] | Iteration[0264/0782] - Loss value: 0.9064182043075562\n",
      "Epoch[00/01] | Iteration[0265/0782] - Loss value: 0.8575674295425415\n",
      "Epoch[00/01] | Iteration[0266/0782] - Loss value: 1.2060364484786987\n",
      "Epoch[00/01] | Iteration[0267/0782] - Loss value: 1.4541326761245728\n",
      "Epoch[00/01] | Iteration[0268/0782] - Loss value: 1.2204047441482544\n",
      "Epoch[00/01] | Iteration[0269/0782] - Loss value: 1.2067694664001465\n",
      "Epoch[00/01] | Iteration[0270/0782] - Loss value: 1.2302523851394653\n",
      "Epoch[00/01] | Iteration[0271/0782] - Loss value: 1.2564843893051147\n",
      "Epoch[00/01] | Iteration[0272/0782] - Loss value: 1.2586253881454468\n",
      "Epoch[00/01] | Iteration[0273/0782] - Loss value: 0.9748513102531433\n",
      "Epoch[00/01] | Iteration[0274/0782] - Loss value: 1.1280561685562134\n",
      "Epoch[00/01] | Iteration[0275/0782] - Loss value: 0.8810552954673767\n",
      "Epoch[00/01] | Iteration[0276/0782] - Loss value: 0.8952402472496033\n",
      "Epoch[00/01] | Iteration[0277/0782] - Loss value: 0.9572967886924744\n",
      "Epoch[00/01] | Iteration[0278/0782] - Loss value: 0.9369551539421082\n",
      "Epoch[00/01] | Iteration[0279/0782] - Loss value: 1.0108063220977783\n",
      "Epoch[00/01] | Iteration[0280/0782] - Loss value: 1.111108422279358\n",
      "Epoch[00/01] | Iteration[0281/0782] - Loss value: 1.0702824592590332\n",
      "Epoch[00/01] | Iteration[0282/0782] - Loss value: 0.9047329425811768\n",
      "Epoch[00/01] | Iteration[0283/0782] - Loss value: 1.023690104484558\n",
      "Epoch[00/01] | Iteration[0284/0782] - Loss value: 1.3219560384750366\n",
      "Epoch[00/01] | Iteration[0285/0782] - Loss value: 0.9393482208251953\n",
      "Epoch[00/01] | Iteration[0286/0782] - Loss value: 1.078476905822754\n",
      "Epoch[00/01] | Iteration[0287/0782] - Loss value: 0.9990033507347107\n",
      "Epoch[00/01] | Iteration[0288/0782] - Loss value: 0.9976346492767334\n",
      "Epoch[00/01] | Iteration[0289/0782] - Loss value: 1.1394689083099365\n",
      "Epoch[00/01] | Iteration[0290/0782] - Loss value: 0.9923833012580872\n",
      "Epoch[00/01] | Iteration[0291/0782] - Loss value: 1.0131354331970215\n",
      "Epoch[00/01] | Iteration[0292/0782] - Loss value: 1.0718514919281006\n",
      "Epoch[00/01] | Iteration[0293/0782] - Loss value: 0.9748257398605347\n",
      "Epoch[00/01] | Iteration[0294/0782] - Loss value: 1.0539857149124146\n",
      "Epoch[00/01] | Iteration[0295/0782] - Loss value: 1.1194647550582886\n",
      "Epoch[00/01] | Iteration[0296/0782] - Loss value: 1.2713674306869507\n",
      "Epoch[00/01] | Iteration[0297/0782] - Loss value: 1.1336003541946411\n",
      "Epoch[00/01] | Iteration[0298/0782] - Loss value: 1.324478030204773\n",
      "Epoch[00/01] | Iteration[0299/0782] - Loss value: 1.1437208652496338\n",
      "Epoch[00/01] | Iteration[0300/0782] - Loss value: 0.9061327576637268\n",
      "Epoch[00/01] | Iteration[0301/0782] - Loss value: 1.2517112493515015\n",
      "Epoch[00/01] | Iteration[0302/0782] - Loss value: 1.0215212106704712\n",
      "Epoch[00/01] | Iteration[0303/0782] - Loss value: 1.0953165292739868\n",
      "Epoch[00/01] | Iteration[0304/0782] - Loss value: 0.9973325133323669\n",
      "Epoch[00/01] | Iteration[0305/0782] - Loss value: 1.2493571043014526\n",
      "Epoch[00/01] | Iteration[0306/0782] - Loss value: 1.146991491317749\n",
      "Epoch[00/01] | Iteration[0307/0782] - Loss value: 1.1066367626190186\n",
      "Epoch[00/01] | Iteration[0308/0782] - Loss value: 0.970264196395874\n",
      "Epoch[00/01] | Iteration[0309/0782] - Loss value: 1.073164701461792\n",
      "Epoch[00/01] | Iteration[0310/0782] - Loss value: 0.9775055050849915\n",
      "Epoch[00/01] | Iteration[0311/0782] - Loss value: 1.2572509050369263\n",
      "Epoch[00/01] | Iteration[0312/0782] - Loss value: 1.0214518308639526\n",
      "Epoch[00/01] | Iteration[0313/0782] - Loss value: 0.9954701066017151\n",
      "Epoch[00/01] | Iteration[0314/0782] - Loss value: 1.2993525266647339\n",
      "Epoch[00/01] | Iteration[0315/0782] - Loss value: 0.9581588506698608\n",
      "Epoch[00/01] | Iteration[0316/0782] - Loss value: 0.9205296635627747\n",
      "Epoch[00/01] | Iteration[0317/0782] - Loss value: 1.0432721376419067\n",
      "Epoch[00/01] | Iteration[0318/0782] - Loss value: 0.9793452024459839\n",
      "Epoch[00/01] | Iteration[0319/0782] - Loss value: 1.5231521129608154\n",
      "Epoch[00/01] | Iteration[0320/0782] - Loss value: 1.0308917760849\n",
      "Epoch[00/01] | Iteration[0321/0782] - Loss value: 1.2223433256149292\n",
      "Epoch[00/01] | Iteration[0322/0782] - Loss value: 1.0049457550048828\n",
      "Epoch[00/01] | Iteration[0323/0782] - Loss value: 1.1251047849655151\n",
      "Epoch[00/01] | Iteration[0324/0782] - Loss value: 1.1445962190628052\n",
      "Epoch[00/01] | Iteration[0325/0782] - Loss value: 1.1386383771896362\n",
      "Epoch[00/01] | Iteration[0326/0782] - Loss value: 0.8846019506454468\n",
      "Epoch[00/01] | Iteration[0327/0782] - Loss value: 1.00546395778656\n",
      "Epoch[00/01] | Iteration[0328/0782] - Loss value: 0.9387769103050232\n",
      "Epoch[00/01] | Iteration[0329/0782] - Loss value: 1.149765133857727\n",
      "Epoch[00/01] | Iteration[0330/0782] - Loss value: 1.0981394052505493\n",
      "Epoch[00/01] | Iteration[0331/0782] - Loss value: 0.9506705403327942\n",
      "Epoch[00/01] | Iteration[0332/0782] - Loss value: 1.3911795616149902\n",
      "Epoch[00/01] | Iteration[0333/0782] - Loss value: 1.0228276252746582\n",
      "Epoch[00/01] | Iteration[0334/0782] - Loss value: 1.08233642578125\n",
      "Epoch[00/01] | Iteration[0335/0782] - Loss value: 1.0576742887496948\n",
      "Epoch[00/01] | Iteration[0336/0782] - Loss value: 0.9450293779373169\n",
      "Epoch[00/01] | Iteration[0337/0782] - Loss value: 1.1627904176712036\n",
      "Epoch[00/01] | Iteration[0338/0782] - Loss value: 1.0260648727416992\n",
      "Epoch[00/01] | Iteration[0339/0782] - Loss value: 1.2094789743423462\n",
      "Epoch[00/01] | Iteration[0340/0782] - Loss value: 1.1121759414672852\n",
      "Epoch[00/01] | Iteration[0341/0782] - Loss value: 1.2125144004821777\n",
      "Epoch[00/01] | Iteration[0342/0782] - Loss value: 1.0447783470153809\n",
      "Epoch[00/01] | Iteration[0343/0782] - Loss value: 0.9699689745903015\n",
      "Epoch[00/01] | Iteration[0344/0782] - Loss value: 1.1199510097503662\n",
      "Epoch[00/01] | Iteration[0345/0782] - Loss value: 1.1070835590362549\n",
      "Epoch[00/01] | Iteration[0346/0782] - Loss value: 1.0063332319259644\n",
      "Epoch[00/01] | Iteration[0347/0782] - Loss value: 1.1001636981964111\n",
      "Epoch[00/01] | Iteration[0348/0782] - Loss value: 0.9582050442695618\n",
      "Epoch[00/01] | Iteration[0349/0782] - Loss value: 1.1048702001571655\n",
      "Epoch[00/01] | Iteration[0350/0782] - Loss value: 1.1047663688659668\n",
      "Epoch[00/01] | Iteration[0351/0782] - Loss value: 0.9464704394340515\n",
      "Epoch[00/01] | Iteration[0352/0782] - Loss value: 1.1166329383850098\n",
      "Epoch[00/01] | Iteration[0353/0782] - Loss value: 0.926939070224762\n",
      "Epoch[00/01] | Iteration[0354/0782] - Loss value: 1.1490459442138672\n",
      "Epoch[00/01] | Iteration[0355/0782] - Loss value: 1.2726596593856812\n",
      "Epoch[00/01] | Iteration[0356/0782] - Loss value: 0.9338793754577637\n",
      "Epoch[00/01] | Iteration[0357/0782] - Loss value: 0.932925820350647\n",
      "Epoch[00/01] | Iteration[0358/0782] - Loss value: 1.1772656440734863\n",
      "Epoch[00/01] | Iteration[0359/0782] - Loss value: 1.0532855987548828\n",
      "Epoch[00/01] | Iteration[0360/0782] - Loss value: 0.9973783493041992\n",
      "Epoch[00/01] | Iteration[0361/0782] - Loss value: 0.9552289843559265\n",
      "Epoch[00/01] | Iteration[0362/0782] - Loss value: 1.048270344734192\n",
      "Epoch[00/01] | Iteration[0363/0782] - Loss value: 1.3221975564956665\n",
      "Epoch[00/01] | Iteration[0364/0782] - Loss value: 1.0372109413146973\n",
      "Epoch[00/01] | Iteration[0365/0782] - Loss value: 0.9869553446769714\n",
      "Epoch[00/01] | Iteration[0366/0782] - Loss value: 0.953741192817688\n",
      "Epoch[00/01] | Iteration[0367/0782] - Loss value: 0.9465950131416321\n",
      "Epoch[00/01] | Iteration[0368/0782] - Loss value: 0.97609943151474\n",
      "Epoch[00/01] | Iteration[0369/0782] - Loss value: 0.9920076727867126\n",
      "Epoch[00/01] | Iteration[0370/0782] - Loss value: 1.0673035383224487\n",
      "Epoch[00/01] | Iteration[0371/0782] - Loss value: 0.9211859703063965\n",
      "Epoch[00/01] | Iteration[0372/0782] - Loss value: 0.9298966526985168\n",
      "Epoch[00/01] | Iteration[0373/0782] - Loss value: 1.0024704933166504\n",
      "Epoch[00/01] | Iteration[0374/0782] - Loss value: 1.1194028854370117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[00/01] | Iteration[0375/0782] - Loss value: 0.9658626317977905\n",
      "Epoch[00/01] | Iteration[0376/0782] - Loss value: 0.9924424290657043\n",
      "Epoch[00/01] | Iteration[0377/0782] - Loss value: 1.0275440216064453\n",
      "Epoch[00/01] | Iteration[0378/0782] - Loss value: 0.9072968363761902\n",
      "Epoch[00/01] | Iteration[0379/0782] - Loss value: 0.9665477275848389\n",
      "Epoch[00/01] | Iteration[0380/0782] - Loss value: 0.966734766960144\n",
      "Epoch[00/01] | Iteration[0381/0782] - Loss value: 0.8336219787597656\n",
      "Epoch[00/01] | Iteration[0382/0782] - Loss value: 0.9676107168197632\n",
      "Epoch[00/01] | Iteration[0383/0782] - Loss value: 0.8312520980834961\n",
      "Epoch[00/01] | Iteration[0384/0782] - Loss value: 1.160073161125183\n",
      "Epoch[00/01] | Iteration[0385/0782] - Loss value: 1.1248979568481445\n",
      "Epoch[00/01] | Iteration[0386/0782] - Loss value: 1.1600863933563232\n",
      "Epoch[00/01] | Iteration[0387/0782] - Loss value: 1.0754854679107666\n",
      "Epoch[00/01] | Iteration[0388/0782] - Loss value: 1.0337696075439453\n",
      "Epoch[00/01] | Iteration[0389/0782] - Loss value: 1.0603010654449463\n",
      "Epoch[00/01] | Iteration[0390/0782] - Loss value: 1.212276577949524\n",
      "Epoch[00/01] | Iteration[0391/0782] - Loss value: 1.0338222980499268\n",
      "Epoch[00/01] | Iteration[0392/0782] - Loss value: 1.1749041080474854\n",
      "Epoch[00/01] | Iteration[0393/0782] - Loss value: 0.8964633941650391\n",
      "Epoch[00/01] | Iteration[0394/0782] - Loss value: 0.8630143404006958\n",
      "Epoch[00/01] | Iteration[0395/0782] - Loss value: 0.7169519662857056\n",
      "Epoch[00/01] | Iteration[0396/0782] - Loss value: 0.9504134058952332\n",
      "Epoch[00/01] | Iteration[0397/0782] - Loss value: 0.9762039184570312\n",
      "Epoch[00/01] | Iteration[0398/0782] - Loss value: 1.102962613105774\n",
      "Epoch[00/01] | Iteration[0399/0782] - Loss value: 0.9341779947280884\n",
      "Epoch[00/01] | Iteration[0400/0782] - Loss value: 0.7966282367706299\n",
      "Epoch[00/01] | Iteration[0401/0782] - Loss value: 0.9623681306838989\n",
      "Epoch[00/01] | Iteration[0402/0782] - Loss value: 0.8762028813362122\n",
      "Epoch[00/01] | Iteration[0403/0782] - Loss value: 1.0065150260925293\n",
      "Epoch[00/01] | Iteration[0404/0782] - Loss value: 0.6938619017601013\n",
      "Epoch[00/01] | Iteration[0405/0782] - Loss value: 0.8170060515403748\n",
      "Epoch[00/01] | Iteration[0406/0782] - Loss value: 1.088843822479248\n",
      "Epoch[00/01] | Iteration[0407/0782] - Loss value: 0.9723332524299622\n",
      "Epoch[00/01] | Iteration[0408/0782] - Loss value: 1.0302951335906982\n",
      "Epoch[00/01] | Iteration[0409/0782] - Loss value: 1.2014371156692505\n",
      "Epoch[00/01] | Iteration[0410/0782] - Loss value: 0.9985186457633972\n",
      "Epoch[00/01] | Iteration[0411/0782] - Loss value: 1.0795625448226929\n",
      "Epoch[00/01] | Iteration[0412/0782] - Loss value: 0.9883760213851929\n",
      "Epoch[00/01] | Iteration[0413/0782] - Loss value: 0.9097574353218079\n",
      "Epoch[00/01] | Iteration[0414/0782] - Loss value: 0.946426510810852\n",
      "Epoch[00/01] | Iteration[0415/0782] - Loss value: 0.9875075221061707\n",
      "Epoch[00/01] | Iteration[0416/0782] - Loss value: 0.9751299023628235\n",
      "Epoch[00/01] | Iteration[0417/0782] - Loss value: 0.9711540341377258\n",
      "Epoch[00/01] | Iteration[0418/0782] - Loss value: 0.6310805082321167\n",
      "Epoch[00/01] | Iteration[0419/0782] - Loss value: 0.874936580657959\n",
      "Epoch[00/01] | Iteration[0420/0782] - Loss value: 0.9829577803611755\n",
      "Epoch[00/01] | Iteration[0421/0782] - Loss value: 0.9618564248085022\n",
      "Epoch[00/01] | Iteration[0422/0782] - Loss value: 0.7567936182022095\n",
      "Epoch[00/01] | Iteration[0423/0782] - Loss value: 1.1360219717025757\n",
      "Epoch[00/01] | Iteration[0424/0782] - Loss value: 0.8597373962402344\n",
      "Epoch[00/01] | Iteration[0425/0782] - Loss value: 0.8401588797569275\n",
      "Epoch[00/01] | Iteration[0426/0782] - Loss value: 0.9129539132118225\n",
      "Epoch[00/01] | Iteration[0427/0782] - Loss value: 0.9147281050682068\n",
      "Epoch[00/01] | Iteration[0428/0782] - Loss value: 0.7787179946899414\n",
      "Epoch[00/01] | Iteration[0429/0782] - Loss value: 0.9486692547798157\n",
      "Epoch[00/01] | Iteration[0430/0782] - Loss value: 0.7966589331626892\n",
      "Epoch[00/01] | Iteration[0431/0782] - Loss value: 1.116626501083374\n",
      "Epoch[00/01] | Iteration[0432/0782] - Loss value: 1.1728979349136353\n",
      "Epoch[00/01] | Iteration[0433/0782] - Loss value: 0.8111604452133179\n",
      "Epoch[00/01] | Iteration[0434/0782] - Loss value: 0.9974561929702759\n",
      "Epoch[00/01] | Iteration[0435/0782] - Loss value: 0.9788665175437927\n",
      "Epoch[00/01] | Iteration[0436/0782] - Loss value: 0.8959273099899292\n",
      "Epoch[00/01] | Iteration[0437/0782] - Loss value: 0.6930134296417236\n",
      "Epoch[00/01] | Iteration[0438/0782] - Loss value: 0.7527276873588562\n",
      "Epoch[00/01] | Iteration[0439/0782] - Loss value: 1.0845626592636108\n",
      "Epoch[00/01] | Iteration[0440/0782] - Loss value: 1.0088618993759155\n",
      "Epoch[00/01] | Iteration[0441/0782] - Loss value: 1.0172003507614136\n",
      "Epoch[00/01] | Iteration[0442/0782] - Loss value: 0.8687179088592529\n",
      "Epoch[00/01] | Iteration[0443/0782] - Loss value: 1.1047133207321167\n",
      "Epoch[00/01] | Iteration[0444/0782] - Loss value: 0.841130793094635\n",
      "Epoch[00/01] | Iteration[0445/0782] - Loss value: 0.8093575239181519\n",
      "Epoch[00/01] | Iteration[0446/0782] - Loss value: 0.7821967601776123\n",
      "Epoch[00/01] | Iteration[0447/0782] - Loss value: 0.7658495903015137\n",
      "Epoch[00/01] | Iteration[0448/0782] - Loss value: 0.7126867771148682\n",
      "Epoch[00/01] | Iteration[0449/0782] - Loss value: 0.973021924495697\n",
      "Epoch[00/01] | Iteration[0450/0782] - Loss value: 0.7032299041748047\n",
      "Epoch[00/01] | Iteration[0451/0782] - Loss value: 0.7578079104423523\n",
      "Epoch[00/01] | Iteration[0452/0782] - Loss value: 0.9404481053352356\n",
      "Epoch[00/01] | Iteration[0453/0782] - Loss value: 0.9268254637718201\n",
      "Epoch[00/01] | Iteration[0454/0782] - Loss value: 0.9558315277099609\n",
      "Epoch[00/01] | Iteration[0455/0782] - Loss value: 0.7566782236099243\n",
      "Epoch[00/01] | Iteration[0456/0782] - Loss value: 0.9039332866668701\n",
      "Epoch[00/01] | Iteration[0457/0782] - Loss value: 0.8116880655288696\n",
      "Epoch[00/01] | Iteration[0458/0782] - Loss value: 0.9581016302108765\n",
      "Epoch[00/01] | Iteration[0459/0782] - Loss value: 0.9212173223495483\n",
      "Epoch[00/01] | Iteration[0460/0782] - Loss value: 0.8328796625137329\n",
      "Epoch[00/01] | Iteration[0461/0782] - Loss value: 0.9385429620742798\n",
      "Epoch[00/01] | Iteration[0462/0782] - Loss value: 0.912462592124939\n",
      "Epoch[00/01] | Iteration[0463/0782] - Loss value: 1.0505752563476562\n",
      "Epoch[00/01] | Iteration[0464/0782] - Loss value: 1.2270985841751099\n",
      "Epoch[00/01] | Iteration[0465/0782] - Loss value: 0.9859310388565063\n",
      "Epoch[00/01] | Iteration[0466/0782] - Loss value: 0.7918716073036194\n",
      "Epoch[00/01] | Iteration[0467/0782] - Loss value: 1.1336549520492554\n",
      "Epoch[00/01] | Iteration[0468/0782] - Loss value: 0.9327391386032104\n",
      "Epoch[00/01] | Iteration[0469/0782] - Loss value: 0.9761552214622498\n",
      "Epoch[00/01] | Iteration[0470/0782] - Loss value: 0.7901009321212769\n",
      "Epoch[00/01] | Iteration[0471/0782] - Loss value: 0.8593343496322632\n",
      "Epoch[00/01] | Iteration[0472/0782] - Loss value: 1.129815936088562\n",
      "Epoch[00/01] | Iteration[0473/0782] - Loss value: 1.163502812385559\n",
      "Epoch[00/01] | Iteration[0474/0782] - Loss value: 0.7217434644699097\n",
      "Epoch[00/01] | Iteration[0475/0782] - Loss value: 0.8848049640655518\n",
      "Epoch[00/01] | Iteration[0476/0782] - Loss value: 0.7622790932655334\n",
      "Epoch[00/01] | Iteration[0477/0782] - Loss value: 0.8569704294204712\n",
      "Epoch[00/01] | Iteration[0478/0782] - Loss value: 0.7084373831748962\n",
      "Epoch[00/01] | Iteration[0479/0782] - Loss value: 0.9553797245025635\n",
      "Epoch[00/01] | Iteration[0480/0782] - Loss value: 0.8546959161758423\n",
      "Epoch[00/01] | Iteration[0481/0782] - Loss value: 0.967142641544342\n",
      "Epoch[00/01] | Iteration[0482/0782] - Loss value: 0.8230190277099609\n",
      "Epoch[00/01] | Iteration[0483/0782] - Loss value: 0.8664373159408569\n",
      "Epoch[00/01] | Iteration[0484/0782] - Loss value: 1.1444549560546875\n",
      "Epoch[00/01] | Iteration[0485/0782] - Loss value: 1.1942225694656372\n",
      "Epoch[00/01] | Iteration[0486/0782] - Loss value: 0.8962849378585815\n",
      "Epoch[00/01] | Iteration[0487/0782] - Loss value: 0.8689424991607666\n",
      "Epoch[00/01] | Iteration[0488/0782] - Loss value: 0.8299594521522522\n",
      "Epoch[00/01] | Iteration[0489/0782] - Loss value: 0.8688235878944397\n",
      "Epoch[00/01] | Iteration[0490/0782] - Loss value: 0.8234193325042725\n",
      "Epoch[00/01] | Iteration[0491/0782] - Loss value: 0.7219650149345398\n",
      "Epoch[00/01] | Iteration[0492/0782] - Loss value: 0.9807356595993042\n",
      "Epoch[00/01] | Iteration[0493/0782] - Loss value: 0.8704413175582886\n",
      "Epoch[00/01] | Iteration[0494/0782] - Loss value: 0.6642406582832336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[00/01] | Iteration[0495/0782] - Loss value: 0.945999264717102\n",
      "Epoch[00/01] | Iteration[0496/0782] - Loss value: 0.8412708044052124\n",
      "Epoch[00/01] | Iteration[0497/0782] - Loss value: 0.7915580868721008\n",
      "Epoch[00/01] | Iteration[0498/0782] - Loss value: 1.036447286605835\n",
      "Epoch[00/01] | Iteration[0499/0782] - Loss value: 0.8610602021217346\n",
      "Epoch[00/01] | Iteration[0500/0782] - Loss value: 1.0804483890533447\n",
      "Epoch[00/01] | Iteration[0501/0782] - Loss value: 0.8755664825439453\n",
      "Epoch[00/01] | Iteration[0502/0782] - Loss value: 1.013404130935669\n",
      "Epoch[00/01] | Iteration[0503/0782] - Loss value: 1.1217541694641113\n",
      "Epoch[00/01] | Iteration[0504/0782] - Loss value: 0.7324658632278442\n",
      "Epoch[00/01] | Iteration[0505/0782] - Loss value: 0.8000478148460388\n",
      "Epoch[00/01] | Iteration[0506/0782] - Loss value: 1.2742356061935425\n",
      "Epoch[00/01] | Iteration[0507/0782] - Loss value: 0.8111630082130432\n",
      "Epoch[00/01] | Iteration[0508/0782] - Loss value: 0.841151237487793\n",
      "Epoch[00/01] | Iteration[0509/0782] - Loss value: 0.9808298945426941\n",
      "Epoch[00/01] | Iteration[0510/0782] - Loss value: 0.8873170018196106\n",
      "Epoch[00/01] | Iteration[0511/0782] - Loss value: 0.7600264549255371\n",
      "Epoch[00/01] | Iteration[0512/0782] - Loss value: 0.7668149471282959\n",
      "Epoch[00/01] | Iteration[0513/0782] - Loss value: 0.8053961992263794\n",
      "Epoch[00/01] | Iteration[0514/0782] - Loss value: 0.9336342811584473\n",
      "Epoch[00/01] | Iteration[0515/0782] - Loss value: 1.0377672910690308\n",
      "Epoch[00/01] | Iteration[0516/0782] - Loss value: 0.9238571524620056\n",
      "Epoch[00/01] | Iteration[0517/0782] - Loss value: 0.8580585718154907\n",
      "Epoch[00/01] | Iteration[0518/0782] - Loss value: 0.8021833896636963\n",
      "Epoch[00/01] | Iteration[0519/0782] - Loss value: 1.0123258829116821\n",
      "Epoch[00/01] | Iteration[0520/0782] - Loss value: 0.8962815999984741\n",
      "Epoch[00/01] | Iteration[0521/0782] - Loss value: 0.9555022716522217\n",
      "Epoch[00/01] | Iteration[0522/0782] - Loss value: 0.7359998226165771\n",
      "Epoch[00/01] | Iteration[0523/0782] - Loss value: 0.8732077479362488\n",
      "Epoch[00/01] | Iteration[0524/0782] - Loss value: 0.9442852735519409\n",
      "Epoch[00/01] | Iteration[0525/0782] - Loss value: 0.7494372129440308\n",
      "Epoch[00/01] | Iteration[0526/0782] - Loss value: 0.7915536165237427\n",
      "Epoch[00/01] | Iteration[0527/0782] - Loss value: 0.7785702347755432\n",
      "Epoch[00/01] | Iteration[0528/0782] - Loss value: 0.9395480155944824\n",
      "Epoch[00/01] | Iteration[0529/0782] - Loss value: 1.130509853363037\n",
      "Epoch[00/01] | Iteration[0530/0782] - Loss value: 0.8942131996154785\n",
      "Epoch[00/01] | Iteration[0531/0782] - Loss value: 1.1053271293640137\n",
      "Epoch[00/01] | Iteration[0532/0782] - Loss value: 0.6944795250892639\n",
      "Epoch[00/01] | Iteration[0533/0782] - Loss value: 0.9983029365539551\n",
      "Epoch[00/01] | Iteration[0534/0782] - Loss value: 0.9841785430908203\n",
      "Epoch[00/01] | Iteration[0535/0782] - Loss value: 0.8552753329277039\n",
      "Epoch[00/01] | Iteration[0536/0782] - Loss value: 0.7639403343200684\n",
      "Epoch[00/01] | Iteration[0537/0782] - Loss value: 0.921718180179596\n",
      "Epoch[00/01] | Iteration[0538/0782] - Loss value: 0.8091646432876587\n",
      "Epoch[00/01] | Iteration[0539/0782] - Loss value: 0.65333092212677\n",
      "Epoch[00/01] | Iteration[0540/0782] - Loss value: 0.9746154546737671\n",
      "Epoch[00/01] | Iteration[0541/0782] - Loss value: 0.8783324956893921\n",
      "Epoch[00/01] | Iteration[0542/0782] - Loss value: 0.9904606938362122\n",
      "Epoch[00/01] | Iteration[0543/0782] - Loss value: 0.8880054354667664\n",
      "Epoch[00/01] | Iteration[0544/0782] - Loss value: 0.8778895735740662\n",
      "Epoch[00/01] | Iteration[0545/0782] - Loss value: 0.8209062218666077\n",
      "Epoch[00/01] | Iteration[0546/0782] - Loss value: 0.8819059133529663\n",
      "Epoch[00/01] | Iteration[0547/0782] - Loss value: 1.3451831340789795\n",
      "Epoch[00/01] | Iteration[0548/0782] - Loss value: 0.983305037021637\n",
      "Epoch[00/01] | Iteration[0549/0782] - Loss value: 1.0558948516845703\n",
      "Epoch[00/01] | Iteration[0550/0782] - Loss value: 0.7358063459396362\n",
      "Epoch[00/01] | Iteration[0551/0782] - Loss value: 0.8744146227836609\n",
      "Epoch[00/01] | Iteration[0552/0782] - Loss value: 0.8741741180419922\n",
      "Epoch[00/01] | Iteration[0553/0782] - Loss value: 0.707688570022583\n",
      "Epoch[00/01] | Iteration[0554/0782] - Loss value: 0.8719371557235718\n",
      "Epoch[00/01] | Iteration[0555/0782] - Loss value: 0.7395247220993042\n",
      "Epoch[00/01] | Iteration[0556/0782] - Loss value: 0.49820762872695923\n",
      "Epoch[00/01] | Iteration[0557/0782] - Loss value: 1.1907784938812256\n",
      "Epoch[00/01] | Iteration[0558/0782] - Loss value: 1.1674463748931885\n",
      "Epoch[00/01] | Iteration[0559/0782] - Loss value: 0.8506368398666382\n",
      "Epoch[00/01] | Iteration[0560/0782] - Loss value: 0.8684293627738953\n",
      "Epoch[00/01] | Iteration[0561/0782] - Loss value: 1.0053743124008179\n",
      "Epoch[00/01] | Iteration[0562/0782] - Loss value: 0.9132494926452637\n",
      "Epoch[00/01] | Iteration[0563/0782] - Loss value: 0.9362077116966248\n",
      "Epoch[00/01] | Iteration[0564/0782] - Loss value: 1.1384085416793823\n",
      "Epoch[00/01] | Iteration[0565/0782] - Loss value: 0.6289908289909363\n",
      "Epoch[00/01] | Iteration[0566/0782] - Loss value: 0.795665442943573\n",
      "Epoch[00/01] | Iteration[0567/0782] - Loss value: 0.93415367603302\n",
      "Epoch[00/01] | Iteration[0568/0782] - Loss value: 0.9561645984649658\n",
      "Epoch[00/01] | Iteration[0569/0782] - Loss value: 0.7317919135093689\n",
      "Epoch[00/01] | Iteration[0570/0782] - Loss value: 0.8032721281051636\n",
      "Epoch[00/01] | Iteration[0571/0782] - Loss value: 0.6817357540130615\n",
      "Epoch[00/01] | Iteration[0572/0782] - Loss value: 1.1183266639709473\n",
      "Epoch[00/01] | Iteration[0573/0782] - Loss value: 0.9071822762489319\n",
      "Epoch[00/01] | Iteration[0574/0782] - Loss value: 0.9391424059867859\n",
      "Epoch[00/01] | Iteration[0575/0782] - Loss value: 0.8694440126419067\n",
      "Epoch[00/01] | Iteration[0576/0782] - Loss value: 0.8803285956382751\n",
      "Epoch[00/01] | Iteration[0577/0782] - Loss value: 0.9176036715507507\n",
      "Epoch[00/01] | Iteration[0578/0782] - Loss value: 0.9695717096328735\n",
      "Epoch[00/01] | Iteration[0579/0782] - Loss value: 0.9583647847175598\n",
      "Epoch[00/01] | Iteration[0580/0782] - Loss value: 0.662503719329834\n",
      "Epoch[00/01] | Iteration[0581/0782] - Loss value: 0.739361047744751\n",
      "Epoch[00/01] | Iteration[0582/0782] - Loss value: 0.7759315371513367\n",
      "Epoch[00/01] | Iteration[0583/0782] - Loss value: 0.9848235845565796\n",
      "Epoch[00/01] | Iteration[0584/0782] - Loss value: 0.7852265238761902\n",
      "Epoch[00/01] | Iteration[0585/0782] - Loss value: 1.0123666524887085\n",
      "Epoch[00/01] | Iteration[0586/0782] - Loss value: 0.8440260291099548\n",
      "Epoch[00/01] | Iteration[0587/0782] - Loss value: 0.8315132856369019\n",
      "Epoch[00/01] | Iteration[0588/0782] - Loss value: 0.7581315040588379\n",
      "Epoch[00/01] | Iteration[0589/0782] - Loss value: 0.730139970779419\n",
      "Epoch[00/01] | Iteration[0590/0782] - Loss value: 0.8983232378959656\n",
      "Epoch[00/01] | Iteration[0591/0782] - Loss value: 0.7922113537788391\n",
      "Epoch[00/01] | Iteration[0592/0782] - Loss value: 0.829792320728302\n",
      "Epoch[00/01] | Iteration[0593/0782] - Loss value: 0.8117794990539551\n",
      "Epoch[00/01] | Iteration[0594/0782] - Loss value: 0.8645875453948975\n",
      "Epoch[00/01] | Iteration[0595/0782] - Loss value: 0.9060097932815552\n",
      "Epoch[00/01] | Iteration[0596/0782] - Loss value: 1.0898560285568237\n",
      "Epoch[00/01] | Iteration[0597/0782] - Loss value: 0.7393606305122375\n",
      "Epoch[00/01] | Iteration[0598/0782] - Loss value: 1.0156514644622803\n",
      "Epoch[00/01] | Iteration[0599/0782] - Loss value: 1.1151998043060303\n",
      "Epoch[00/01] | Iteration[0600/0782] - Loss value: 0.8836379647254944\n",
      "Epoch[00/01] | Iteration[0601/0782] - Loss value: 0.6586849093437195\n",
      "Epoch[00/01] | Iteration[0602/0782] - Loss value: 0.7723878622055054\n",
      "Epoch[00/01] | Iteration[0603/0782] - Loss value: 0.9814478754997253\n",
      "Epoch[00/01] | Iteration[0604/0782] - Loss value: 0.8808497786521912\n",
      "Epoch[00/01] | Iteration[0605/0782] - Loss value: 0.9069368839263916\n",
      "Epoch[00/01] | Iteration[0606/0782] - Loss value: 0.7708104252815247\n",
      "Epoch[00/01] | Iteration[0607/0782] - Loss value: 0.78285813331604\n",
      "Epoch[00/01] | Iteration[0608/0782] - Loss value: 1.0129845142364502\n",
      "Epoch[00/01] | Iteration[0609/0782] - Loss value: 1.0132582187652588\n",
      "Epoch[00/01] | Iteration[0610/0782] - Loss value: 1.114654779434204\n",
      "Epoch[00/01] | Iteration[0611/0782] - Loss value: 0.7278679609298706\n",
      "Epoch[00/01] | Iteration[0612/0782] - Loss value: 0.6647136211395264\n",
      "Epoch[00/01] | Iteration[0613/0782] - Loss value: 0.7865883111953735\n",
      "Epoch[00/01] | Iteration[0614/0782] - Loss value: 0.7334190011024475\n",
      "Epoch[00/01] | Iteration[0615/0782] - Loss value: 0.89596027135849\n",
      "Epoch[00/01] | Iteration[0616/0782] - Loss value: 0.720599353313446\n",
      "Epoch[00/01] | Iteration[0617/0782] - Loss value: 0.7819626927375793\n",
      "Epoch[00/01] | Iteration[0618/0782] - Loss value: 0.76645427942276\n",
      "Epoch[00/01] | Iteration[0619/0782] - Loss value: 0.9399409890174866\n",
      "Epoch[00/01] | Iteration[0620/0782] - Loss value: 0.8328078389167786\n",
      "Epoch[00/01] | Iteration[0621/0782] - Loss value: 0.7102532982826233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[00/01] | Iteration[0622/0782] - Loss value: 0.749736487865448\n",
      "Epoch[00/01] | Iteration[0623/0782] - Loss value: 1.0444854497909546\n",
      "Epoch[00/01] | Iteration[0624/0782] - Loss value: 0.8185207843780518\n",
      "Epoch[00/01] | Iteration[0625/0782] - Loss value: 0.7499045133590698\n",
      "Epoch[00/01] | Iteration[0626/0782] - Loss value: 0.8512576818466187\n",
      "Epoch[00/01] | Iteration[0627/0782] - Loss value: 0.9600308537483215\n",
      "Epoch[00/01] | Iteration[0628/0782] - Loss value: 0.7963044047355652\n",
      "Epoch[00/01] | Iteration[0629/0782] - Loss value: 0.7864964604377747\n",
      "Epoch[00/01] | Iteration[0630/0782] - Loss value: 0.7303049564361572\n",
      "Epoch[00/01] | Iteration[0631/0782] - Loss value: 0.8515620827674866\n",
      "Epoch[00/01] | Iteration[0632/0782] - Loss value: 0.9333844184875488\n",
      "Epoch[00/01] | Iteration[0633/0782] - Loss value: 0.7573062777519226\n",
      "Epoch[00/01] | Iteration[0634/0782] - Loss value: 0.8725529909133911\n",
      "Epoch[00/01] | Iteration[0635/0782] - Loss value: 0.809981644153595\n",
      "Epoch[00/01] | Iteration[0636/0782] - Loss value: 0.9199807047843933\n",
      "Epoch[00/01] | Iteration[0637/0782] - Loss value: 0.9320208430290222\n",
      "Epoch[00/01] | Iteration[0638/0782] - Loss value: 1.0168083906173706\n",
      "Epoch[00/01] | Iteration[0639/0782] - Loss value: 0.7360538840293884\n",
      "Epoch[00/01] | Iteration[0640/0782] - Loss value: 0.9094473123550415\n",
      "Epoch[00/01] | Iteration[0641/0782] - Loss value: 0.8276633024215698\n",
      "Epoch[00/01] | Iteration[0642/0782] - Loss value: 0.7280135154724121\n",
      "Epoch[00/01] | Iteration[0643/0782] - Loss value: 0.8825738430023193\n",
      "Epoch[00/01] | Iteration[0644/0782] - Loss value: 0.9832212328910828\n",
      "Epoch[00/01] | Iteration[0645/0782] - Loss value: 0.5952388644218445\n",
      "Epoch[00/01] | Iteration[0646/0782] - Loss value: 1.0999521017074585\n",
      "Epoch[00/01] | Iteration[0647/0782] - Loss value: 0.8251306414604187\n",
      "Epoch[00/01] | Iteration[0648/0782] - Loss value: 1.1237777471542358\n",
      "Epoch[00/01] | Iteration[0649/0782] - Loss value: 0.9716600179672241\n",
      "Epoch[00/01] | Iteration[0650/0782] - Loss value: 0.6616915464401245\n",
      "Epoch[00/01] | Iteration[0651/0782] - Loss value: 0.9980383515357971\n",
      "Epoch[00/01] | Iteration[0652/0782] - Loss value: 0.7448464632034302\n",
      "Epoch[00/01] | Iteration[0653/0782] - Loss value: 1.1412017345428467\n",
      "Epoch[00/01] | Iteration[0654/0782] - Loss value: 0.9932489395141602\n",
      "Epoch[00/01] | Iteration[0655/0782] - Loss value: 1.0477529764175415\n",
      "Epoch[00/01] | Iteration[0656/0782] - Loss value: 0.885242223739624\n",
      "Epoch[00/01] | Iteration[0657/0782] - Loss value: 0.836675226688385\n",
      "Epoch[00/01] | Iteration[0658/0782] - Loss value: 0.9746367931365967\n",
      "Epoch[00/01] | Iteration[0659/0782] - Loss value: 0.774319589138031\n",
      "Epoch[00/01] | Iteration[0660/0782] - Loss value: 0.9653835296630859\n",
      "Epoch[00/01] | Iteration[0661/0782] - Loss value: 0.86970454454422\n",
      "Epoch[00/01] | Iteration[0662/0782] - Loss value: 0.8234046101570129\n",
      "Epoch[00/01] | Iteration[0663/0782] - Loss value: 0.7804480195045471\n",
      "Epoch[00/01] | Iteration[0664/0782] - Loss value: 0.7772639393806458\n",
      "Epoch[00/01] | Iteration[0665/0782] - Loss value: 0.8800199627876282\n",
      "Epoch[00/01] | Iteration[0666/0782] - Loss value: 0.785430908203125\n",
      "Epoch[00/01] | Iteration[0667/0782] - Loss value: 0.6733706593513489\n",
      "Epoch[00/01] | Iteration[0668/0782] - Loss value: 1.0187231302261353\n",
      "Epoch[00/01] | Iteration[0669/0782] - Loss value: 1.032705545425415\n",
      "Epoch[00/01] | Iteration[0670/0782] - Loss value: 0.8915057182312012\n",
      "Epoch[00/01] | Iteration[0671/0782] - Loss value: 0.6627835035324097\n",
      "Epoch[00/01] | Iteration[0672/0782] - Loss value: 0.9729800820350647\n",
      "Epoch[00/01] | Iteration[0673/0782] - Loss value: 0.9333804249763489\n",
      "Epoch[00/01] | Iteration[0674/0782] - Loss value: 0.9536896347999573\n",
      "Epoch[00/01] | Iteration[0675/0782] - Loss value: 0.8835173845291138\n",
      "Epoch[00/01] | Iteration[0676/0782] - Loss value: 0.9734168648719788\n",
      "Epoch[00/01] | Iteration[0677/0782] - Loss value: 0.7437276840209961\n",
      "Epoch[00/01] | Iteration[0678/0782] - Loss value: 0.9583170413970947\n",
      "Epoch[00/01] | Iteration[0679/0782] - Loss value: 0.680300235748291\n",
      "Epoch[00/01] | Iteration[0680/0782] - Loss value: 0.6506926417350769\n",
      "Epoch[00/01] | Iteration[0681/0782] - Loss value: 0.8929063677787781\n",
      "Epoch[00/01] | Iteration[0682/0782] - Loss value: 0.740189254283905\n",
      "Epoch[00/01] | Iteration[0683/0782] - Loss value: 0.7657985687255859\n",
      "Epoch[00/01] | Iteration[0684/0782] - Loss value: 1.0579193830490112\n",
      "Epoch[00/01] | Iteration[0685/0782] - Loss value: 0.8654815554618835\n",
      "Epoch[00/01] | Iteration[0686/0782] - Loss value: 1.1940932273864746\n",
      "Epoch[00/01] | Iteration[0687/0782] - Loss value: 0.9290606379508972\n",
      "Epoch[00/01] | Iteration[0688/0782] - Loss value: 0.734794020652771\n",
      "Epoch[00/01] | Iteration[0689/0782] - Loss value: 0.8658069968223572\n",
      "Epoch[00/01] | Iteration[0690/0782] - Loss value: 1.2880300283432007\n",
      "Epoch[00/01] | Iteration[0691/0782] - Loss value: 0.9766905307769775\n",
      "Epoch[00/01] | Iteration[0692/0782] - Loss value: 0.5946109294891357\n",
      "Epoch[00/01] | Iteration[0693/0782] - Loss value: 0.9192792177200317\n",
      "Epoch[00/01] | Iteration[0694/0782] - Loss value: 1.0175185203552246\n",
      "Epoch[00/01] | Iteration[0695/0782] - Loss value: 0.8681297302246094\n",
      "Epoch[00/01] | Iteration[0696/0782] - Loss value: 0.980521023273468\n",
      "Epoch[00/01] | Iteration[0697/0782] - Loss value: 0.9448004961013794\n",
      "Epoch[00/01] | Iteration[0698/0782] - Loss value: 1.0286712646484375\n",
      "Epoch[00/01] | Iteration[0699/0782] - Loss value: 0.9906535148620605\n",
      "Epoch[00/01] | Iteration[0700/0782] - Loss value: 0.8052213788032532\n",
      "Epoch[00/01] | Iteration[0701/0782] - Loss value: 0.9715439081192017\n",
      "Epoch[00/01] | Iteration[0702/0782] - Loss value: 0.958895742893219\n",
      "Epoch[00/01] | Iteration[0703/0782] - Loss value: 0.9151962995529175\n",
      "Epoch[00/01] | Iteration[0704/0782] - Loss value: 0.8695364594459534\n",
      "Epoch[00/01] | Iteration[0705/0782] - Loss value: 0.7482373714447021\n",
      "Epoch[00/01] | Iteration[0706/0782] - Loss value: 0.7614598274230957\n",
      "Epoch[00/01] | Iteration[0707/0782] - Loss value: 0.7831444144248962\n",
      "Epoch[00/01] | Iteration[0708/0782] - Loss value: 0.44362038373947144\n",
      "Epoch[00/01] | Iteration[0709/0782] - Loss value: 0.643593966960907\n",
      "Epoch[00/01] | Iteration[0710/0782] - Loss value: 0.7018431425094604\n",
      "Epoch[00/01] | Iteration[0711/0782] - Loss value: 0.9033041000366211\n",
      "Epoch[00/01] | Iteration[0712/0782] - Loss value: 0.8803365230560303\n",
      "Epoch[00/01] | Iteration[0713/0782] - Loss value: 1.0135605335235596\n",
      "Epoch[00/01] | Iteration[0714/0782] - Loss value: 0.6061922311782837\n",
      "Epoch[00/01] | Iteration[0715/0782] - Loss value: 0.9834786057472229\n",
      "Epoch[00/01] | Iteration[0716/0782] - Loss value: 0.6860373020172119\n",
      "Epoch[00/01] | Iteration[0717/0782] - Loss value: 0.7565320730209351\n",
      "Epoch[00/01] | Iteration[0718/0782] - Loss value: 0.7282311916351318\n",
      "Epoch[00/01] | Iteration[0719/0782] - Loss value: 0.8959360718727112\n",
      "Epoch[00/01] | Iteration[0720/0782] - Loss value: 0.7341044545173645\n",
      "Epoch[00/01] | Iteration[0721/0782] - Loss value: 0.5559278130531311\n",
      "Epoch[00/01] | Iteration[0722/0782] - Loss value: 0.8591057062149048\n",
      "Epoch[00/01] | Iteration[0723/0782] - Loss value: 0.8347694277763367\n",
      "Epoch[00/01] | Iteration[0724/0782] - Loss value: 0.6611649990081787\n",
      "Epoch[00/01] | Iteration[0725/0782] - Loss value: 0.8336600065231323\n",
      "Epoch[00/01] | Iteration[0726/0782] - Loss value: 1.0558602809906006\n",
      "Epoch[00/01] | Iteration[0727/0782] - Loss value: 0.8276200294494629\n",
      "Epoch[00/01] | Iteration[0728/0782] - Loss value: 0.7614067196846008\n",
      "Epoch[00/01] | Iteration[0729/0782] - Loss value: 0.7718410491943359\n",
      "Epoch[00/01] | Iteration[0730/0782] - Loss value: 0.5556100606918335\n",
      "Epoch[00/01] | Iteration[0731/0782] - Loss value: 0.7215896248817444\n",
      "Epoch[00/01] | Iteration[0732/0782] - Loss value: 0.7387627959251404\n",
      "Epoch[00/01] | Iteration[0733/0782] - Loss value: 0.8793308138847351\n",
      "Epoch[00/01] | Iteration[0734/0782] - Loss value: 0.5962048768997192\n",
      "Epoch[00/01] | Iteration[0735/0782] - Loss value: 0.9516997337341309\n",
      "Epoch[00/01] | Iteration[0736/0782] - Loss value: 0.9176599383354187\n",
      "Epoch[00/01] | Iteration[0737/0782] - Loss value: 0.8291056156158447\n",
      "Epoch[00/01] | Iteration[0738/0782] - Loss value: 0.659378170967102\n",
      "Epoch[00/01] | Iteration[0739/0782] - Loss value: 0.5808723568916321\n",
      "Epoch[00/01] | Iteration[0740/0782] - Loss value: 1.0643963813781738\n",
      "Epoch[00/01] | Iteration[0741/0782] - Loss value: 0.8803566694259644\n",
      "Epoch[00/01] | Iteration[0742/0782] - Loss value: 1.064555048942566\n",
      "Epoch[00/01] | Iteration[0743/0782] - Loss value: 0.662787675857544\n",
      "Epoch[00/01] | Iteration[0744/0782] - Loss value: 0.8300246000289917\n",
      "Epoch[00/01] | Iteration[0745/0782] - Loss value: 0.8937334418296814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[00/01] | Iteration[0746/0782] - Loss value: 0.8741610646247864\n",
      "Epoch[00/01] | Iteration[0747/0782] - Loss value: 0.8474491834640503\n",
      "Epoch[00/01] | Iteration[0748/0782] - Loss value: 0.830021858215332\n",
      "Epoch[00/01] | Iteration[0749/0782] - Loss value: 0.665376603603363\n",
      "Epoch[00/01] | Iteration[0750/0782] - Loss value: 0.7279273867607117\n",
      "Epoch[00/01] | Iteration[0751/0782] - Loss value: 1.0568702220916748\n",
      "Epoch[00/01] | Iteration[0752/0782] - Loss value: 0.6793195009231567\n",
      "Epoch[00/01] | Iteration[0753/0782] - Loss value: 0.7984551191329956\n",
      "Epoch[00/01] | Iteration[0754/0782] - Loss value: 0.893115222454071\n",
      "Epoch[00/01] | Iteration[0755/0782] - Loss value: 0.7304772138595581\n",
      "Epoch[00/01] | Iteration[0756/0782] - Loss value: 0.8375009894371033\n",
      "Epoch[00/01] | Iteration[0757/0782] - Loss value: 0.7736523747444153\n",
      "Epoch[00/01] | Iteration[0758/0782] - Loss value: 0.6990271806716919\n",
      "Epoch[00/01] | Iteration[0759/0782] - Loss value: 0.8158426880836487\n",
      "Epoch[00/01] | Iteration[0760/0782] - Loss value: 0.7308855652809143\n",
      "Epoch[00/01] | Iteration[0761/0782] - Loss value: 0.8199225664138794\n",
      "Epoch[00/01] | Iteration[0762/0782] - Loss value: 0.7360551953315735\n",
      "Epoch[00/01] | Iteration[0763/0782] - Loss value: 0.8255448341369629\n",
      "Epoch[00/01] | Iteration[0764/0782] - Loss value: 0.740161120891571\n",
      "Epoch[00/01] | Iteration[0765/0782] - Loss value: 0.7483453154563904\n",
      "Epoch[00/01] | Iteration[0766/0782] - Loss value: 0.6066810488700867\n",
      "Epoch[00/01] | Iteration[0767/0782] - Loss value: 0.9220056533813477\n",
      "Epoch[00/01] | Iteration[0768/0782] - Loss value: 0.7286603450775146\n",
      "Epoch[00/01] | Iteration[0769/0782] - Loss value: 0.5713376402854919\n",
      "Epoch[00/01] | Iteration[0770/0782] - Loss value: 0.8801165223121643\n",
      "Epoch[00/01] | Iteration[0771/0782] - Loss value: 0.8306559324264526\n",
      "Epoch[00/01] | Iteration[0772/0782] - Loss value: 0.8034238815307617\n",
      "Epoch[00/01] | Iteration[0773/0782] - Loss value: 0.7353503108024597\n",
      "Epoch[00/01] | Iteration[0774/0782] - Loss value: 0.5857791304588318\n",
      "Epoch[00/01] | Iteration[0775/0782] - Loss value: 0.8750259876251221\n",
      "Epoch[00/01] | Iteration[0776/0782] - Loss value: 0.7324784994125366\n",
      "Epoch[00/01] | Iteration[0777/0782] - Loss value: 0.698982834815979\n",
      "Epoch[00/01] | Iteration[0778/0782] - Loss value: 0.7730395793914795\n",
      "Epoch[00/01] | Iteration[0779/0782] - Loss value: 0.7433878183364868\n",
      "Epoch[00/01] | Iteration[0780/0782] - Loss value: 0.6683834195137024\n",
      "Epoch[00/01] | Iteration[0781/0782] - Loss value: 0.7158364057540894\n",
      "Epoch[00/01] - Validation accuracy:  71.37%\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1\n",
    "fit(device, train_loader, valid_loader, network, loss_fn, optimiser, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887dfe6e",
   "metadata": {},
   "source": [
    "<a id='sec:float2fake'></a>\n",
    "## Training QNNs with the PACT algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a17799",
   "metadata": {},
   "source": [
    "Whenever possible, we prefer to avoid the time-consuming part of machine learning, which is the training process.\n",
    "In the ML research field, it is common practice to start working from pre-trained models whenever possible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c1fa3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vgg_dir_logs() -> os.PathLike:\n",
    "    \"\"\"Create a directory where PyTorch checkpoints of the VGG network can be saved.\"\"\"\n",
    "    dir_logs = os.path.join(os.curdir, 'logs_vgg')\n",
    "    if not os.path.isdir(dir_logs):\n",
    "        os.makedirs(dir_logs, exist_ok=True)\n",
    "        \n",
    "    return dir_logs\n",
    "\n",
    "\n",
    "def load_ckpt(network: nn.Module, device: torch.device, dir_logs: os.PathLike, ckpt_name: str) -> None:\n",
    "    ckpt_file = os.path.join(dir_logs, ckpt_name)\n",
    "    ckpt      = torch.load(ckpt_file, map_location=device)\n",
    "    network.load_state_dict(ckpt)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11943700",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = VGG('VGG9')\n",
    "network = network.to(device=device)\n",
    "\n",
    "dir_logs = create_vgg_dir_logs()\n",
    "load_ckpt(network, device, dir_logs, 'pretrained_vgg9_fp.ckpt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f9aab7",
   "metadata": {},
   "source": [
    "The pre-trained model that we just loaded has been trained on CIFAR-10 data points which were normalised, i.e., transformed in such a way that the mean of the pixel components over all the training set is zero and their variance is one. If we run our model on the points provided by the current `Dataset` objects, we can see that the performance is not great.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b65e3fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(device:       torch.device,\n",
    "             valid_loader: torch.utils.data.DataLoader,\n",
    "             network:      nn.Module) -> None:\n",
    "\n",
    "    network.eval()\n",
    "    correct = 0\n",
    "\n",
    "    for x, y_gt_int in valid_loader:\n",
    "        \n",
    "        x        = x.to(device=device)\n",
    "        y_gt_int = y_gt_int.to(device=device)\n",
    "        \n",
    "        y_pr     = network(x)\n",
    "        y_pr_int = y_pr.argmax(axis=1)\n",
    "        \n",
    "        correct += torch.sum(y_pr_int == y_gt_int).item()\n",
    "    \n",
    "    print(\"Validation accuracy: {:6.2f}%\".format(100.0 * correct / len(valid_loader.dataset)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a60b78bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy:  38.23%\n"
     ]
    }
   ],
   "source": [
    "validate(device, valid_loader, network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cabd9da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CIFAR10_STATS_NORMALISE = \\\n",
    "{\n",
    "    'mean': (0.4914, 0.4822, 0.4465),\n",
    "    'std':  (0.2470, 0.2430, 0.2610)\n",
    "}\n",
    "\n",
    "\n",
    "def add_normalisation_transform(data_loader: torch.utils.data.DataLoader) -> None:\n",
    "    \n",
    "    transform_list  = []\n",
    "    transform_list += [data_loader.dataset.transform]\n",
    "    transform_list += [transforms.Normalize(**CIFAR10_STATS_NORMALISE)]\n",
    "    \n",
    "    data_loader.dataset.transform = transforms.Compose(transform_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c286663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy:  93.05%\n"
     ]
    }
   ],
   "source": [
    "add_normalisation_transform(train_loader)\n",
    "add_normalisation_transform(valid_loader)\n",
    "\n",
    "validate(device, valid_loader, network)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd3004a",
   "metadata": {},
   "source": [
    "This performance is much more in line with what we would expect from a pre-trained network solving CIFAR-10.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0017f1d",
   "metadata": {},
   "source": [
    "We are now ready to introduce the quantisation problem.\n",
    "`quantlib` is a component of a two-part tool designed to experiment with quantised neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d2ac64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import quantlib.editing.lightweight as qlw\n",
    "\n",
    "\n",
    "def all_pact_f2f_recipe(network: nn.Module, name2config: Dict[str, Dict]) -> nn.Module:\n",
    "\n",
    "    lwg = qlw.LightweightGraph(network)\n",
    "    name2type = {n.name: n.module.__class__.__name__ for n in lwg.nodes_list}\n",
    "\n",
    "    # generate lightweight (i.e., atomic) replacement rules\n",
    "    assert set(name2config.keys()).issubset(set(name2type.keys()))\n",
    "    type2rule = \\\n",
    "    {\n",
    "        'Conv2d': qlw.rules.pact.ReplaceConvLinearPACTRule,\n",
    "        'Linear': qlw.rules.pact.ReplaceConvLinearPACTRule,\n",
    "        'ReLU':   qlw.rules.pact.ReplaceActPACTRule\n",
    "    }\n",
    "    rhos = list(map(lambda n: type2rule[name2type[n]](qlw.rules.NameFilter(n), **name2config[n]), name2config.keys()))\n",
    "    \n",
    "    # boot lightweight editor and apply atomic rules\n",
    "    lwe = qlw.LightweightEditor(lwg)\n",
    "    lwe.startup()\n",
    "    for rho in rhos:\n",
    "        lwe.set_lwr(rho)\n",
    "        lwe.apply()\n",
    "    lwe.shutdown()\n",
    "    \n",
    "    return lwe.graph.net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a78b276",
   "metadata": {},
   "source": [
    "To simplify the exploration of different quantisation policies, we aim at defining the configurations of quantised nodes in a programmatic way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ffd6470f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def all_pact_create_configs(network: nn.Module, patches: Dict[str, Dict]) -> Dict[str, Dict]:\n",
    "\n",
    "    lwg = qlw.LightweightGraph(network)\n",
    "    conv2d_nodes = set([n.name for n in lwg.nodes_list if n.module.__class__.__name__ == 'Conv2d'])\n",
    "    linear_nodes = set([n.name for n in lwg.nodes_list if n.module.__class__.__name__ == 'Linear'])\n",
    "    relu_nodes   = set([n.name for n in lwg.nodes_list if n.module.__class__.__name__ == 'ReLU'])\n",
    "    assert set(patches.keys()).issubset(conv2d_nodes | linear_nodes | relu_nodes)\n",
    "\n",
    "    # configure convolutional nodes\n",
    "    conv2d_default = \\\n",
    "    {\n",
    "        'quantize':   'per_channel',\n",
    "        'init_clip':  'sawb_asymm',\n",
    "        'learn_clip': False,\n",
    "        'symm_wts':   True,\n",
    "        'tqt':        False,\n",
    "        'n_levels':   4\n",
    "    }\n",
    "\n",
    "    conv2d_config = defaultdict(lambda: conv2d_default.copy())  # it is EXTREMELY important that we return a copy of the dictionary and not the dictionary itself; otherwise, all configs would point to the same (possibly updated) object\n",
    "    for n in conv2d_nodes:\n",
    "        conv2d_config[n].update(patches[n] if n in patches.keys() else {})  # patches have higher priority than default configurations\n",
    "\n",
    "    # configure linear nodes\n",
    "    linear_default = \\\n",
    "    {\n",
    "        'quantize':   'per_layer',\n",
    "        'init_clip':  'sawb_asymm',\n",
    "        'learn_clip': False,\n",
    "        'symm_wts':   True,\n",
    "        'tqt':        False,\n",
    "        'n_levels':   4\n",
    "    }\n",
    "\n",
    "    linear_config = defaultdict(lambda: linear_default.copy())\n",
    "    for n in linear_nodes:\n",
    "        linear_config[n].update(patches[n] if n in patches.keys() else {})  # patches have higher priority than default configurations\n",
    "\n",
    "    # configure ReLU nodes\n",
    "    relu_default = \\\n",
    "    {\n",
    "        'init_clip':  'std',\n",
    "        'learn_clip': True,\n",
    "        'nb_std':     3,\n",
    "        'rounding':   False,\n",
    "        'tqt':        False,\n",
    "        'n_levels':   4\n",
    "    }\n",
    "\n",
    "    relu_config = defaultdict(lambda: relu_default.copy())\n",
    "    for n in relu_nodes:\n",
    "        relu_config[n].update(patches[n] if n in patches.keys() else {})  # patches have higher priority than default configurations\n",
    "\n",
    "    # create complete configuration\n",
    "    config = {**conv2d_config, **linear_config, **relu_config}\n",
    "\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2e5917",
   "metadata": {},
   "source": [
    "We can see that the default configuration for each PACT node sets $2^{2} = 4$ quantisation levels, in such a way that the operands can be encoded using only two bits.\n",
    "\n",
    "However, the pixels of RGB images are encoded using three 8-bit bytes each.\n",
    "Moreover, both theoretical and experimental research on QNNs has shown that keeping the last layers operands at high precision benefits accuracy.\n",
    "For these reasons, we increase the precision of the weights of the first convolutional node, of the last feature array, and of the weights of the last linear node to $2^{8} = 256$ quantisation levels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e68d990f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (pilot): Sequential(\n",
       "    (0): PACTConv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, n_levels=256, quantize='per_channel', init_clip='sawb_asymm', learn_clip=False, symm_wts=True, nb_std=3, tqt=False, tqt_beta=0.90, tqt_clip_grad=True)\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): PACTUnsignedAct(n_levels=4, init_clip='std', learn_clip=True, act_kind='relu', leaky=0.1, nb_std=3, tqt=False, tqt_beta=0.90, tqt_clip_grad=True)\n",
       "  )\n",
       "  (features): Sequential(\n",
       "    (0): PACTConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, n_levels=4, quantize='per_channel', init_clip='sawb_asymm', learn_clip=False, symm_wts=True, nb_std=3, tqt=False, tqt_beta=0.90, tqt_clip_grad=True)\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): PACTUnsignedAct(n_levels=4, init_clip='std', learn_clip=True, act_kind='relu', leaky=0.1, nb_std=3, tqt=False, tqt_beta=0.90, tqt_clip_grad=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): PACTConv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, n_levels=4, quantize='per_channel', init_clip='sawb_asymm', learn_clip=False, symm_wts=True, nb_std=3, tqt=False, tqt_beta=0.90, tqt_clip_grad=True)\n",
       "    (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): PACTUnsignedAct(n_levels=4, init_clip='std', learn_clip=True, act_kind='relu', leaky=0.1, nb_std=3, tqt=False, tqt_beta=0.90, tqt_clip_grad=True)\n",
       "    (7): PACTConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, n_levels=4, quantize='per_channel', init_clip='sawb_asymm', learn_clip=False, symm_wts=True, nb_std=3, tqt=False, tqt_beta=0.90, tqt_clip_grad=True)\n",
       "    (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): PACTUnsignedAct(n_levels=4, init_clip='std', learn_clip=True, act_kind='relu', leaky=0.1, nb_std=3, tqt=False, tqt_beta=0.90, tqt_clip_grad=True)\n",
       "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (11): PACTConv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, n_levels=4, quantize='per_channel', init_clip='sawb_asymm', learn_clip=False, symm_wts=True, nb_std=3, tqt=False, tqt_beta=0.90, tqt_clip_grad=True)\n",
       "    (12): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (13): PACTUnsignedAct(n_levels=4, init_clip='std', learn_clip=True, act_kind='relu', leaky=0.1, nb_std=3, tqt=False, tqt_beta=0.90, tqt_clip_grad=True)\n",
       "    (14): PACTConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, n_levels=4, quantize='per_channel', init_clip='sawb_asymm', learn_clip=False, symm_wts=True, nb_std=3, tqt=False, tqt_beta=0.90, tqt_clip_grad=True)\n",
       "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): PACTUnsignedAct(n_levels=4, init_clip='std', learn_clip=True, act_kind='relu', leaky=0.1, nb_std=3, tqt=False, tqt_beta=0.90, tqt_clip_grad=True)\n",
       "    (17): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(4, 4))\n",
       "  (classifier): Sequential(\n",
       "    (0): PACTLinear(in_features=8192, out_features=1024, bias=False, n_levels=4, quantize='per_layer', init_clip='sawb_asymm', learn_clip=False, symm_wts=True, nb_std=3, tqt=False, tqt_beta=0.90, tqt_clip_grad=True)\n",
       "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): PACTUnsignedAct(n_levels=4, init_clip='std', learn_clip=True, act_kind='relu', leaky=0.1, nb_std=3, tqt=False, tqt_beta=0.90, tqt_clip_grad=True)\n",
       "    (3): PACTLinear(in_features=1024, out_features=1024, bias=False, n_levels=4, quantize='per_layer', init_clip='sawb_asymm', learn_clip=False, symm_wts=True, nb_std=3, tqt=False, tqt_beta=0.90, tqt_clip_grad=True)\n",
       "    (4): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): PACTUnsignedAct(n_levels=256, init_clip='std', learn_clip=True, act_kind='relu', leaky=0.1, nb_std=3, tqt=False, tqt_beta=0.90, tqt_clip_grad=True)\n",
       "    (6): PACTLinear(in_features=1024, out_features=10, bias=True, n_levels=256, quantize='per_layer', init_clip='sawb_asymm', learn_clip=False, symm_wts=True, nb_std=3, tqt=False, tqt_beta=0.90, tqt_clip_grad=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create configuration for PACT float-to-fake conversion\n",
    "patches = \\\n",
    "{\n",
    "    'pilot.0':      {'n_levels': 256},\n",
    "    'classifier.5': {'n_levels': 256},\n",
    "    'classifier.6': {'n_levels': 256}\n",
    "}\n",
    "name2config = all_pact_create_configs(network, patches)\n",
    "\n",
    "# apply PACT float-to-fake conversion\n",
    "pact_network = all_pact_f2f_recipe(network, name2config)\n",
    "pact_network.to(device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df4830b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import quantlib.algorithms as qa\n",
    "\n",
    "\n",
    "def all_ana_get_controllers(network:         nn.Module,\n",
    "                            schedule_linear: Dict[int, Union[str, List[str]]],\n",
    "                            schedule_act:    Dict[int, Union[str, List[str]]],\n",
    "                            kwargs_linear:   Dict = {},\n",
    "                            kwargs_act:      Dict = {}) -> Tuple[qa.pact.PACTLinearController, qa.pact.PACTActController]:\n",
    "    \n",
    "    modules_linear    = qa.pact.PACTLinearController.get_modules(network)\n",
    "    controller_linear = qa.pact.PACTLinearController(modules_linear, schedule_linear, **kwargs_linear)\n",
    "    \n",
    "    modules_act    = qa.pact.PACTActController.get_modules(network)\n",
    "    controller_act = qa.pact.PACTActController(modules_act, schedule_act, **kwargs_act)\n",
    "\n",
    "    return controller_linear, controller_act\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07d4d992",
   "metadata": {},
   "outputs": [],
   "source": [
    "pact_schedule_linear = \\\n",
    "{\n",
    "    0: ['verbose_on', 'start']\n",
    "}\n",
    "\n",
    "pact_schedule_act = \\\n",
    "{\n",
    "    0: 'verbose_on',\n",
    "    0: 'start'\n",
    "}\n",
    "\n",
    "pact_controller_linear, pact_controller_act = all_ana_get_controllers(pact_network, pact_schedule_linear, pact_schedule_act)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e8342c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn   = nn.CrossEntropyLoss()\n",
    "optimiser = qa.pact.PACTAdam(pact_network, pact_decay=0.001, lr=0.004)\n",
    "lr_sched  = optim.lr_scheduler.CosineAnnealingLR(optimiser, T_max=250, eta_min=0.00001)  # in this 'LRScheduler', the value of 'T_max' should be set equal to the number of epochs for which you plan to train the network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "42f18f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_quantised(device:          torch.device,\n",
    "                  train_loader:    torch.utils.data.DataLoader,\n",
    "                  valid_loader:    torch.utils.data.DataLoader,\n",
    "                  network:         nn.Module,\n",
    "                  qnt_controllers: List[Union[qa.pact.PACTLinearController, qa.pact.PACTActController]],\n",
    "                  loss_fn:         nn.Module,\n",
    "                  optimiser:       torch.optim.Optimizer,\n",
    "                  lr_sched:        torch.optim.lr_scheduler._LRScheduler,\n",
    "                  n_epochs:        int) -> None:\n",
    "    \n",
    "    for i_epoch in range(0, n_epochs):\n",
    "        \n",
    "        # training pass\n",
    "        network.train()\n",
    "        optimiser.zero_grad()\n",
    "        \n",
    "        for qnt_ctrl in qnt_controllers:\n",
    "            qnt_ctrl.step_pre_training_epoch(i_epoch)  # NOTE: the hyper-parameters of the QAT algorithm could be updated at each epoch\n",
    "        \n",
    "        for i_batch, (x, y_gt_int) in enumerate(train_loader):\n",
    "            \n",
    "            x        = x.to(device=device)\n",
    "            y_gt_int = y_gt_int.to(device=device)\n",
    "            \n",
    "            y_pr       = network(x)\n",
    "            loss_value = loss_fn(y_pr, y_gt_int)\n",
    "            \n",
    "            loss_value.backward()\n",
    "            optimiser.step()\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "            print(\"Epoch[{:02d}/{:02d}] | Iteration[{:04d}/{:04d}] - Loss value: {}\".format(i_epoch, n_epochs, i_batch, len(train_loader), loss_value.item()))\n",
    "\n",
    "        lr_sched.step()\n",
    "        \n",
    "        # validation pass\n",
    "        network.eval()\n",
    "        correct = 0\n",
    "        \n",
    "        for qnt_ctrl in qnt_controllers:\n",
    "            qnt_ctrl.step_pre_validation_epoch(i_epoch)  # NOTE: the hyper-parameters of the QAT algorithm could be updated at each epoch\n",
    "        \n",
    "        for x, y_gt_int in valid_loader:\n",
    "            \n",
    "            x        = x.to(device=device)\n",
    "            y_gt_int = y_gt_int.to(device=device)\n",
    "            \n",
    "            y_pr     = network(x)\n",
    "            y_pr_int = y_pr.argmax(axis=1)\n",
    "            \n",
    "            correct += torch.sum(y_pr_int == y_gt_int).item()\n",
    "\n",
    "        print(\"Epoch[{:02d}/{:02d}] - Validation accuracy: {:6.2f}%\".format(i_epoch, n_epochs, 100.0 * correct / len(valid_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "292f0440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PACTLinearController]    Verbose mode enabled!\n",
      "[PACTLinearController]    Epoch 0 - running command start\n",
      "[PACTLinearController]    Started quantization!\n",
      "Epoch[00/01] | Iteration[0000/0782] - Loss value: 1.3649822473526\n",
      "Epoch[00/01] | Iteration[0001/0782] - Loss value: 1.3864147663116455\n",
      "Epoch[00/01] | Iteration[0002/0782] - Loss value: 1.2676969766616821\n",
      "Epoch[00/01] | Iteration[0003/0782] - Loss value: 0.7712691426277161\n",
      "Epoch[00/01] | Iteration[0004/0782] - Loss value: 1.0890517234802246\n",
      "Epoch[00/01] | Iteration[0005/0782] - Loss value: 1.2165560722351074\n",
      "Epoch[00/01] | Iteration[0006/0782] - Loss value: 0.7119269371032715\n",
      "Epoch[00/01] | Iteration[0007/0782] - Loss value: 1.4478610754013062\n",
      "Epoch[00/01] | Iteration[0008/0782] - Loss value: 1.080918788909912\n",
      "Epoch[00/01] | Iteration[0009/0782] - Loss value: 0.3377886712551117\n",
      "Epoch[00/01] | Iteration[0010/0782] - Loss value: 0.4388735592365265\n",
      "Epoch[00/01] | Iteration[0011/0782] - Loss value: 0.9364312887191772\n",
      "Epoch[00/01] | Iteration[0012/0782] - Loss value: 0.6538751721382141\n",
      "Epoch[00/01] | Iteration[0013/0782] - Loss value: 1.5484079122543335\n",
      "Epoch[00/01] | Iteration[0014/0782] - Loss value: 0.562757670879364\n",
      "Epoch[00/01] | Iteration[0015/0782] - Loss value: 0.8551536202430725\n",
      "Epoch[00/01] | Iteration[0016/0782] - Loss value: 1.173814296722412\n",
      "Epoch[00/01] | Iteration[0017/0782] - Loss value: 0.8720769286155701\n",
      "Epoch[00/01] | Iteration[0018/0782] - Loss value: 0.449607789516449\n",
      "Epoch[00/01] | Iteration[0019/0782] - Loss value: 1.5704561471939087\n",
      "Epoch[00/01] | Iteration[0020/0782] - Loss value: 0.6626887917518616\n",
      "Epoch[00/01] | Iteration[0021/0782] - Loss value: 2.1298389434814453\n",
      "Epoch[00/01] | Iteration[0022/0782] - Loss value: 0.8328819274902344\n",
      "Epoch[00/01] | Iteration[0023/0782] - Loss value: 0.5273377895355225\n",
      "Epoch[00/01] | Iteration[0024/0782] - Loss value: 0.8008001446723938\n",
      "Epoch[00/01] | Iteration[0025/0782] - Loss value: 0.9414742588996887\n",
      "Epoch[00/01] | Iteration[0026/0782] - Loss value: 0.4543400704860687\n",
      "Epoch[00/01] | Iteration[0027/0782] - Loss value: 0.9410673975944519\n",
      "Epoch[00/01] | Iteration[0028/0782] - Loss value: 0.5429941415786743\n",
      "Epoch[00/01] | Iteration[0029/0782] - Loss value: 0.5783135890960693\n",
      "Epoch[00/01] | Iteration[0030/0782] - Loss value: 0.6929391622543335\n",
      "Epoch[00/01] | Iteration[0031/0782] - Loss value: 0.7861203551292419\n",
      "Epoch[00/01] | Iteration[0032/0782] - Loss value: 0.6895595192909241\n",
      "Epoch[00/01] | Iteration[0033/0782] - Loss value: 0.49025624990463257\n",
      "Epoch[00/01] | Iteration[0034/0782] - Loss value: 1.2484335899353027\n",
      "Epoch[00/01] | Iteration[0035/0782] - Loss value: 1.0573632717132568\n",
      "Epoch[00/01] | Iteration[0036/0782] - Loss value: 0.5296262502670288\n",
      "Epoch[00/01] | Iteration[0037/0782] - Loss value: 0.4842369258403778\n",
      "Epoch[00/01] | Iteration[0038/0782] - Loss value: 0.5283074975013733\n",
      "Epoch[00/01] | Iteration[0039/0782] - Loss value: 0.33130356669425964\n",
      "Epoch[00/01] | Iteration[0040/0782] - Loss value: 0.4640355110168457\n",
      "Epoch[00/01] | Iteration[0041/0782] - Loss value: 0.7399927377700806\n",
      "Epoch[00/01] | Iteration[0042/0782] - Loss value: 0.7502328157424927\n",
      "Epoch[00/01] | Iteration[0043/0782] - Loss value: 0.5770150423049927\n",
      "Epoch[00/01] | Iteration[0044/0782] - Loss value: 0.6637421250343323\n",
      "Epoch[00/01] | Iteration[0045/0782] - Loss value: 0.7101256847381592\n",
      "Epoch[00/01] | Iteration[0046/0782] - Loss value: 0.4830147325992584\n",
      "Epoch[00/01] | Iteration[0047/0782] - Loss value: 0.8299443125724792\n",
      "Epoch[00/01] | Iteration[0048/0782] - Loss value: 0.36582860350608826\n",
      "Epoch[00/01] | Iteration[0049/0782] - Loss value: 0.618636429309845\n",
      "Epoch[00/01] | Iteration[0050/0782] - Loss value: 0.847808301448822\n",
      "Epoch[00/01] | Iteration[0051/0782] - Loss value: 0.9718819856643677\n",
      "Epoch[00/01] | Iteration[0052/0782] - Loss value: 0.5466440916061401\n",
      "Epoch[00/01] | Iteration[0053/0782] - Loss value: 0.12331748753786087\n",
      "Epoch[00/01] | Iteration[0054/0782] - Loss value: 0.4845571517944336\n",
      "Epoch[00/01] | Iteration[0055/0782] - Loss value: 1.4746549129486084\n",
      "Epoch[00/01] | Iteration[0056/0782] - Loss value: 0.579073429107666\n",
      "Epoch[00/01] | Iteration[0057/0782] - Loss value: 0.24696321785449982\n",
      "Epoch[00/01] | Iteration[0058/0782] - Loss value: 0.4836690127849579\n",
      "Epoch[00/01] | Iteration[0059/0782] - Loss value: 0.5310434699058533\n",
      "Epoch[00/01] | Iteration[0060/0782] - Loss value: 0.5423176288604736\n",
      "Epoch[00/01] | Iteration[0061/0782] - Loss value: 0.4912475347518921\n",
      "Epoch[00/01] | Iteration[0062/0782] - Loss value: 0.25823715329170227\n",
      "Epoch[00/01] | Iteration[0063/0782] - Loss value: 1.019940733909607\n",
      "Epoch[00/01] | Iteration[0064/0782] - Loss value: 0.4288502335548401\n",
      "Epoch[00/01] | Iteration[0065/0782] - Loss value: 0.33470970392227173\n",
      "Epoch[00/01] | Iteration[0066/0782] - Loss value: 0.1752747893333435\n",
      "Epoch[00/01] | Iteration[0067/0782] - Loss value: 0.47916340827941895\n",
      "Epoch[00/01] | Iteration[0068/0782] - Loss value: 0.4315550625324249\n",
      "Epoch[00/01] | Iteration[0069/0782] - Loss value: 0.4896458387374878\n",
      "Epoch[00/01] | Iteration[0070/0782] - Loss value: 0.9495306015014648\n",
      "Epoch[00/01] | Iteration[0071/0782] - Loss value: 0.6216250658035278\n",
      "Epoch[00/01] | Iteration[0072/0782] - Loss value: 0.8384654521942139\n",
      "Epoch[00/01] | Iteration[0073/0782] - Loss value: 0.2140299379825592\n",
      "Epoch[00/01] | Iteration[0074/0782] - Loss value: 0.46984362602233887\n",
      "Epoch[00/01] | Iteration[0075/0782] - Loss value: 0.5210368037223816\n",
      "Epoch[00/01] | Iteration[0076/0782] - Loss value: 0.8760862946510315\n",
      "Epoch[00/01] | Iteration[0077/0782] - Loss value: 0.5729584693908691\n",
      "Epoch[00/01] | Iteration[0078/0782] - Loss value: 0.3137483596801758\n",
      "Epoch[00/01] | Iteration[0079/0782] - Loss value: 0.583249032497406\n",
      "Epoch[00/01] | Iteration[0080/0782] - Loss value: 0.2844347655773163\n",
      "Epoch[00/01] | Iteration[0081/0782] - Loss value: 0.49106380343437195\n",
      "Epoch[00/01] | Iteration[0082/0782] - Loss value: 0.5903195738792419\n",
      "Epoch[00/01] | Iteration[0083/0782] - Loss value: 0.2683943808078766\n",
      "Epoch[00/01] | Iteration[0084/0782] - Loss value: 0.19657108187675476\n",
      "Epoch[00/01] | Iteration[0085/0782] - Loss value: 0.297944039106369\n",
      "Epoch[00/01] | Iteration[0086/0782] - Loss value: 0.3263314962387085\n",
      "Epoch[00/01] | Iteration[0087/0782] - Loss value: 0.16135980188846588\n",
      "Epoch[00/01] | Iteration[0088/0782] - Loss value: 0.30997249484062195\n",
      "Epoch[00/01] | Iteration[0089/0782] - Loss value: 0.18759480118751526\n",
      "Epoch[00/01] | Iteration[0090/0782] - Loss value: 0.23503606021404266\n",
      "Epoch[00/01] | Iteration[0091/0782] - Loss value: 0.42781418561935425\n",
      "Epoch[00/01] | Iteration[0092/0782] - Loss value: 0.18077993392944336\n",
      "Epoch[00/01] | Iteration[0093/0782] - Loss value: 0.33536067605018616\n",
      "Epoch[00/01] | Iteration[0094/0782] - Loss value: 0.29657724499702454\n",
      "Epoch[00/01] | Iteration[0095/0782] - Loss value: 0.8300204873085022\n",
      "Epoch[00/01] | Iteration[0096/0782] - Loss value: 0.17915558815002441\n",
      "Epoch[00/01] | Iteration[0097/0782] - Loss value: 0.06859508156776428\n",
      "Epoch[00/01] | Iteration[0098/0782] - Loss value: 0.19454120099544525\n",
      "Epoch[00/01] | Iteration[0099/0782] - Loss value: 0.515330970287323\n",
      "Epoch[00/01] | Iteration[0100/0782] - Loss value: 0.12372564524412155\n",
      "Epoch[00/01] | Iteration[0101/0782] - Loss value: 0.5608578324317932\n",
      "Epoch[00/01] | Iteration[0102/0782] - Loss value: 0.2492336928844452\n",
      "Epoch[00/01] | Iteration[0103/0782] - Loss value: 0.28345033526420593\n",
      "Epoch[00/01] | Iteration[0104/0782] - Loss value: 0.4429648816585541\n",
      "Epoch[00/01] | Iteration[0105/0782] - Loss value: 0.1648997664451599\n",
      "Epoch[00/01] | Iteration[0106/0782] - Loss value: 0.2922458052635193\n",
      "Epoch[00/01] | Iteration[0107/0782] - Loss value: 0.4052785634994507\n",
      "Epoch[00/01] | Iteration[0108/0782] - Loss value: 0.38445109128952026\n",
      "Epoch[00/01] | Iteration[0109/0782] - Loss value: 0.277225524187088\n",
      "Epoch[00/01] | Iteration[0110/0782] - Loss value: 0.5654721856117249\n",
      "Epoch[00/01] | Iteration[0111/0782] - Loss value: 0.1755971759557724\n",
      "Epoch[00/01] | Iteration[0112/0782] - Loss value: 0.23848922550678253\n",
      "Epoch[00/01] | Iteration[0113/0782] - Loss value: 0.6228685975074768\n",
      "Epoch[00/01] | Iteration[0114/0782] - Loss value: 0.2372252345085144\n",
      "Epoch[00/01] | Iteration[0115/0782] - Loss value: 0.3360671401023865\n",
      "Epoch[00/01] | Iteration[0116/0782] - Loss value: 0.2816767692565918\n",
      "Epoch[00/01] | Iteration[0117/0782] - Loss value: 0.3361029028892517\n",
      "Epoch[00/01] | Iteration[0118/0782] - Loss value: 0.1910710632801056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[00/01] | Iteration[0119/0782] - Loss value: 0.20198634266853333\n",
      "Epoch[00/01] | Iteration[0120/0782] - Loss value: 0.40961214900016785\n",
      "Epoch[00/01] | Iteration[0121/0782] - Loss value: 0.21836911141872406\n",
      "Epoch[00/01] | Iteration[0122/0782] - Loss value: 0.6723942160606384\n",
      "Epoch[00/01] | Iteration[0123/0782] - Loss value: 0.3189288377761841\n",
      "Epoch[00/01] | Iteration[0124/0782] - Loss value: 0.22710178792476654\n",
      "Epoch[00/01] | Iteration[0125/0782] - Loss value: 0.35605552792549133\n",
      "Epoch[00/01] | Iteration[0126/0782] - Loss value: 0.18888449668884277\n",
      "Epoch[00/01] | Iteration[0127/0782] - Loss value: 0.07583968341350555\n",
      "Epoch[00/01] | Iteration[0128/0782] - Loss value: 0.07357415556907654\n",
      "Epoch[00/01] | Iteration[0129/0782] - Loss value: 0.7157891392707825\n",
      "Epoch[00/01] | Iteration[0130/0782] - Loss value: 0.4664106070995331\n",
      "Epoch[00/01] | Iteration[0131/0782] - Loss value: 0.37695640325546265\n",
      "Epoch[00/01] | Iteration[0132/0782] - Loss value: 0.669472873210907\n",
      "Epoch[00/01] | Iteration[0133/0782] - Loss value: 0.41690272092819214\n",
      "Epoch[00/01] | Iteration[0134/0782] - Loss value: 0.6119940876960754\n",
      "Epoch[00/01] | Iteration[0135/0782] - Loss value: 0.29567524790763855\n",
      "Epoch[00/01] | Iteration[0136/0782] - Loss value: 0.3231891393661499\n",
      "Epoch[00/01] | Iteration[0137/0782] - Loss value: 0.570820689201355\n",
      "Epoch[00/01] | Iteration[0138/0782] - Loss value: 0.37358394265174866\n",
      "Epoch[00/01] | Iteration[0139/0782] - Loss value: 0.3827842175960541\n",
      "Epoch[00/01] | Iteration[0140/0782] - Loss value: 0.6429774761199951\n",
      "Epoch[00/01] | Iteration[0141/0782] - Loss value: 0.22344042360782623\n",
      "Epoch[00/01] | Iteration[0142/0782] - Loss value: 0.02304682508111\n",
      "Epoch[00/01] | Iteration[0143/0782] - Loss value: 0.06231074407696724\n",
      "Epoch[00/01] | Iteration[0144/0782] - Loss value: 0.1523997187614441\n",
      "Epoch[00/01] | Iteration[0145/0782] - Loss value: 0.13144217431545258\n",
      "Epoch[00/01] | Iteration[0146/0782] - Loss value: 0.4565020501613617\n",
      "Epoch[00/01] | Iteration[0147/0782] - Loss value: 0.3042319416999817\n",
      "Epoch[00/01] | Iteration[0148/0782] - Loss value: 0.21223662793636322\n",
      "Epoch[00/01] | Iteration[0149/0782] - Loss value: 0.6645030379295349\n",
      "Epoch[00/01] | Iteration[0150/0782] - Loss value: 0.4324384033679962\n",
      "Epoch[00/01] | Iteration[0151/0782] - Loss value: 0.40717729926109314\n",
      "Epoch[00/01] | Iteration[0152/0782] - Loss value: 0.39008545875549316\n",
      "Epoch[00/01] | Iteration[0153/0782] - Loss value: 0.28198927640914917\n",
      "Epoch[00/01] | Iteration[0154/0782] - Loss value: 0.350637286901474\n",
      "Epoch[00/01] | Iteration[0155/0782] - Loss value: 0.4193076491355896\n",
      "Epoch[00/01] | Iteration[0156/0782] - Loss value: 0.10941074043512344\n",
      "Epoch[00/01] | Iteration[0157/0782] - Loss value: 0.29641416668891907\n",
      "Epoch[00/01] | Iteration[0158/0782] - Loss value: 0.3270634412765503\n",
      "Epoch[00/01] | Iteration[0159/0782] - Loss value: 0.6812766194343567\n",
      "Epoch[00/01] | Iteration[0160/0782] - Loss value: 0.22885560989379883\n",
      "Epoch[00/01] | Iteration[0161/0782] - Loss value: 0.41816413402557373\n",
      "Epoch[00/01] | Iteration[0162/0782] - Loss value: 0.22749529778957367\n",
      "Epoch[00/01] | Iteration[0163/0782] - Loss value: 0.13296066224575043\n",
      "Epoch[00/01] | Iteration[0164/0782] - Loss value: 0.4055662453174591\n",
      "Epoch[00/01] | Iteration[0165/0782] - Loss value: 0.15977783501148224\n",
      "Epoch[00/01] | Iteration[0166/0782] - Loss value: 0.24624882638454437\n",
      "Epoch[00/01] | Iteration[0167/0782] - Loss value: 0.27502328157424927\n",
      "Epoch[00/01] | Iteration[0168/0782] - Loss value: 0.3049132525920868\n",
      "Epoch[00/01] | Iteration[0169/0782] - Loss value: 0.10732147842645645\n",
      "Epoch[00/01] | Iteration[0170/0782] - Loss value: 0.3369605243206024\n",
      "Epoch[00/01] | Iteration[0171/0782] - Loss value: 0.34491297602653503\n",
      "Epoch[00/01] | Iteration[0172/0782] - Loss value: 0.16230303049087524\n",
      "Epoch[00/01] | Iteration[0173/0782] - Loss value: 0.40060850977897644\n",
      "Epoch[00/01] | Iteration[0174/0782] - Loss value: 0.20639051496982574\n",
      "Epoch[00/01] | Iteration[0175/0782] - Loss value: 0.3819960951805115\n",
      "Epoch[00/01] | Iteration[0176/0782] - Loss value: 0.19266489148139954\n",
      "Epoch[00/01] | Iteration[0177/0782] - Loss value: 0.25140735507011414\n",
      "Epoch[00/01] | Iteration[0178/0782] - Loss value: 0.6591866612434387\n",
      "Epoch[00/01] | Iteration[0179/0782] - Loss value: 0.10303158313035965\n",
      "Epoch[00/01] | Iteration[0180/0782] - Loss value: 0.26097509264945984\n",
      "Epoch[00/01] | Iteration[0181/0782] - Loss value: 0.23391278088092804\n",
      "Epoch[00/01] | Iteration[0182/0782] - Loss value: 0.4433891177177429\n",
      "Epoch[00/01] | Iteration[0183/0782] - Loss value: 0.2772050201892853\n",
      "Epoch[00/01] | Iteration[0184/0782] - Loss value: 0.12155944108963013\n",
      "Epoch[00/01] | Iteration[0185/0782] - Loss value: 0.31483933329582214\n",
      "Epoch[00/01] | Iteration[0186/0782] - Loss value: 0.20420585572719574\n",
      "Epoch[00/01] | Iteration[0187/0782] - Loss value: 0.09077624976634979\n",
      "Epoch[00/01] | Iteration[0188/0782] - Loss value: 0.2922605574131012\n",
      "Epoch[00/01] | Iteration[0189/0782] - Loss value: 0.2191779762506485\n",
      "Epoch[00/01] | Iteration[0190/0782] - Loss value: 0.1288766860961914\n",
      "Epoch[00/01] | Iteration[0191/0782] - Loss value: 0.1893131136894226\n",
      "Epoch[00/01] | Iteration[0192/0782] - Loss value: 0.08258669823408127\n",
      "Epoch[00/01] | Iteration[0193/0782] - Loss value: 0.09898782521486282\n",
      "Epoch[00/01] | Iteration[0194/0782] - Loss value: 0.19915533065795898\n",
      "Epoch[00/01] | Iteration[0195/0782] - Loss value: 0.5049940347671509\n",
      "Epoch[00/01] | Iteration[0196/0782] - Loss value: 0.3156000077724457\n",
      "Epoch[00/01] | Iteration[0197/0782] - Loss value: 0.40577781200408936\n",
      "Epoch[00/01] | Iteration[0198/0782] - Loss value: 0.3216328024864197\n",
      "Epoch[00/01] | Iteration[0199/0782] - Loss value: 0.1305505335330963\n",
      "Epoch[00/01] | Iteration[0200/0782] - Loss value: 0.11901826411485672\n",
      "Epoch[00/01] | Iteration[0201/0782] - Loss value: 0.22237536311149597\n",
      "Epoch[00/01] | Iteration[0202/0782] - Loss value: 0.29605522751808167\n",
      "Epoch[00/01] | Iteration[0203/0782] - Loss value: 0.19191178679466248\n",
      "Epoch[00/01] | Iteration[0204/0782] - Loss value: 0.2637072503566742\n",
      "Epoch[00/01] | Iteration[0205/0782] - Loss value: 0.38745594024658203\n",
      "Epoch[00/01] | Iteration[0206/0782] - Loss value: 0.34772929549217224\n",
      "Epoch[00/01] | Iteration[0207/0782] - Loss value: 0.3037429451942444\n",
      "Epoch[00/01] | Iteration[0208/0782] - Loss value: 0.14593294262886047\n",
      "Epoch[00/01] | Iteration[0209/0782] - Loss value: 0.17176604270935059\n",
      "Epoch[00/01] | Iteration[0210/0782] - Loss value: 0.5832068920135498\n",
      "Epoch[00/01] | Iteration[0211/0782] - Loss value: 0.09149996936321259\n",
      "Epoch[00/01] | Iteration[0212/0782] - Loss value: 0.3149419128894806\n",
      "Epoch[00/01] | Iteration[0213/0782] - Loss value: 0.3213259279727936\n",
      "Epoch[00/01] | Iteration[0214/0782] - Loss value: 0.10727962851524353\n",
      "Epoch[00/01] | Iteration[0215/0782] - Loss value: 0.3534083664417267\n",
      "Epoch[00/01] | Iteration[0216/0782] - Loss value: 0.202043816447258\n",
      "Epoch[00/01] | Iteration[0217/0782] - Loss value: 0.2733381390571594\n",
      "Epoch[00/01] | Iteration[0218/0782] - Loss value: 0.15946780145168304\n",
      "Epoch[00/01] | Iteration[0219/0782] - Loss value: 0.1974635124206543\n",
      "Epoch[00/01] | Iteration[0220/0782] - Loss value: 0.46127617359161377\n",
      "Epoch[00/01] | Iteration[0221/0782] - Loss value: 0.35063207149505615\n",
      "Epoch[00/01] | Iteration[0222/0782] - Loss value: 0.3662204146385193\n",
      "Epoch[00/01] | Iteration[0223/0782] - Loss value: 0.24532532691955566\n",
      "Epoch[00/01] | Iteration[0224/0782] - Loss value: 0.23956114053726196\n",
      "Epoch[00/01] | Iteration[0225/0782] - Loss value: 0.15983447432518005\n",
      "Epoch[00/01] | Iteration[0226/0782] - Loss value: 0.1813906729221344\n",
      "Epoch[00/01] | Iteration[0227/0782] - Loss value: 0.24001038074493408\n",
      "Epoch[00/01] | Iteration[0228/0782] - Loss value: 0.4925879240036011\n",
      "Epoch[00/01] | Iteration[0229/0782] - Loss value: 0.2207547128200531\n",
      "Epoch[00/01] | Iteration[0230/0782] - Loss value: 0.12335167825222015\n",
      "Epoch[00/01] | Iteration[0231/0782] - Loss value: 0.28878894448280334\n",
      "Epoch[00/01] | Iteration[0232/0782] - Loss value: 0.21771074831485748\n",
      "Epoch[00/01] | Iteration[0233/0782] - Loss value: 0.26565900444984436\n",
      "Epoch[00/01] | Iteration[0234/0782] - Loss value: 0.12809444963932037\n",
      "Epoch[00/01] | Iteration[0235/0782] - Loss value: 0.19927574694156647\n",
      "Epoch[00/01] | Iteration[0236/0782] - Loss value: 0.1500471830368042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[00/01] | Iteration[0237/0782] - Loss value: 0.1181827113032341\n",
      "Epoch[00/01] | Iteration[0238/0782] - Loss value: 0.0875772014260292\n",
      "Epoch[00/01] | Iteration[0239/0782] - Loss value: 0.09100190550088882\n",
      "Epoch[00/01] | Iteration[0240/0782] - Loss value: 0.1340603083372116\n",
      "Epoch[00/01] | Iteration[0241/0782] - Loss value: 0.1288434863090515\n",
      "Epoch[00/01] | Iteration[0242/0782] - Loss value: 0.27416616678237915\n",
      "Epoch[00/01] | Iteration[0243/0782] - Loss value: 0.3268621563911438\n",
      "Epoch[00/01] | Iteration[0244/0782] - Loss value: 0.22137773036956787\n",
      "Epoch[00/01] | Iteration[0245/0782] - Loss value: 0.1909811645746231\n",
      "Epoch[00/01] | Iteration[0246/0782] - Loss value: 0.16214585304260254\n",
      "Epoch[00/01] | Iteration[0247/0782] - Loss value: 0.1955493539571762\n",
      "Epoch[00/01] | Iteration[0248/0782] - Loss value: 0.13888610899448395\n",
      "Epoch[00/01] | Iteration[0249/0782] - Loss value: 0.09197580069303513\n",
      "Epoch[00/01] | Iteration[0250/0782] - Loss value: 0.2723263204097748\n",
      "Epoch[00/01] | Iteration[0251/0782] - Loss value: 0.4430370628833771\n",
      "Epoch[00/01] | Iteration[0252/0782] - Loss value: 0.24830368161201477\n",
      "Epoch[00/01] | Iteration[0253/0782] - Loss value: 0.14083388447761536\n",
      "Epoch[00/01] | Iteration[0254/0782] - Loss value: 0.11050676554441452\n",
      "Epoch[00/01] | Iteration[0255/0782] - Loss value: 0.19275665283203125\n",
      "Epoch[00/01] | Iteration[0256/0782] - Loss value: 0.3728482723236084\n",
      "Epoch[00/01] | Iteration[0257/0782] - Loss value: 0.19983069598674774\n",
      "Epoch[00/01] | Iteration[0258/0782] - Loss value: 0.11380492150783539\n",
      "Epoch[00/01] | Iteration[0259/0782] - Loss value: 0.1247139498591423\n",
      "Epoch[00/01] | Iteration[0260/0782] - Loss value: 0.06975895911455154\n",
      "Epoch[00/01] | Iteration[0261/0782] - Loss value: 0.07671815901994705\n",
      "Epoch[00/01] | Iteration[0262/0782] - Loss value: 0.23864729702472687\n",
      "Epoch[00/01] | Iteration[0263/0782] - Loss value: 0.24361762404441833\n",
      "Epoch[00/01] | Iteration[0264/0782] - Loss value: 0.2378890961408615\n",
      "Epoch[00/01] | Iteration[0265/0782] - Loss value: 0.1553458273410797\n",
      "Epoch[00/01] | Iteration[0266/0782] - Loss value: 0.34038498997688293\n",
      "Epoch[00/01] | Iteration[0267/0782] - Loss value: 0.2686387002468109\n",
      "Epoch[00/01] | Iteration[0268/0782] - Loss value: 0.15861202776432037\n",
      "Epoch[00/01] | Iteration[0269/0782] - Loss value: 0.2214050441980362\n",
      "Epoch[00/01] | Iteration[0270/0782] - Loss value: 0.1028696820139885\n",
      "Epoch[00/01] | Iteration[0271/0782] - Loss value: 0.24419423937797546\n",
      "Epoch[00/01] | Iteration[0272/0782] - Loss value: 0.36960965394973755\n",
      "Epoch[00/01] | Iteration[0273/0782] - Loss value: 0.2823657691478729\n",
      "Epoch[00/01] | Iteration[0274/0782] - Loss value: 0.36344999074935913\n",
      "Epoch[00/01] | Iteration[0275/0782] - Loss value: 0.11057000607252121\n",
      "Epoch[00/01] | Iteration[0276/0782] - Loss value: 0.39988023042678833\n",
      "Epoch[00/01] | Iteration[0277/0782] - Loss value: 0.1627047210931778\n",
      "Epoch[00/01] | Iteration[0278/0782] - Loss value: 0.1689460724592209\n",
      "Epoch[00/01] | Iteration[0279/0782] - Loss value: 0.15971875190734863\n",
      "Epoch[00/01] | Iteration[0280/0782] - Loss value: 0.07755815237760544\n",
      "Epoch[00/01] | Iteration[0281/0782] - Loss value: 0.38761594891548157\n",
      "Epoch[00/01] | Iteration[0282/0782] - Loss value: 0.32668977975845337\n",
      "Epoch[00/01] | Iteration[0283/0782] - Loss value: 0.06012917682528496\n",
      "Epoch[00/01] | Iteration[0284/0782] - Loss value: 0.145316943526268\n",
      "Epoch[00/01] | Iteration[0285/0782] - Loss value: 0.29711148142814636\n",
      "Epoch[00/01] | Iteration[0286/0782] - Loss value: 0.052169788628816605\n",
      "Epoch[00/01] | Iteration[0287/0782] - Loss value: 0.14293280243873596\n",
      "Epoch[00/01] | Iteration[0288/0782] - Loss value: 0.1348574012517929\n",
      "Epoch[00/01] | Iteration[0289/0782] - Loss value: 0.1611844301223755\n",
      "Epoch[00/01] | Iteration[0290/0782] - Loss value: 0.31307268142700195\n",
      "Epoch[00/01] | Iteration[0291/0782] - Loss value: 0.2555292844772339\n",
      "Epoch[00/01] | Iteration[0292/0782] - Loss value: 0.2317095696926117\n",
      "Epoch[00/01] | Iteration[0293/0782] - Loss value: 0.12071441859006882\n",
      "Epoch[00/01] | Iteration[0294/0782] - Loss value: 0.2943950891494751\n",
      "Epoch[00/01] | Iteration[0295/0782] - Loss value: 0.05727885663509369\n",
      "Epoch[00/01] | Iteration[0296/0782] - Loss value: 0.15527597069740295\n",
      "Epoch[00/01] | Iteration[0297/0782] - Loss value: 0.09162764996290207\n",
      "Epoch[00/01] | Iteration[0298/0782] - Loss value: 0.23431161046028137\n",
      "Epoch[00/01] | Iteration[0299/0782] - Loss value: 0.40098780393600464\n",
      "Epoch[00/01] | Iteration[0300/0782] - Loss value: 0.24152632057666779\n",
      "Epoch[00/01] | Iteration[0301/0782] - Loss value: 0.18111610412597656\n",
      "Epoch[00/01] | Iteration[0302/0782] - Loss value: 0.17131809890270233\n",
      "Epoch[00/01] | Iteration[0303/0782] - Loss value: 0.12751398980617523\n",
      "Epoch[00/01] | Iteration[0304/0782] - Loss value: 0.20557165145874023\n",
      "Epoch[00/01] | Iteration[0305/0782] - Loss value: 0.06505129486322403\n",
      "Epoch[00/01] | Iteration[0306/0782] - Loss value: 0.2676771581172943\n",
      "Epoch[00/01] | Iteration[0307/0782] - Loss value: 0.3048330843448639\n",
      "Epoch[00/01] | Iteration[0308/0782] - Loss value: 0.42775624990463257\n",
      "Epoch[00/01] | Iteration[0309/0782] - Loss value: 0.24446223676204681\n",
      "Epoch[00/01] | Iteration[0310/0782] - Loss value: 0.16188234090805054\n",
      "Epoch[00/01] | Iteration[0311/0782] - Loss value: 0.21485157310962677\n",
      "Epoch[00/01] | Iteration[0312/0782] - Loss value: 0.05066733434796333\n",
      "Epoch[00/01] | Iteration[0313/0782] - Loss value: 0.13266634941101074\n",
      "Epoch[00/01] | Iteration[0314/0782] - Loss value: 0.08656884729862213\n",
      "Epoch[00/01] | Iteration[0315/0782] - Loss value: 0.2178805023431778\n",
      "Epoch[00/01] | Iteration[0316/0782] - Loss value: 0.13029147684574127\n",
      "Epoch[00/01] | Iteration[0317/0782] - Loss value: 0.2516372799873352\n",
      "Epoch[00/01] | Iteration[0318/0782] - Loss value: 0.16212689876556396\n",
      "Epoch[00/01] | Iteration[0319/0782] - Loss value: 0.15209579467773438\n",
      "Epoch[00/01] | Iteration[0320/0782] - Loss value: 0.41754674911499023\n",
      "Epoch[00/01] | Iteration[0321/0782] - Loss value: 0.12655311822891235\n",
      "Epoch[00/01] | Iteration[0322/0782] - Loss value: 0.31632140278816223\n",
      "Epoch[00/01] | Iteration[0323/0782] - Loss value: 0.2892492115497589\n",
      "Epoch[00/01] | Iteration[0324/0782] - Loss value: 0.0814635381102562\n",
      "Epoch[00/01] | Iteration[0325/0782] - Loss value: 0.0787939503788948\n",
      "Epoch[00/01] | Iteration[0326/0782] - Loss value: 0.12046943604946136\n",
      "Epoch[00/01] | Iteration[0327/0782] - Loss value: 0.1485862284898758\n",
      "Epoch[00/01] | Iteration[0328/0782] - Loss value: 0.1312693953514099\n",
      "Epoch[00/01] | Iteration[0329/0782] - Loss value: 0.15896038711071014\n",
      "Epoch[00/01] | Iteration[0330/0782] - Loss value: 0.3478996157646179\n",
      "Epoch[00/01] | Iteration[0331/0782] - Loss value: 0.04004662483930588\n",
      "Epoch[00/01] | Iteration[0332/0782] - Loss value: 0.40987807512283325\n",
      "Epoch[00/01] | Iteration[0333/0782] - Loss value: 0.267046719789505\n",
      "Epoch[00/01] | Iteration[0334/0782] - Loss value: 0.05579553544521332\n",
      "Epoch[00/01] | Iteration[0335/0782] - Loss value: 0.3129006326198578\n",
      "Epoch[00/01] | Iteration[0336/0782] - Loss value: 0.17092110216617584\n",
      "Epoch[00/01] | Iteration[0337/0782] - Loss value: 0.35040193796157837\n",
      "Epoch[00/01] | Iteration[0338/0782] - Loss value: 0.14188377559185028\n",
      "Epoch[00/01] | Iteration[0339/0782] - Loss value: 0.1658754050731659\n",
      "Epoch[00/01] | Iteration[0340/0782] - Loss value: 0.2195078730583191\n",
      "Epoch[00/01] | Iteration[0341/0782] - Loss value: 0.2103234976530075\n",
      "Epoch[00/01] | Iteration[0342/0782] - Loss value: 0.052105870097875595\n",
      "Epoch[00/01] | Iteration[0343/0782] - Loss value: 0.08143056184053421\n",
      "Epoch[00/01] | Iteration[0344/0782] - Loss value: 0.5629903078079224\n",
      "Epoch[00/01] | Iteration[0345/0782] - Loss value: 0.24190710484981537\n",
      "Epoch[00/01] | Iteration[0346/0782] - Loss value: 0.20113356411457062\n",
      "Epoch[00/01] | Iteration[0347/0782] - Loss value: 0.22455036640167236\n",
      "Epoch[00/01] | Iteration[0348/0782] - Loss value: 0.36409077048301697\n",
      "Epoch[00/01] | Iteration[0349/0782] - Loss value: 0.02602434530854225\n",
      "Epoch[00/01] | Iteration[0350/0782] - Loss value: 0.1731858253479004\n",
      "Epoch[00/01] | Iteration[0351/0782] - Loss value: 0.17871461808681488\n",
      "Epoch[00/01] | Iteration[0352/0782] - Loss value: 0.09133324772119522\n",
      "Epoch[00/01] | Iteration[0353/0782] - Loss value: 0.14865882694721222\n",
      "Epoch[00/01] | Iteration[0354/0782] - Loss value: 0.019566787406802177\n",
      "Epoch[00/01] | Iteration[0355/0782] - Loss value: 0.1323983520269394\n",
      "Epoch[00/01] | Iteration[0356/0782] - Loss value: 0.11230367422103882\n",
      "Epoch[00/01] | Iteration[0357/0782] - Loss value: 0.0823821872472763\n",
      "Epoch[00/01] | Iteration[0358/0782] - Loss value: 0.1751912534236908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[00/01] | Iteration[0359/0782] - Loss value: 0.40915927290916443\n",
      "Epoch[00/01] | Iteration[0360/0782] - Loss value: 0.08012872189283371\n",
      "Epoch[00/01] | Iteration[0361/0782] - Loss value: 0.03803182393312454\n",
      "Epoch[00/01] | Iteration[0362/0782] - Loss value: 0.3022640645503998\n",
      "Epoch[00/01] | Iteration[0363/0782] - Loss value: 0.16017718613147736\n",
      "Epoch[00/01] | Iteration[0364/0782] - Loss value: 0.07613269984722137\n",
      "Epoch[00/01] | Iteration[0365/0782] - Loss value: 0.030433299019932747\n",
      "Epoch[00/01] | Iteration[0366/0782] - Loss value: 0.3823438286781311\n",
      "Epoch[00/01] | Iteration[0367/0782] - Loss value: 0.14491577446460724\n",
      "Epoch[00/01] | Iteration[0368/0782] - Loss value: 0.10653472691774368\n",
      "Epoch[00/01] | Iteration[0369/0782] - Loss value: 0.13480380177497864\n",
      "Epoch[00/01] | Iteration[0370/0782] - Loss value: 0.22758366167545319\n",
      "Epoch[00/01] | Iteration[0371/0782] - Loss value: 0.24468018114566803\n",
      "Epoch[00/01] | Iteration[0372/0782] - Loss value: 0.10691259056329727\n",
      "Epoch[00/01] | Iteration[0373/0782] - Loss value: 0.17556963860988617\n",
      "Epoch[00/01] | Iteration[0374/0782] - Loss value: 0.15607613325119019\n",
      "Epoch[00/01] | Iteration[0375/0782] - Loss value: 0.026131343096494675\n",
      "Epoch[00/01] | Iteration[0376/0782] - Loss value: 0.1423603594303131\n",
      "Epoch[00/01] | Iteration[0377/0782] - Loss value: 0.21867744624614716\n",
      "Epoch[00/01] | Iteration[0378/0782] - Loss value: 0.06550281494855881\n",
      "Epoch[00/01] | Iteration[0379/0782] - Loss value: 0.3082897365093231\n",
      "Epoch[00/01] | Iteration[0380/0782] - Loss value: 0.26107603311538696\n",
      "Epoch[00/01] | Iteration[0381/0782] - Loss value: 0.17441163957118988\n",
      "Epoch[00/01] | Iteration[0382/0782] - Loss value: 0.19569651782512665\n",
      "Epoch[00/01] | Iteration[0383/0782] - Loss value: 0.17816956341266632\n",
      "Epoch[00/01] | Iteration[0384/0782] - Loss value: 0.03373914211988449\n",
      "Epoch[00/01] | Iteration[0385/0782] - Loss value: 0.29081401228904724\n",
      "Epoch[00/01] | Iteration[0386/0782] - Loss value: 0.19832542538642883\n",
      "Epoch[00/01] | Iteration[0387/0782] - Loss value: 0.2763773202896118\n",
      "Epoch[00/01] | Iteration[0388/0782] - Loss value: 0.20354574918746948\n",
      "Epoch[00/01] | Iteration[0389/0782] - Loss value: 0.10095557570457458\n",
      "Epoch[00/01] | Iteration[0390/0782] - Loss value: 0.08213574439287186\n",
      "Epoch[00/01] | Iteration[0391/0782] - Loss value: 0.12147697806358337\n",
      "Epoch[00/01] | Iteration[0392/0782] - Loss value: 0.23707497119903564\n",
      "Epoch[00/01] | Iteration[0393/0782] - Loss value: 0.20108669996261597\n",
      "Epoch[00/01] | Iteration[0394/0782] - Loss value: 0.09296765923500061\n",
      "Epoch[00/01] | Iteration[0395/0782] - Loss value: 0.16522741317749023\n",
      "Epoch[00/01] | Iteration[0396/0782] - Loss value: 0.05744900554418564\n",
      "Epoch[00/01] | Iteration[0397/0782] - Loss value: 0.1589600294828415\n",
      "Epoch[00/01] | Iteration[0398/0782] - Loss value: 0.2022029310464859\n",
      "Epoch[00/01] | Iteration[0399/0782] - Loss value: 0.13366179168224335\n",
      "Epoch[00/01] | Iteration[0400/0782] - Loss value: 0.08540486544370651\n",
      "Epoch[00/01] | Iteration[0401/0782] - Loss value: 0.1242600828409195\n",
      "Epoch[00/01] | Iteration[0402/0782] - Loss value: 0.13853098452091217\n",
      "Epoch[00/01] | Iteration[0403/0782] - Loss value: 0.15191295742988586\n",
      "Epoch[00/01] | Iteration[0404/0782] - Loss value: 0.16549251973628998\n",
      "Epoch[00/01] | Iteration[0405/0782] - Loss value: 0.19025731086730957\n",
      "Epoch[00/01] | Iteration[0406/0782] - Loss value: 0.1185799315571785\n",
      "Epoch[00/01] | Iteration[0407/0782] - Loss value: 0.10617279261350632\n",
      "Epoch[00/01] | Iteration[0408/0782] - Loss value: 0.12029135227203369\n",
      "Epoch[00/01] | Iteration[0409/0782] - Loss value: 0.0976257249712944\n",
      "Epoch[00/01] | Iteration[0410/0782] - Loss value: 0.24749037623405457\n",
      "Epoch[00/01] | Iteration[0411/0782] - Loss value: 0.08259530365467072\n",
      "Epoch[00/01] | Iteration[0412/0782] - Loss value: 0.20219190418720245\n",
      "Epoch[00/01] | Iteration[0413/0782] - Loss value: 0.11347480863332748\n",
      "Epoch[00/01] | Iteration[0414/0782] - Loss value: 0.062426015734672546\n",
      "Epoch[00/01] | Iteration[0415/0782] - Loss value: 0.16564521193504333\n",
      "Epoch[00/01] | Iteration[0416/0782] - Loss value: 0.08350684493780136\n",
      "Epoch[00/01] | Iteration[0417/0782] - Loss value: 0.053757309913635254\n",
      "Epoch[00/01] | Iteration[0418/0782] - Loss value: 0.10439585894346237\n",
      "Epoch[00/01] | Iteration[0419/0782] - Loss value: 0.08136450499296188\n",
      "Epoch[00/01] | Iteration[0420/0782] - Loss value: 0.05396144837141037\n",
      "Epoch[00/01] | Iteration[0421/0782] - Loss value: 0.02020777389407158\n",
      "Epoch[00/01] | Iteration[0422/0782] - Loss value: 0.22607502341270447\n",
      "Epoch[00/01] | Iteration[0423/0782] - Loss value: 0.30363205075263977\n",
      "Epoch[00/01] | Iteration[0424/0782] - Loss value: 0.15317028760910034\n",
      "Epoch[00/01] | Iteration[0425/0782] - Loss value: 0.09057784080505371\n",
      "Epoch[00/01] | Iteration[0426/0782] - Loss value: 0.21767529845237732\n",
      "Epoch[00/01] | Iteration[0427/0782] - Loss value: 0.08134165406227112\n",
      "Epoch[00/01] | Iteration[0428/0782] - Loss value: 0.12318112701177597\n",
      "Epoch[00/01] | Iteration[0429/0782] - Loss value: 0.27663543820381165\n",
      "Epoch[00/01] | Iteration[0430/0782] - Loss value: 0.10295942425727844\n",
      "Epoch[00/01] | Iteration[0431/0782] - Loss value: 0.13075420260429382\n",
      "Epoch[00/01] | Iteration[0432/0782] - Loss value: 0.20817707479000092\n",
      "Epoch[00/01] | Iteration[0433/0782] - Loss value: 0.16185036301612854\n",
      "Epoch[00/01] | Iteration[0434/0782] - Loss value: 0.08173121511936188\n",
      "Epoch[00/01] | Iteration[0435/0782] - Loss value: 0.1496419906616211\n",
      "Epoch[00/01] | Iteration[0436/0782] - Loss value: 0.022036856040358543\n",
      "Epoch[00/01] | Iteration[0437/0782] - Loss value: 0.21010839939117432\n",
      "Epoch[00/01] | Iteration[0438/0782] - Loss value: 0.06352152675390244\n",
      "Epoch[00/01] | Iteration[0439/0782] - Loss value: 0.07660896331071854\n",
      "Epoch[00/01] | Iteration[0440/0782] - Loss value: 0.16392552852630615\n",
      "Epoch[00/01] | Iteration[0441/0782] - Loss value: 0.10393184423446655\n",
      "Epoch[00/01] | Iteration[0442/0782] - Loss value: 0.05755794048309326\n",
      "Epoch[00/01] | Iteration[0443/0782] - Loss value: 0.22082410752773285\n",
      "Epoch[00/01] | Iteration[0444/0782] - Loss value: 0.18721045553684235\n",
      "Epoch[00/01] | Iteration[0445/0782] - Loss value: 0.11451423168182373\n",
      "Epoch[00/01] | Iteration[0446/0782] - Loss value: 0.0366499200463295\n",
      "Epoch[00/01] | Iteration[0447/0782] - Loss value: 0.1709856390953064\n",
      "Epoch[00/01] | Iteration[0448/0782] - Loss value: 0.24867497384548187\n",
      "Epoch[00/01] | Iteration[0449/0782] - Loss value: 0.28473204374313354\n",
      "Epoch[00/01] | Iteration[0450/0782] - Loss value: 0.11013952642679214\n",
      "Epoch[00/01] | Iteration[0451/0782] - Loss value: 0.19689412415027618\n",
      "Epoch[00/01] | Iteration[0452/0782] - Loss value: 0.22498345375061035\n",
      "Epoch[00/01] | Iteration[0453/0782] - Loss value: 0.08381649106740952\n",
      "Epoch[00/01] | Iteration[0454/0782] - Loss value: 0.015509781427681446\n",
      "Epoch[00/01] | Iteration[0455/0782] - Loss value: 0.24138161540031433\n",
      "Epoch[00/01] | Iteration[0456/0782] - Loss value: 0.06149163469672203\n",
      "Epoch[00/01] | Iteration[0457/0782] - Loss value: 0.16900281608104706\n",
      "Epoch[00/01] | Iteration[0458/0782] - Loss value: 0.23167577385902405\n",
      "Epoch[00/01] | Iteration[0459/0782] - Loss value: 0.054476048797369\n",
      "Epoch[00/01] | Iteration[0460/0782] - Loss value: 0.14200232923030853\n",
      "Epoch[00/01] | Iteration[0461/0782] - Loss value: 0.038518473505973816\n",
      "Epoch[00/01] | Iteration[0462/0782] - Loss value: 0.2958260476589203\n",
      "Epoch[00/01] | Iteration[0463/0782] - Loss value: 0.0948496162891388\n",
      "Epoch[00/01] | Iteration[0464/0782] - Loss value: 0.2449807971715927\n",
      "Epoch[00/01] | Iteration[0465/0782] - Loss value: 0.20329806208610535\n",
      "Epoch[00/01] | Iteration[0466/0782] - Loss value: 0.21658064424991608\n",
      "Epoch[00/01] | Iteration[0467/0782] - Loss value: 0.19900405406951904\n",
      "Epoch[00/01] | Iteration[0468/0782] - Loss value: 0.0668051540851593\n",
      "Epoch[00/01] | Iteration[0469/0782] - Loss value: 0.1469237357378006\n",
      "Epoch[00/01] | Iteration[0470/0782] - Loss value: 0.1265670657157898\n",
      "Epoch[00/01] | Iteration[0471/0782] - Loss value: 0.23028592765331268\n",
      "Epoch[00/01] | Iteration[0472/0782] - Loss value: 0.4274646043777466\n",
      "Epoch[00/01] | Iteration[0473/0782] - Loss value: 0.15517379343509674\n",
      "Epoch[00/01] | Iteration[0474/0782] - Loss value: 0.22153589129447937\n",
      "Epoch[00/01] | Iteration[0475/0782] - Loss value: 0.07603658735752106\n",
      "Epoch[00/01] | Iteration[0476/0782] - Loss value: 0.18821191787719727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[00/01] | Iteration[0477/0782] - Loss value: 0.08460993319749832\n",
      "Epoch[00/01] | Iteration[0478/0782] - Loss value: 0.11040042340755463\n",
      "Epoch[00/01] | Iteration[0479/0782] - Loss value: 0.11998090893030167\n",
      "Epoch[00/01] | Iteration[0480/0782] - Loss value: 0.10416624695062637\n",
      "Epoch[00/01] | Iteration[0481/0782] - Loss value: 0.2466140240430832\n",
      "Epoch[00/01] | Iteration[0482/0782] - Loss value: 0.10442531853914261\n",
      "Epoch[00/01] | Iteration[0483/0782] - Loss value: 0.07712551951408386\n",
      "Epoch[00/01] | Iteration[0484/0782] - Loss value: 0.16599446535110474\n",
      "Epoch[00/01] | Iteration[0485/0782] - Loss value: 0.20722974836826324\n",
      "Epoch[00/01] | Iteration[0486/0782] - Loss value: 0.21319587528705597\n",
      "Epoch[00/01] | Iteration[0487/0782] - Loss value: 0.2301797866821289\n",
      "Epoch[00/01] | Iteration[0488/0782] - Loss value: 0.06956618279218674\n",
      "Epoch[00/01] | Iteration[0489/0782] - Loss value: 0.09167423844337463\n",
      "Epoch[00/01] | Iteration[0490/0782] - Loss value: 0.09030677378177643\n",
      "Epoch[00/01] | Iteration[0491/0782] - Loss value: 0.25031059980392456\n",
      "Epoch[00/01] | Iteration[0492/0782] - Loss value: 0.06906343996524811\n",
      "Epoch[00/01] | Iteration[0493/0782] - Loss value: 0.12938910722732544\n",
      "Epoch[00/01] | Iteration[0494/0782] - Loss value: 0.09521811455488205\n",
      "Epoch[00/01] | Iteration[0495/0782] - Loss value: 0.0254895631223917\n",
      "Epoch[00/01] | Iteration[0496/0782] - Loss value: 0.2500512897968292\n",
      "Epoch[00/01] | Iteration[0497/0782] - Loss value: 0.08189350366592407\n",
      "Epoch[00/01] | Iteration[0498/0782] - Loss value: 0.1694292277097702\n",
      "Epoch[00/01] | Iteration[0499/0782] - Loss value: 0.1559499055147171\n",
      "Epoch[00/01] | Iteration[0500/0782] - Loss value: 0.05874338746070862\n",
      "Epoch[00/01] | Iteration[0501/0782] - Loss value: 0.17160700261592865\n",
      "Epoch[00/01] | Iteration[0502/0782] - Loss value: 0.15815778076648712\n",
      "Epoch[00/01] | Iteration[0503/0782] - Loss value: 0.14318881928920746\n",
      "Epoch[00/01] | Iteration[0504/0782] - Loss value: 0.09400230646133423\n",
      "Epoch[00/01] | Iteration[0505/0782] - Loss value: 0.12492489069700241\n",
      "Epoch[00/01] | Iteration[0506/0782] - Loss value: 0.0741279125213623\n",
      "Epoch[00/01] | Iteration[0507/0782] - Loss value: 0.3206559717655182\n",
      "Epoch[00/01] | Iteration[0508/0782] - Loss value: 0.2456754595041275\n",
      "Epoch[00/01] | Iteration[0509/0782] - Loss value: 0.0951184406876564\n",
      "Epoch[00/01] | Iteration[0510/0782] - Loss value: 0.11922135949134827\n",
      "Epoch[00/01] | Iteration[0511/0782] - Loss value: 0.34850743412971497\n",
      "Epoch[00/01] | Iteration[0512/0782] - Loss value: 0.1102820411324501\n",
      "Epoch[00/01] | Iteration[0513/0782] - Loss value: 0.06494966894388199\n",
      "Epoch[00/01] | Iteration[0514/0782] - Loss value: 0.20338642597198486\n",
      "Epoch[00/01] | Iteration[0515/0782] - Loss value: 0.06821191310882568\n",
      "Epoch[00/01] | Iteration[0516/0782] - Loss value: 0.07346735894680023\n",
      "Epoch[00/01] | Iteration[0517/0782] - Loss value: 0.10010030120611191\n",
      "Epoch[00/01] | Iteration[0518/0782] - Loss value: 0.2628152370452881\n",
      "Epoch[00/01] | Iteration[0519/0782] - Loss value: 0.20632152259349823\n",
      "Epoch[00/01] | Iteration[0520/0782] - Loss value: 0.043570857495069504\n",
      "Epoch[00/01] | Iteration[0521/0782] - Loss value: 0.09975181519985199\n",
      "Epoch[00/01] | Iteration[0522/0782] - Loss value: 0.1245843842625618\n",
      "Epoch[00/01] | Iteration[0523/0782] - Loss value: 0.11261109262704849\n",
      "Epoch[00/01] | Iteration[0524/0782] - Loss value: 0.20430493354797363\n",
      "Epoch[00/01] | Iteration[0525/0782] - Loss value: 0.2595720887184143\n",
      "Epoch[00/01] | Iteration[0526/0782] - Loss value: 0.1518964320421219\n",
      "Epoch[00/01] | Iteration[0527/0782] - Loss value: 0.12791843712329865\n",
      "Epoch[00/01] | Iteration[0528/0782] - Loss value: 0.24432848393917084\n",
      "Epoch[00/01] | Iteration[0529/0782] - Loss value: 0.05295026674866676\n",
      "Epoch[00/01] | Iteration[0530/0782] - Loss value: 0.06120399758219719\n",
      "Epoch[00/01] | Iteration[0531/0782] - Loss value: 0.0566730797290802\n",
      "Epoch[00/01] | Iteration[0532/0782] - Loss value: 0.07731816172599792\n",
      "Epoch[00/01] | Iteration[0533/0782] - Loss value: 0.15997569262981415\n",
      "Epoch[00/01] | Iteration[0534/0782] - Loss value: 0.30284810066223145\n",
      "Epoch[00/01] | Iteration[0535/0782] - Loss value: 0.10641714185476303\n",
      "Epoch[00/01] | Iteration[0536/0782] - Loss value: 0.2228163629770279\n",
      "Epoch[00/01] | Iteration[0537/0782] - Loss value: 0.09887674450874329\n",
      "Epoch[00/01] | Iteration[0538/0782] - Loss value: 0.14368247985839844\n",
      "Epoch[00/01] | Iteration[0539/0782] - Loss value: 0.3247683346271515\n",
      "Epoch[00/01] | Iteration[0540/0782] - Loss value: 0.15390674769878387\n",
      "Epoch[00/01] | Iteration[0541/0782] - Loss value: 0.18972301483154297\n",
      "Epoch[00/01] | Iteration[0542/0782] - Loss value: 0.1314905881881714\n",
      "Epoch[00/01] | Iteration[0543/0782] - Loss value: 0.08702899515628815\n",
      "Epoch[00/01] | Iteration[0544/0782] - Loss value: 0.10377497971057892\n",
      "Epoch[00/01] | Iteration[0545/0782] - Loss value: 0.3499573767185211\n",
      "Epoch[00/01] | Iteration[0546/0782] - Loss value: 0.08295302093029022\n",
      "Epoch[00/01] | Iteration[0547/0782] - Loss value: 0.12454016506671906\n",
      "Epoch[00/01] | Iteration[0548/0782] - Loss value: 0.09076603502035141\n",
      "Epoch[00/01] | Iteration[0549/0782] - Loss value: 0.023762371391057968\n",
      "Epoch[00/01] | Iteration[0550/0782] - Loss value: 0.11161138117313385\n",
      "Epoch[00/01] | Iteration[0551/0782] - Loss value: 0.2921258509159088\n",
      "Epoch[00/01] | Iteration[0552/0782] - Loss value: 0.2050734907388687\n",
      "Epoch[00/01] | Iteration[0553/0782] - Loss value: 0.07596656680107117\n",
      "Epoch[00/01] | Iteration[0554/0782] - Loss value: 0.04125498607754707\n",
      "Epoch[00/01] | Iteration[0555/0782] - Loss value: 0.18963360786437988\n",
      "Epoch[00/01] | Iteration[0556/0782] - Loss value: 0.029394658282399178\n",
      "Epoch[00/01] | Iteration[0557/0782] - Loss value: 0.06251048296689987\n",
      "Epoch[00/01] | Iteration[0558/0782] - Loss value: 0.0386091023683548\n",
      "Epoch[00/01] | Iteration[0559/0782] - Loss value: 0.23196664452552795\n",
      "Epoch[00/01] | Iteration[0560/0782] - Loss value: 0.0997423604130745\n",
      "Epoch[00/01] | Iteration[0561/0782] - Loss value: 0.09224472939968109\n",
      "Epoch[00/01] | Iteration[0562/0782] - Loss value: 0.12583027780056\n",
      "Epoch[00/01] | Iteration[0563/0782] - Loss value: 0.14179080724716187\n",
      "Epoch[00/01] | Iteration[0564/0782] - Loss value: 0.187743678689003\n",
      "Epoch[00/01] | Iteration[0565/0782] - Loss value: 0.21297255158424377\n",
      "Epoch[00/01] | Iteration[0566/0782] - Loss value: 0.11588405072689056\n",
      "Epoch[00/01] | Iteration[0567/0782] - Loss value: 0.1850452870130539\n",
      "Epoch[00/01] | Iteration[0568/0782] - Loss value: 0.08034113794565201\n",
      "Epoch[00/01] | Iteration[0569/0782] - Loss value: 0.09327759593725204\n",
      "Epoch[00/01] | Iteration[0570/0782] - Loss value: 0.06476105749607086\n",
      "Epoch[00/01] | Iteration[0571/0782] - Loss value: 0.019276032224297523\n",
      "Epoch[00/01] | Iteration[0572/0782] - Loss value: 0.19960924983024597\n",
      "Epoch[00/01] | Iteration[0573/0782] - Loss value: 0.1105085089802742\n",
      "Epoch[00/01] | Iteration[0574/0782] - Loss value: 0.24407392740249634\n",
      "Epoch[00/01] | Iteration[0575/0782] - Loss value: 0.08260822296142578\n",
      "Epoch[00/01] | Iteration[0576/0782] - Loss value: 0.11938055604696274\n",
      "Epoch[00/01] | Iteration[0577/0782] - Loss value: 0.09010842442512512\n",
      "Epoch[00/01] | Iteration[0578/0782] - Loss value: 0.10024940967559814\n",
      "Epoch[00/01] | Iteration[0579/0782] - Loss value: 0.14797452092170715\n",
      "Epoch[00/01] | Iteration[0580/0782] - Loss value: 0.08781427145004272\n",
      "Epoch[00/01] | Iteration[0581/0782] - Loss value: 0.10144554823637009\n",
      "Epoch[00/01] | Iteration[0582/0782] - Loss value: 0.20164045691490173\n",
      "Epoch[00/01] | Iteration[0583/0782] - Loss value: 0.1398508995771408\n",
      "Epoch[00/01] | Iteration[0584/0782] - Loss value: 0.14096350967884064\n",
      "Epoch[00/01] | Iteration[0585/0782] - Loss value: 0.20733202993869781\n",
      "Epoch[00/01] | Iteration[0586/0782] - Loss value: 0.1653064638376236\n",
      "Epoch[00/01] | Iteration[0587/0782] - Loss value: 0.24513259530067444\n",
      "Epoch[00/01] | Iteration[0588/0782] - Loss value: 0.07318419963121414\n",
      "Epoch[00/01] | Iteration[0589/0782] - Loss value: 0.04969925805926323\n",
      "Epoch[00/01] | Iteration[0590/0782] - Loss value: 0.07149612903594971\n",
      "Epoch[00/01] | Iteration[0591/0782] - Loss value: 0.22623680531978607\n",
      "Epoch[00/01] | Iteration[0592/0782] - Loss value: 0.17628434300422668\n",
      "Epoch[00/01] | Iteration[0593/0782] - Loss value: 0.1199481412768364\n",
      "Epoch[00/01] | Iteration[0594/0782] - Loss value: 0.22850844264030457\n",
      "Epoch[00/01] | Iteration[0595/0782] - Loss value: 0.11608892679214478\n",
      "Epoch[00/01] | Iteration[0596/0782] - Loss value: 0.06858218461275101\n",
      "Epoch[00/01] | Iteration[0597/0782] - Loss value: 0.07148034870624542\n",
      "Epoch[00/01] | Iteration[0598/0782] - Loss value: 0.21651475131511688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[00/01] | Iteration[0599/0782] - Loss value: 0.09706275165081024\n",
      "Epoch[00/01] | Iteration[0600/0782] - Loss value: 0.05007591098546982\n",
      "Epoch[00/01] | Iteration[0601/0782] - Loss value: 0.08078373968601227\n",
      "Epoch[00/01] | Iteration[0602/0782] - Loss value: 0.15020538866519928\n",
      "Epoch[00/01] | Iteration[0603/0782] - Loss value: 0.21797452867031097\n",
      "Epoch[00/01] | Iteration[0604/0782] - Loss value: 0.20851077139377594\n",
      "Epoch[00/01] | Iteration[0605/0782] - Loss value: 0.09858282655477524\n",
      "Epoch[00/01] | Iteration[0606/0782] - Loss value: 0.31953468918800354\n",
      "Epoch[00/01] | Iteration[0607/0782] - Loss value: 0.09418922662734985\n",
      "Epoch[00/01] | Iteration[0608/0782] - Loss value: 0.2053930014371872\n",
      "Epoch[00/01] | Iteration[0609/0782] - Loss value: 0.10572849959135056\n",
      "Epoch[00/01] | Iteration[0610/0782] - Loss value: 0.18936197459697723\n",
      "Epoch[00/01] | Iteration[0611/0782] - Loss value: 0.09397268295288086\n",
      "Epoch[00/01] | Iteration[0612/0782] - Loss value: 0.10246345400810242\n",
      "Epoch[00/01] | Iteration[0613/0782] - Loss value: 0.07508952170610428\n",
      "Epoch[00/01] | Iteration[0614/0782] - Loss value: 0.07756324112415314\n",
      "Epoch[00/01] | Iteration[0615/0782] - Loss value: 0.2979273796081543\n",
      "Epoch[00/01] | Iteration[0616/0782] - Loss value: 0.36543604731559753\n",
      "Epoch[00/01] | Iteration[0617/0782] - Loss value: 0.06303924322128296\n",
      "Epoch[00/01] | Iteration[0618/0782] - Loss value: 0.17256587743759155\n",
      "Epoch[00/01] | Iteration[0619/0782] - Loss value: 0.14041784405708313\n",
      "Epoch[00/01] | Iteration[0620/0782] - Loss value: 0.1145240068435669\n",
      "Epoch[00/01] | Iteration[0621/0782] - Loss value: 0.08454401791095734\n",
      "Epoch[00/01] | Iteration[0622/0782] - Loss value: 0.06887529790401459\n",
      "Epoch[00/01] | Iteration[0623/0782] - Loss value: 0.0329773910343647\n",
      "Epoch[00/01] | Iteration[0624/0782] - Loss value: 0.31478196382522583\n",
      "Epoch[00/01] | Iteration[0625/0782] - Loss value: 0.16582201421260834\n",
      "Epoch[00/01] | Iteration[0626/0782] - Loss value: 0.11078070849180222\n",
      "Epoch[00/01] | Iteration[0627/0782] - Loss value: 0.49669352173805237\n",
      "Epoch[00/01] | Iteration[0628/0782] - Loss value: 0.04995182156562805\n",
      "Epoch[00/01] | Iteration[0629/0782] - Loss value: 0.12402655929327011\n",
      "Epoch[00/01] | Iteration[0630/0782] - Loss value: 0.191436767578125\n",
      "Epoch[00/01] | Iteration[0631/0782] - Loss value: 0.16187988221645355\n",
      "Epoch[00/01] | Iteration[0632/0782] - Loss value: 0.10213540494441986\n",
      "Epoch[00/01] | Iteration[0633/0782] - Loss value: 0.1086372658610344\n",
      "Epoch[00/01] | Iteration[0634/0782] - Loss value: 0.10523484647274017\n",
      "Epoch[00/01] | Iteration[0635/0782] - Loss value: 0.22619172930717468\n",
      "Epoch[00/01] | Iteration[0636/0782] - Loss value: 0.11589924246072769\n",
      "Epoch[00/01] | Iteration[0637/0782] - Loss value: 0.058266621083021164\n",
      "Epoch[00/01] | Iteration[0638/0782] - Loss value: 0.08894068747758865\n",
      "Epoch[00/01] | Iteration[0639/0782] - Loss value: 0.12097491323947906\n",
      "Epoch[00/01] | Iteration[0640/0782] - Loss value: 0.19288437068462372\n",
      "Epoch[00/01] | Iteration[0641/0782] - Loss value: 0.22333793342113495\n",
      "Epoch[00/01] | Iteration[0642/0782] - Loss value: 0.2502741515636444\n",
      "Epoch[00/01] | Iteration[0643/0782] - Loss value: 0.07352915406227112\n",
      "Epoch[00/01] | Iteration[0644/0782] - Loss value: 0.04319646209478378\n",
      "Epoch[00/01] | Iteration[0645/0782] - Loss value: 0.08951107412576675\n",
      "Epoch[00/01] | Iteration[0646/0782] - Loss value: 0.3472050428390503\n",
      "Epoch[00/01] | Iteration[0647/0782] - Loss value: 0.03525533154606819\n",
      "Epoch[00/01] | Iteration[0648/0782] - Loss value: 0.07268581539392471\n",
      "Epoch[00/01] | Iteration[0649/0782] - Loss value: 0.026984363794326782\n",
      "Epoch[00/01] | Iteration[0650/0782] - Loss value: 0.07732249796390533\n",
      "Epoch[00/01] | Iteration[0651/0782] - Loss value: 0.14712586998939514\n",
      "Epoch[00/01] | Iteration[0652/0782] - Loss value: 0.1781122386455536\n",
      "Epoch[00/01] | Iteration[0653/0782] - Loss value: 0.1764395534992218\n",
      "Epoch[00/01] | Iteration[0654/0782] - Loss value: 0.07961404323577881\n",
      "Epoch[00/01] | Iteration[0655/0782] - Loss value: 0.14329056441783905\n",
      "Epoch[00/01] | Iteration[0656/0782] - Loss value: 0.21192723512649536\n",
      "Epoch[00/01] | Iteration[0657/0782] - Loss value: 0.12872619926929474\n",
      "Epoch[00/01] | Iteration[0658/0782] - Loss value: 0.07949613034725189\n",
      "Epoch[00/01] | Iteration[0659/0782] - Loss value: 0.3037041127681732\n",
      "Epoch[00/01] | Iteration[0660/0782] - Loss value: 0.08472853153944016\n",
      "Epoch[00/01] | Iteration[0661/0782] - Loss value: 0.19224581122398376\n",
      "Epoch[00/01] | Iteration[0662/0782] - Loss value: 0.05648487061262131\n",
      "Epoch[00/01] | Iteration[0663/0782] - Loss value: 0.0422520637512207\n",
      "Epoch[00/01] | Iteration[0664/0782] - Loss value: 0.20008698105812073\n",
      "Epoch[00/01] | Iteration[0665/0782] - Loss value: 0.014685036614537239\n",
      "Epoch[00/01] | Iteration[0666/0782] - Loss value: 0.1485283076763153\n",
      "Epoch[00/01] | Iteration[0667/0782] - Loss value: 0.11208993941545486\n",
      "Epoch[00/01] | Iteration[0668/0782] - Loss value: 0.10315074026584625\n",
      "Epoch[00/01] | Iteration[0669/0782] - Loss value: 0.054818663746118546\n",
      "Epoch[00/01] | Iteration[0670/0782] - Loss value: 0.11529919505119324\n",
      "Epoch[00/01] | Iteration[0671/0782] - Loss value: 0.10930857807397842\n",
      "Epoch[00/01] | Iteration[0672/0782] - Loss value: 0.19949941337108612\n",
      "Epoch[00/01] | Iteration[0673/0782] - Loss value: 0.16797226667404175\n",
      "Epoch[00/01] | Iteration[0674/0782] - Loss value: 0.09108426421880722\n",
      "Epoch[00/01] | Iteration[0675/0782] - Loss value: 0.10510846227407455\n",
      "Epoch[00/01] | Iteration[0676/0782] - Loss value: 0.14019331336021423\n",
      "Epoch[00/01] | Iteration[0677/0782] - Loss value: 0.03119446337223053\n",
      "Epoch[00/01] | Iteration[0678/0782] - Loss value: 0.20829474925994873\n",
      "Epoch[00/01] | Iteration[0679/0782] - Loss value: 0.06641882658004761\n",
      "Epoch[00/01] | Iteration[0680/0782] - Loss value: 0.2392496019601822\n",
      "Epoch[00/01] | Iteration[0681/0782] - Loss value: 0.20914070308208466\n",
      "Epoch[00/01] | Iteration[0682/0782] - Loss value: 0.06300660222768784\n",
      "Epoch[00/01] | Iteration[0683/0782] - Loss value: 0.16662399470806122\n",
      "Epoch[00/01] | Iteration[0684/0782] - Loss value: 0.13261429965496063\n",
      "Epoch[00/01] | Iteration[0685/0782] - Loss value: 0.035321902483701706\n",
      "Epoch[00/01] | Iteration[0686/0782] - Loss value: 0.18055132031440735\n",
      "Epoch[00/01] | Iteration[0687/0782] - Loss value: 0.07000888884067535\n",
      "Epoch[00/01] | Iteration[0688/0782] - Loss value: 0.05845190957188606\n",
      "Epoch[00/01] | Iteration[0689/0782] - Loss value: 0.07735751569271088\n",
      "Epoch[00/01] | Iteration[0690/0782] - Loss value: 0.17134958505630493\n",
      "Epoch[00/01] | Iteration[0691/0782] - Loss value: 0.12985143065452576\n",
      "Epoch[00/01] | Iteration[0692/0782] - Loss value: 0.10792189091444016\n",
      "Epoch[00/01] | Iteration[0693/0782] - Loss value: 0.07216545939445496\n",
      "Epoch[00/01] | Iteration[0694/0782] - Loss value: 0.122102752327919\n",
      "Epoch[00/01] | Iteration[0695/0782] - Loss value: 0.11702932417392731\n",
      "Epoch[00/01] | Iteration[0696/0782] - Loss value: 0.18080578744411469\n",
      "Epoch[00/01] | Iteration[0697/0782] - Loss value: 0.09020956605672836\n",
      "Epoch[00/01] | Iteration[0698/0782] - Loss value: 0.06064940243959427\n",
      "Epoch[00/01] | Iteration[0699/0782] - Loss value: 0.13572926819324493\n",
      "Epoch[00/01] | Iteration[0700/0782] - Loss value: 0.07568460702896118\n",
      "Epoch[00/01] | Iteration[0701/0782] - Loss value: 0.13287894427776337\n",
      "Epoch[00/01] | Iteration[0702/0782] - Loss value: 0.11053527146577835\n",
      "Epoch[00/01] | Iteration[0703/0782] - Loss value: 0.02083508111536503\n",
      "Epoch[00/01] | Iteration[0704/0782] - Loss value: 0.08807109296321869\n",
      "Epoch[00/01] | Iteration[0705/0782] - Loss value: 0.23738887906074524\n",
      "Epoch[00/01] | Iteration[0706/0782] - Loss value: 0.037001095712184906\n",
      "Epoch[00/01] | Iteration[0707/0782] - Loss value: 0.0690956637263298\n",
      "Epoch[00/01] | Iteration[0708/0782] - Loss value: 0.07357845455408096\n",
      "Epoch[00/01] | Iteration[0709/0782] - Loss value: 0.08714190125465393\n",
      "Epoch[00/01] | Iteration[0710/0782] - Loss value: 0.1753746122121811\n",
      "Epoch[00/01] | Iteration[0711/0782] - Loss value: 0.10695502161979675\n",
      "Epoch[00/01] | Iteration[0712/0782] - Loss value: 0.17387020587921143\n",
      "Epoch[00/01] | Iteration[0713/0782] - Loss value: 0.11019562929868698\n",
      "Epoch[00/01] | Iteration[0714/0782] - Loss value: 0.06528788805007935\n",
      "Epoch[00/01] | Iteration[0715/0782] - Loss value: 0.18846814334392548\n",
      "Epoch[00/01] | Iteration[0716/0782] - Loss value: 0.027127286419272423\n",
      "Epoch[00/01] | Iteration[0717/0782] - Loss value: 0.026997601613402367\n",
      "Epoch[00/01] | Iteration[0718/0782] - Loss value: 0.11818600445985794\n",
      "Epoch[00/01] | Iteration[0719/0782] - Loss value: 0.017119497060775757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[00/01] | Iteration[0720/0782] - Loss value: 0.06338536739349365\n",
      "Epoch[00/01] | Iteration[0721/0782] - Loss value: 0.16699323058128357\n",
      "Epoch[00/01] | Iteration[0722/0782] - Loss value: 0.17088772356510162\n",
      "Epoch[00/01] | Iteration[0723/0782] - Loss value: 0.06423000991344452\n",
      "Epoch[00/01] | Iteration[0724/0782] - Loss value: 0.18206164240837097\n",
      "Epoch[00/01] | Iteration[0725/0782] - Loss value: 0.16315095126628876\n",
      "Epoch[00/01] | Iteration[0726/0782] - Loss value: 0.12108190357685089\n",
      "Epoch[00/01] | Iteration[0727/0782] - Loss value: 0.08939852565526962\n",
      "Epoch[00/01] | Iteration[0728/0782] - Loss value: 0.03191738948225975\n",
      "Epoch[00/01] | Iteration[0729/0782] - Loss value: 0.12267317622900009\n",
      "Epoch[00/01] | Iteration[0730/0782] - Loss value: 0.16905847191810608\n",
      "Epoch[00/01] | Iteration[0731/0782] - Loss value: 0.0716976746916771\n",
      "Epoch[00/01] | Iteration[0732/0782] - Loss value: 0.14435286819934845\n",
      "Epoch[00/01] | Iteration[0733/0782] - Loss value: 0.23367495834827423\n",
      "Epoch[00/01] | Iteration[0734/0782] - Loss value: 0.0716129019856453\n",
      "Epoch[00/01] | Iteration[0735/0782] - Loss value: 0.06080136075615883\n",
      "Epoch[00/01] | Iteration[0736/0782] - Loss value: 0.20388954877853394\n",
      "Epoch[00/01] | Iteration[0737/0782] - Loss value: 0.09666518867015839\n",
      "Epoch[00/01] | Iteration[0738/0782] - Loss value: 0.03265055641531944\n",
      "Epoch[00/01] | Iteration[0739/0782] - Loss value: 0.041410181671381\n",
      "Epoch[00/01] | Iteration[0740/0782] - Loss value: 0.07032416760921478\n",
      "Epoch[00/01] | Iteration[0741/0782] - Loss value: 0.03832544386386871\n",
      "Epoch[00/01] | Iteration[0742/0782] - Loss value: 0.02389199659228325\n",
      "Epoch[00/01] | Iteration[0743/0782] - Loss value: 0.0920407697558403\n",
      "Epoch[00/01] | Iteration[0744/0782] - Loss value: 0.17551183700561523\n",
      "Epoch[00/01] | Iteration[0745/0782] - Loss value: 0.10910087823867798\n",
      "Epoch[00/01] | Iteration[0746/0782] - Loss value: 0.16340959072113037\n",
      "Epoch[00/01] | Iteration[0747/0782] - Loss value: 0.08239217102527618\n",
      "Epoch[00/01] | Iteration[0748/0782] - Loss value: 0.07727141678333282\n",
      "Epoch[00/01] | Iteration[0749/0782] - Loss value: 0.19605979323387146\n",
      "Epoch[00/01] | Iteration[0750/0782] - Loss value: 0.14328204095363617\n",
      "Epoch[00/01] | Iteration[0751/0782] - Loss value: 0.06195857748389244\n",
      "Epoch[00/01] | Iteration[0752/0782] - Loss value: 0.064564049243927\n",
      "Epoch[00/01] | Iteration[0753/0782] - Loss value: 0.11600852012634277\n",
      "Epoch[00/01] | Iteration[0754/0782] - Loss value: 0.09124273806810379\n",
      "Epoch[00/01] | Iteration[0755/0782] - Loss value: 0.11524616926908493\n",
      "Epoch[00/01] | Iteration[0756/0782] - Loss value: 0.1382787674665451\n",
      "Epoch[00/01] | Iteration[0757/0782] - Loss value: 0.20784763991832733\n",
      "Epoch[00/01] | Iteration[0758/0782] - Loss value: 0.21762582659721375\n",
      "Epoch[00/01] | Iteration[0759/0782] - Loss value: 0.0586993433535099\n",
      "Epoch[00/01] | Iteration[0760/0782] - Loss value: 0.039668090641498566\n",
      "Epoch[00/01] | Iteration[0761/0782] - Loss value: 0.12103371322154999\n",
      "Epoch[00/01] | Iteration[0762/0782] - Loss value: 0.07254590094089508\n",
      "Epoch[00/01] | Iteration[0763/0782] - Loss value: 0.19797293841838837\n",
      "Epoch[00/01] | Iteration[0764/0782] - Loss value: 0.05361000820994377\n",
      "Epoch[00/01] | Iteration[0765/0782] - Loss value: 0.06040054187178612\n",
      "Epoch[00/01] | Iteration[0766/0782] - Loss value: 0.06278519332408905\n",
      "Epoch[00/01] | Iteration[0767/0782] - Loss value: 0.09438163787126541\n",
      "Epoch[00/01] | Iteration[0768/0782] - Loss value: 0.14249517023563385\n",
      "Epoch[00/01] | Iteration[0769/0782] - Loss value: 0.07007794082164764\n",
      "Epoch[00/01] | Iteration[0770/0782] - Loss value: 0.032849930226802826\n",
      "Epoch[00/01] | Iteration[0771/0782] - Loss value: 0.14823739230632782\n",
      "Epoch[00/01] | Iteration[0772/0782] - Loss value: 0.0962023138999939\n",
      "Epoch[00/01] | Iteration[0773/0782] - Loss value: 0.056863151490688324\n",
      "Epoch[00/01] | Iteration[0774/0782] - Loss value: 0.07369568943977356\n",
      "Epoch[00/01] | Iteration[0775/0782] - Loss value: 0.15329104661941528\n",
      "Epoch[00/01] | Iteration[0776/0782] - Loss value: 0.06471110135316849\n",
      "Epoch[00/01] | Iteration[0777/0782] - Loss value: 0.08676553517580032\n",
      "Epoch[00/01] | Iteration[0778/0782] - Loss value: 0.060454174876213074\n",
      "Epoch[00/01] | Iteration[0779/0782] - Loss value: 0.12210410088300705\n",
      "Epoch[00/01] | Iteration[0780/0782] - Loss value: 0.23892831802368164\n",
      "Epoch[00/01] | Iteration[0781/0782] - Loss value: 0.22110000252723694\n",
      "Epoch[00/01] - Validation accuracy: 861.72%\n"
     ]
    }
   ],
   "source": [
    "n_epochs=1\n",
    "fit_quantised(device, train_loader, valid_loader, pact_network, [pact_controller_linear, pact_controller_act], loss_fn, optimiser, lr_sched, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43467fdc",
   "metadata": {},
   "source": [
    "### Hands-on\n",
    "\n",
    "In this sub-section, you have the possibility of training your own PACT-quantised VGG convolutional network to solve the CIFAR-10 problem.\n",
    "\n",
    "A quick recap of the required steps:\n",
    "1. instantiate a floating-point network using the constructor of the `VGG` class;\n",
    "2. inspect the network's structure wrapping it inside a `LightweightGraph` and calling its method `show_nodes_list`: in this way, you can get a visual feeling of how the information flows through the network, and how you might want to quantise it;\n",
    "3. set the precision of different network nodes by creating a `patches` dictionary mapping node names to the number of quantisation levels that they should use (`n_levels`); we suggest that you select `n_levels` to take on power-of-two values (e.g., three bits amount to $2^{3} = 8$ quantisation levels): in this way;\n",
    "4. use the `all_pact_create_configs` and `all_ana_f2f_recipe` defined in previous cells to convert the original floating-point network to a fake-quantised PACT network (float-to-fake conversion);\n",
    "5. get handles on the PACT fake-quantised nodes and set their training hyper-parameters by instantiating `PACTController`s using the `all_pact_get_controllers` function defined in a previous cell;\n",
    "6. define the loss function, the optimiser (remember that PACT-trained networks require a custom `PACTOptimizer`), and (possibly) a learning rate scheduler;\n",
    "7. train the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd7d33df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def shiftandscale_tensor_01(tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Shift-and-scale an array so that its components fall in the range [0, 1].\"\"\"\n",
    "    if tensor.min() < tensor.max():  # the target 'tensor' is non-constant\n",
    "        tensor = (tensor - tensor.min()) / (tensor.max() - tensor.min())\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def show_2d_tensor(tensor: torch.Tensor) -> None:\n",
    "    \n",
    "    assert tensor.ndim == 2\n",
    "    out_channels, in_channels = tensor.shape\n",
    "    \n",
    "    tensor = tensor.detach().cpu()\n",
    "    tensor = shiftandscale_tensor_01(tensor)\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(16, 12)\n",
    "    ax = plt.imshow(tensor)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_3d_tensor(tensor: torch.Tensor) -> None:\n",
    "    \"\"\"Show a three-dimensional tensor by juxtaposing two-dimensional plots of its slices.\"\"\"\n",
    "    \n",
    "    assert tensor.ndim == 3\n",
    "    n_channels, h, w = tensor.shape\n",
    "    \n",
    "    tensor = tensor.detach().cpu()\n",
    "    tensor = shiftandscale_tensor_01(tensor)\n",
    "\n",
    "    # compute size of image grid (try to keep a square aspect ratio)\n",
    "    m = math.ceil(math.sqrt(n_channels))\n",
    "    n = (n_channels + (m - 1)) // m\n",
    "    \n",
    "    # it looks ugly to have empty 'Axes' objects in a grid of plots: https://matplotlib.org/3.2.2/gallery/lines_bars_and_markers/markevery_demo.html#sphx-glr-gallery-lines-bars-and-markers-markevery-demo-py\n",
    "    def trim_axs(axis_array, N):\n",
    "        \"\"\"Reduce 'axis_array' to 'N' 'Axes' objects. All further 'Axes' are removed from the figure.\"\"\"\n",
    "        axis_array = axis_array.flat\n",
    "        for ax in axis_array[N:]:\n",
    "            ax.remove()\n",
    "        return axis_array[:N]\n",
    "\n",
    "    fig, axis_array = plt.subplots(m, n)\n",
    "    fig.set_size_inches(16, 12)\n",
    "    axis_array = trim_axs(axis_array, n_channels)\n",
    "    for ax, slice_ in zip(axis_array, tensor):\n",
    "        ax.imshow(slice_, vmin=tensor.min(), vmax=tensor.max())  # explicitly setting 'vmin' and 'vmax' implies that colours have the same meaning across different sub-plots\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_tensor(device:   torch.device,\n",
    "                data_set: torch.utils.data.Dataset,\n",
    "                network:  nn.Module,\n",
    "                nodename: str) -> None:\n",
    "    \n",
    "    network.eval()\n",
    "    \n",
    "    name2module = {n.name: n.module for n in qlw.LightweightGraph(network).nodes_list}\n",
    "    assert nodename in name2module.keys()\n",
    "    \n",
    "    nodetype = name2module[nodename].__class__.__name__\n",
    "    node     = name2module[nodename]\n",
    "    \n",
    "    if nodetype in {'Conv2d', 'Linear'} | {'PACTConv2d', 'PACTLinear'}:\n",
    "        \n",
    "        tensor = node.weight_q if nodetype.startswith('PACT') else node.weight\n",
    "\n",
    "        if node.weight.ndim == 2:  # nodetype in {'Linear', 'PACTLinear'}\n",
    "            show_2d_tensor(tensor)\n",
    "            \n",
    "        else:  # nodetype in {'Conv2d', 'PACTConv2d'}\n",
    "            assert node.weight.ndim == 4\n",
    "            out_channels, in_channels, h, w = tensor.shape\n",
    "            show_3d_tensor(tensor[random.randrange(0, out_channels)])\n",
    "            \n",
    "    elif nodetype in {'ReLU'} | {'PACTUnsignedAct'}:\n",
    "        \n",
    "        # PyTorch uses dynamic graphs, so we do not have symbolic names to access feature 'Tensor's directly;\n",
    "        # instead, we need to attach callbacks to 'Module's ('register hook's in PyTorch jargon) which will intercept the required 'Tensor' when computation is triggered\n",
    "\n",
    "        def hook_show_features(self, input_, output):\n",
    "\n",
    "            if output.ndim == 2:\n",
    "                bs, n_channels = output.shape\n",
    "                show_2d_tensor(output[random.randrange(0, bs)].unsqueeze(0))\n",
    "            \n",
    "            else:\n",
    "                assert output.ndim == 4\n",
    "                bs, n_channels, h, w = output.shape\n",
    "                show_3d_tensor(output[random.randrange(0, bs)])\n",
    "            \n",
    "        handle = node.register_forward_hook(hook_show_features)\n",
    "        x, _   = data_set.__getitem__(random.randrange(0, len(data_set)))\n",
    "        x      = x.unsqueeze(0).to(device=device)\n",
    "        y_pr   = network(x)\n",
    "        handle.remove()\n",
    "        \n",
    "    else:\n",
    "        raise ValueError  # neither weights or features for this 'Module' can be visualised\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3abe28c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x864 with 128 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_tensor(device, valid_data, pact_network, 'features.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbf7862",
   "metadata": {},
   "source": [
    "<a id='sec:fake2true'></a>\n",
    "## Exporting trained QNNs to integerised ONNX IRs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebdcb19",
   "metadata": {},
   "source": [
    "The network that we have trained is not an integerised program yet.\n",
    "In particular, not even its input data points have integer components.\n",
    "Therefore, as a first step along the integerisation pass we need to create `DataLoader`s that emit data points with integer components.\n",
    "\n",
    "As pointed out earlier, the ground truth files containing CIFAR-10 data points encode images using the RGB colour model.\n",
    "Each pixel is a triple of 8-bit bytes, each of which encodes the intensity of a different colour channel.\n",
    "However, `torchvision`'s importer objects scale each value down by a factor of $255$, returning arrays of floating-point numbers in the range $[0, 1]$.\n",
    "Then, we applied the shift-and-scale normalisation procedure.\n",
    "These operations involve integer-to-floating casting operations and floating-point operations that are not necessarily precisely invertible: therefore, retrieving the original integer pixel values from the pre-processed floating-point values is likely to be tedious and imprecise.\n",
    "\n",
    "Therefore, we opted for the following solution: instead of writing our own importer for CIFAR-10 images with integer components and computing analytically the quantum associated with our pre-processing (which would essentially amount to exactly inverting the pre-processing function), we simply add an additional pre-processing step that takes in input the cast-to-floating and shifted-and-scaled pixels, and outputs integerised versions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e79525b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_range(data_loader: torch.utils.data.DataLoader) -> Tuple[float, float]:\n",
    "    \"\"\"Traverse the available data set to get the empirical range of input pixels.\"\"\"\n",
    "    min_ = 0.0\n",
    "    max_ = 0.0\n",
    "\n",
    "    for x, _ in data_loader:\n",
    "        min_ = min(min_, x.min().item())\n",
    "        max_ = max(max_, x.max().item())\n",
    "    \n",
    "    return min_, max_\n",
    "\n",
    "\n",
    "def add_quantisation_transform(data_loader: torch.utils.data.DataLoader, n_levels: int, min_: float, max_: float) -> float:\n",
    "    \n",
    "    quantiser = qa.pact.PACTAsymmetricAct(n_levels=n_levels, symm=True, learn_clip=False, init_clip='max', act_kind='identity')\n",
    "\n",
    "    clip_lo, clip_hi       = qa.pact.util.almost_symm_quant(torch.Tensor([max(abs(min_), abs(max_))]), n_levels)\n",
    "    quantiser.clip_lo.data = clip_lo\n",
    "    quantiser.clip_hi.data = clip_hi\n",
    "    \n",
    "    quantiser.started |= True\n",
    "    \n",
    "    transform_list  = []\n",
    "    transform_list += [data_loader.dataset.transform]\n",
    "    transform_list += [quantiser]\n",
    "    transform_list += [transforms.Lambda(lambda x: x / quantiser.get_eps())]\n",
    "    \n",
    "    data_loader.dataset.transform = transforms.Compose(transform_list)\n",
    "    \n",
    "    return quantiser.get_eps()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d338fb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input_levels = 2**8\n",
    "min_, max_     = get_input_range(valid_loader)\n",
    "\n",
    "input_eps = add_quantisation_transform(valid_loader, n_input_levels, min_, max_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959850ce",
   "metadata": {},
   "source": [
    "The components of the data points produce by the updated `DataLoader` are 8-bit signed integers (i.e., they are of the `INT8` data type).\n",
    "We can now proceed to the fake-to-true conversion step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3f4dfddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import quantlib.editing.fx as qfx\n",
    "\n",
    "\n",
    "def f2t_convert(dataloader: torch.utils.data.DataLoader,\n",
    "                input_eps:  float,\n",
    "                network:    nn.Module) -> nn.Module:\n",
    "\n",
    "    network.eval()\n",
    "    network = network.to(device=torch.device('cpu'))\n",
    "    \n",
    "    x, _ = dataloader.dataset.__getitem__(0)\n",
    "    x    = x.unsqueeze(0)\n",
    "    \n",
    "    fake2true_converter = qfx.passes.pact.IntegerizePACTNetPass(shape_in=x.shape, eps_in=input_eps, D=2**19)\n",
    "    \n",
    "    return fake2true_converter(pact_network)\n",
    "\n",
    "#export_net(tq_pact_network,\n",
    "#           name='VGG_PACT',\n",
    "#           out_dir='ONNX', eps_in=CIFAR10_STATS_QUANTISE['eps'], integerize=False, D=2**19, in_data=dummy_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4ea66321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key <class 'torch.nn.modules.batchnorm.BatchNorm2d'> not found in _EPS_CONVERSIONS!\n",
      "Using identity epsilon propagation on node with op call_module, target pilot.1!\n",
      "key <class 'torch.nn.modules.batchnorm.BatchNorm2d'> not found in _EPS_CONVERSIONS!\n",
      "Using identity epsilon propagation on node with op call_module, target features.1!\n",
      "key <class 'torch.nn.modules.pooling.MaxPool2d'> not found in _EPS_CONVERSIONS!\n",
      "Using identity epsilon propagation on node with op call_module, target features.3!\n",
      "key <class 'torch.nn.modules.batchnorm.BatchNorm2d'> not found in _EPS_CONVERSIONS!\n",
      "Using identity epsilon propagation on node with op call_module, target features.5!\n",
      "key <class 'torch.nn.modules.batchnorm.BatchNorm2d'> not found in _EPS_CONVERSIONS!\n",
      "Using identity epsilon propagation on node with op call_module, target features.8!\n",
      "key <class 'torch.nn.modules.pooling.MaxPool2d'> not found in _EPS_CONVERSIONS!\n",
      "Using identity epsilon propagation on node with op call_module, target features.10!\n",
      "key <class 'torch.nn.modules.batchnorm.BatchNorm2d'> not found in _EPS_CONVERSIONS!\n",
      "Using identity epsilon propagation on node with op call_module, target features.12!\n",
      "key <class 'torch.nn.modules.batchnorm.BatchNorm2d'> not found in _EPS_CONVERSIONS!\n",
      "Using identity epsilon propagation on node with op call_module, target features.15!\n",
      "key <class 'torch.nn.modules.pooling.MaxPool2d'> not found in _EPS_CONVERSIONS!\n",
      "Using identity epsilon propagation on node with op call_module, target features.17!\n",
      "key <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'> not found in _EPS_CONVERSIONS!\n",
      "Using identity epsilon propagation on node with op call_module, target avgpool!\n",
      "key _CALL_METHOD_size not found in _EPS_CONVERSIONS!\n",
      "Using identity epsilon propagation on node with op call_method, target size!\n",
      "key _CALL_METHOD_view not found in _EPS_CONVERSIONS!\n",
      "Using identity epsilon propagation on node with op call_method, target view!\n",
      "key <class 'torch.nn.modules.batchnorm.BatchNorm1d'> not found in _EPS_CONVERSIONS!\n",
      "Using identity epsilon propagation on node with op call_module, target classifier.1!\n",
      "key <class 'torch.nn.modules.batchnorm.BatchNorm1d'> not found in _EPS_CONVERSIONS!\n",
      "Using identity epsilon propagation on node with op call_module, target classifier.4!\n",
      "key _OUTPUT_output not found in _EPS_CONVERSIONS!\n",
      "Using identity epsilon propagation on node with op output, target output!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spmatteo/anaconda3/envs/quantlab/lib/python3.8/site-packages/torch/fx/graph.py:606: UserWarning: Attempted to insert a call_module Node with no underlying reference in the owning GraphModule! Call GraphModule.add_submodule to add the necessary submodule\n",
      "  warnings.warn(\"Attempted to insert a call_module Node with \"\n"
     ]
    }
   ],
   "source": [
    "tq_pact_network = f2t_convert(valid_loader, input_eps, pact_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1440b9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True: Airplane - Predicted: Airplane\n"
     ]
    }
   ],
   "source": [
    "x, y_gt_int = valid_loader.dataset.__getitem__(random.randrange(0, len(valid_loader.dataset)))\n",
    "\n",
    "x = x.unsqueeze(0)\n",
    "y_pr = tq_pact_network(x)\n",
    "y_pr_int = y_pr.argmax(axis=1)\n",
    "\n",
    "print(\"True: {} - Predicted: {}\".format(int2label[y_gt_int], int2label[y_pr_int.item()]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a0d4a238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import quantlib.backends as qb\n",
    "\n",
    "\n",
    "def create_vgg_dir_export(dir_logs: os.PathLike) -> os.PathLike:\n",
    "    dir_export = os.path.join(dir_logs, 'onnx_exports')\n",
    "    if not os.path.isdir(dir_export):\n",
    "        os.makedirs(dir_export, exist_ok=True)\n",
    "    return dir_export\n",
    "\n",
    "def export_network(data_loader: torch.utils.data.DataLoader,\n",
    "                   input_eps:   float,\n",
    "                   network:     nn.Module,\n",
    "                   filename:    str,\n",
    "                   dir_export:  os.PathLike) -> None:\n",
    "    \n",
    "    network.eval()\n",
    "    \n",
    "    x, _ = data_loader.dataset.__getitem__(random.randrange(0, len(data_loader.dataset)))\n",
    "    x = x.unsqueeze(0)\n",
    "    \n",
    "    qb.dory.export_net(network,\n",
    "                       name=filename,\n",
    "                       out_dir=dir_export,\n",
    "                       eps_in=input_eps,\n",
    "                       integerize=False,\n",
    "                       D=2**19,\n",
    "                       in_data=x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d37038d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not permuting output of layer output...\n",
      "Not permuting output of layer classifier._QL_REPLACED__INTEGERIZE_BN1D_UNSIGNED_ACT_PASS_0...\n",
      "Not permuting output of layer classifier._QL_REPLACED__INTEGERIZE_BN1D_UNSIGNED_ACT_PASS_1...\n"
     ]
    }
   ],
   "source": [
    "dir_export = create_vgg_dir_export(dir_logs)\n",
    "export_network(valid_loader, input_eps, tq_pact_network, 'VGG_PACT_DORY.onnx', dir_export)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f9169e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
