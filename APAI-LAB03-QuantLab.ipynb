{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "781ec3b1",
   "metadata": {},
   "source": [
    "# QuantLab\n",
    "\n",
    "This notebook is a companion to the material presented during the lecture held on Friday, 29th October 2021, in the scope of the \"Architecture and Platforms for Artificial Intelligence\" (APAI) course offered by the University of Bologna.\n",
    "\n",
    "The notebook is structured into three parts:\n",
    "* [**introduction to PyTorch**](#sec:pytorch); this section is a brief tutorial on using the PyTorch deep learning framework to describe and train DNNs;\n",
    "* [**training QNNs with the PACT algorithm**](#sec:float2fake); this section shows how to train mixed-precision QNNs using the `quantlib` Python package, an add-on for PyTorch developed to support the training and manipulation of QNNs;\n",
    "* [**exporting trained QNNs to integerised ONNX IRs**](#sec:fake2true); this section shows how to use `quantlib` to convert a trained PyTorch QNN into an ONNX intermediate representation specifically annotated to be processed by DORY, a deployment tool targetting the PULP platform.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d2a822",
   "metadata": {},
   "source": [
    "To facilitate the understanding of the code, it is useful to annotate functions with the types of their inputs and outputs.\n",
    "Therefore, in this notebook we will extensively use the `typing` Python package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e48db6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "from typing import Union, Tuple, List, Dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5979aab3",
   "metadata": {},
   "source": [
    "<a id='sec:pytorch'></a>\n",
    "## Introduction to PyTorch\n",
    "\n",
    "In this section, we will describe and train a simple deep convolutional neural network using PyTorch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af0f6a3",
   "metadata": {},
   "source": [
    "Let's start by defining a fully feedforward, VGG-like network topology.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2cbb37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "_CONFIGS = {\n",
    "    'VGG8':  ['M', 256, 256, 'M', 512, 512, 'M'],\n",
    "    'VGG9':  [128, 'M', 256, 256, 'M', 512, 512, 'M'],\n",
    "    'VGG11': [128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "\n",
    "class VGG(nn.Module):\n",
    "\n",
    "    def __init__(self, config: str) -> None:\n",
    "\n",
    "        super(VGG, self).__init__()\n",
    "\n",
    "        self.pilot      = self._make_pilot(config)\n",
    "        self.features   = self._make_features(config)\n",
    "        self.avgpool    = self._make_avgpool(config)\n",
    "        self.classifier = self._make_classifier(config)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_pilot(config: str) -> nn.Sequential:\n",
    "\n",
    "        out_channels = 128\n",
    "        modules = []\n",
    "\n",
    "        modules += [nn.Conv2d(3, out_channels, kernel_size=3, padding=1, bias=False)]\n",
    "        modules += [nn.BatchNorm2d(out_channels)]\n",
    "        modules += [nn.ReLU(inplace=True)]\n",
    "\n",
    "        return nn.Sequential(*modules)\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_features(config: str) -> nn.Sequential:\n",
    "\n",
    "        in_channels = 128\n",
    "        modules = []\n",
    "\n",
    "        for v in _CONFIGS[config]:\n",
    "            if v == 'M':\n",
    "                modules += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                out_channels = v\n",
    "                modules += [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)]\n",
    "                modules += [nn.BatchNorm2d(out_channels)]\n",
    "                modules += [nn.ReLU(inplace=True)]\n",
    "                in_channels = out_channels\n",
    "\n",
    "        return nn.Sequential(*modules)\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_avgpool(config: str):\n",
    "        return nn.AdaptiveAvgPool2d((4, 4))\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_classifier(config: str) -> nn.Sequential:\n",
    "\n",
    "        modules = []\n",
    "\n",
    "        modules += [nn.Linear(512 * 4 * 4, 1024, bias=False)]\n",
    "        modules += [nn.BatchNorm1d(1024)]\n",
    "        modules += [nn.ReLU(inplace=True)]\n",
    "        modules += [nn.Linear(1024, 1024, bias=False)]\n",
    "        modules += [nn.BatchNorm1d(1024)]\n",
    "        modules += [nn.ReLU(inplace=True)]\n",
    "        modules += [nn.Linear(1024, 10)]\n",
    "\n",
    "        return nn.Sequential(*modules)\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "\n",
    "        for m in self.modules():\n",
    "\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        x = self.pilot(x)\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)  # https://stackoverflow.com/questions/57234095/what-is-the-difference-of-flatten-and-view-1-in-pytorch\n",
    "\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149b9d82",
   "metadata": {},
   "source": [
    "A first sanity check: we verify that the network can process CIFAR-10 data points (see below for details on CIFAR-10).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca1b935a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (pilot): Sequential(\n",
      "    (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (12): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(4, 4))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (4): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "`dummy_x` shape: torch.Size([1, 3, 32, 32])\n",
      "`dummy_y` shape: torch.Size([1, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spmatteo/anaconda3/envs/quantlab/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448234945/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(torch.cuda.current_device() if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# create the network\n",
    "network = VGG('VGG9')\n",
    "network = network.to(device=device)  # REMEMBER: place the parameters of the 'Module' on the device that guarantees the best performance\n",
    "print(network)\n",
    "print()\n",
    "\n",
    "# verify that it can process tensor data\n",
    "network.eval()  # REMEMBER: before evaluating a network, freeze the batch-normalisation and dropout parameters\n",
    "dummy_x = torch.randn(1, 3, 32, 32).to(device=device)\n",
    "dummy_y = network(dummy_x)\n",
    "\n",
    "print(\"`dummy_x` shape: {}\".format(dummy_x.shape))\n",
    "print(\"`dummy_y` shape: {}\".format(dummy_y.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd24d0f",
   "metadata": {},
   "source": [
    "### The CIFAR-10 data set\n",
    "\n",
    "The ten-classes Canadian institute for advanced research (CIFAR-10) data set contains $60000$ images depicting objects belonging to ten different classes.\n",
    "Each image has a resolution of $32 \\times 32$ pixels, and is encoded using the RGB color model.\n",
    "\n",
    "Each class is represented by exactly $60000 / 10 = 6000$ images, $5000$ of which belong to the training partition and $1000$ of which belong to the validation partition.\n",
    "Overall, the training partition contains $50000$ data points, whereas the validation partition contains $10000$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64810b61",
   "metadata": {},
   "source": [
    "We start by downloading the data set.\n",
    "Then, we create two distinct PyTorch `Dataset` objects: one for the training partition and one for the validation partition.\n",
    "\n",
    "`Dataset`s are PyTorch abstractions to map files (stored on disk) to objects in the space allocated to the Python process executing PyTorch.\n",
    "`Dataset`s can be attached transforms (`transform` for the input points, `target_transform` for the labels), object representing pre-processing functions that should be applied to the raw data points before they are returned to the Python process space.\n",
    "A `Dataset` object can be queried for a data point by passing an integer index to the `__getitem__` method; when this happens, the `Dataset` objects looks up the path of the file from an internal look-up table, applies a user-defined function to load the file into a raw Python object, and finally applies the `transform`s defined by the user.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58c5937f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "def create_cifar10_dir_data() -> os.PathLike:\n",
    "    \"\"\"Create a directory where `torchvision` should store CIFAR-10 data.\"\"\"\n",
    "    dir_data = os.path.join(os.curdir, 'data_cifar10')\n",
    "    if not os.path.isdir(dir_data):\n",
    "        os.makedirs(dir_data, exist_ok=True)\n",
    "\n",
    "    return dir_data\n",
    "    \n",
    "\n",
    "def load_cifar10_data_set(dir_data: os.PathLike, train: bool) -> torch.utils.data.Dataset:\n",
    "    \n",
    "    # define the pre-processing transform that will be applied to each data point\n",
    "    transform_list = []\n",
    "    if train:\n",
    "        transform_list += [transforms.RandomHorizontalFlip()]\n",
    "        transform_list += [transforms.ToTensor()]\n",
    "    else:\n",
    "        transform_list += [transforms.ToTensor()]\n",
    "    transform = transforms.Compose(transform_list)\n",
    "    \n",
    "    # load the data into `Dataset` objects (PyTorch exposes specialised loader objects for CIFAR-10: https://pytorch.org/vision/stable/datasets.html#torchvision.datasets.CIFAR10)\n",
    "    data_set = torchvision.datasets.CIFAR10(root=dir_data, train=train, download=True, transform=transform)\n",
    "\n",
    "    return data_set\n",
    "\n",
    "\n",
    "def load_cifar10_int2label(dir_data: os.PathLike) -> Dict[int, str]:\n",
    "\n",
    "    # see https://www.cs.toronto.edu/~kriz/cifar.html to understand how the CIFAR-10 data is organised\n",
    "    labels_file = os.path.join(dir_data, 'cifar-10-batches-py', 'batches.meta')\n",
    "    with open(labels_file, 'rb') as fp:\n",
    "        labels = {i: name.capitalize() for i, name in enumerate(pickle.load(fp)['label_names'])}\n",
    "    \n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a5e25b",
   "metadata": {},
   "source": [
    "A sanity check: we verify that the `Dataset`s actually contain the number of data points that we expect from CIFAR-10, and have a look at their classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17aaf26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Training data set contains 50000 data points.\n",
      "Validation data set contains 10000 data points.\n",
      "\n",
      "CIFAR-10 classes: \n",
      "0: Airplane\n",
      "1: Automobile\n",
      "2: Bird\n",
      "3: Cat\n",
      "4: Deer\n",
      "5: Dog\n",
      "6: Frog\n",
      "7: Horse\n",
      "8: Ship\n",
      "9: Truck\n"
     ]
    }
   ],
   "source": [
    "dir_data   = create_cifar10_dir_data()\n",
    "train_data = load_cifar10_data_set(dir_data, train=True)\n",
    "valid_data = load_cifar10_data_set(dir_data, train=False)\n",
    "int2label  = load_cifar10_int2label(dir_data)\n",
    "\n",
    "print()\n",
    "print(\"Training data set contains {} data points.\".format(len(train_data)))\n",
    "print(\"Validation data set contains {} data points.\".format(len(valid_data)))\n",
    "print()\n",
    "print(\"CIFAR-10 classes: \")\n",
    "for i, name in int2label.items():\n",
    "    print(\"{}: {}\".format(i, name))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548b4fc6",
   "metadata": {},
   "source": [
    "Visualising the data you are working on is usually motivating and educational.\n",
    "Therefore, we define a utility function that can show us labelled images sampled at random from one of the two partitions of the CIFAR-10 data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15ec0a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def show_random_cifar10_image(data_set: torch.utils.data.Dataset, int2label: Dict[int, str]) -> None:\n",
    "    x, y = data_set.__getitem__(random.randrange(0, len(data_set)))  # https://docs.python.org/3/library/random.html#random.randrange\n",
    "    plt.imshow(x.permute(1, 2, 0))\n",
    "    plt.title(int2label[y])  # https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.title.html\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a130ed22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# REPLAY: this cell should show different CIFAR-10 data points\n",
    "show_random_cifar10_image(valid_data, int2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfec5fd",
   "metadata": {},
   "source": [
    "The cost of training DNNs on real hardware can be optimised by means of mini-batching.\n",
    "Indeed, since each data point is processed independently of the others we can rewrite the collection of vector-matrix products\n",
    "$$(\\mathbf{x}^{(i-1, 1)} W^{(i)}, \\dots, \\mathbf{x}^{(i, K)} W^{(i)})$$\n",
    "as a single matrix-matrix product\n",
    "$$X^{(i-1)} W^{(i)} \\,,$$\n",
    "where $X^{(i-1)}_{(k)} := \\mathbf{x}^{(i-1, k)}, k = 1, \\dots, K$.\n",
    "This rewriting allows minimising the number of memory transactions on host-device computing systems (e.g., those using GPGPUs) and maximising the utilisation of data movement components, as well as minimising the number of idle processing elements on parallel hardware (e.g., the CUDA cores of NVidia GPGPUs).\n",
    "\n",
    "The PyTorch mechanism to create mini-batches of data points consists of two components:\n",
    "* a `Sampler`, which implements the policy to split the complete data set into mini-batches; for instance, at training time it is important that the distribution of data inside a mini-batch is as heterogeneous as possible, to avoid overfitting the model's parameters to just a few classes;\n",
    "* a `DataLoader`, which queries the `Sampler` for the indices of the data points that will compose the next batch, reads the associated data points from the `Dataset` (which applies the assigned pre-processing `transform` to each data point as soon as it is loaded from disk), and composes them into batches ready to be fed to the PyTorch DNN that we want to train.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44c2b774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(data_set: torch.utils.data.Dataset, train: bool, batch_size: int) -> torch.utils.data.DataLoader:\n",
    "    \n",
    "    if train:\n",
    "        sampler = torch.utils.data.RandomSampler(data_set)\n",
    "    else:\n",
    "        sampler = torch.utils.data.SequentialSampler(data_set)\n",
    "        \n",
    "    loader = torch.utils.data.DataLoader(data_set, batch_size=batch_size, sampler=sampler)\n",
    "    \n",
    "    return loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7177d0a9",
   "metadata": {},
   "source": [
    "A sanity check: we verify that the `DataLoader`s actually return `Tensor`s of the size that we expect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a516b252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input `torch.Tensor`: torch.Size([64, 3, 32, 32])\n",
      "Shape of label array: 64\n"
     ]
    }
   ],
   "source": [
    "bs = 64\n",
    "train_loader = create_data_loader(train_data, True,  bs)\n",
    "valid_loader = create_data_loader(valid_data, False, bs)\n",
    "\n",
    "\n",
    "bx, by = next(iter(train_loader))\n",
    "print(\"Shape of input `torch.Tensor`: {}\".format(bx.shape))\n",
    "print(\"Shape of label array: {}\".format(len(by)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cfefce",
   "metadata": {},
   "source": [
    "### Training DNNs\n",
    "\n",
    "\n",
    "#### DNN training as an optimisation problem.\n",
    "\n",
    "In principle, the optimal DNN $f^{*}$ minimises the loss functional with respect to the true data distribution $\\mu$:\n",
    "$$f^{*} := \\underset{f \\in \\mathcal{F}}{\\arg\\min} \\int_{X \\times Y} \\ell(f(x), y) \\, d\\mu(x, y) \\,.$$\n",
    "Here, $\\ell : Y \\times Y \\to \\mathbb{R}^{+}_{0}$ is the loss function, a function which should satisfy $\\ell(f(x), y) = 0 \\iff f(x) = y$.\n",
    "\n",
    "Since the DNN $f$ is parametrised by $\\theta \\in \\Theta$, we can rewrite this functional optimisation problem as the minimisation of an integral equation that depends on $\\theta$:\n",
    "$$\\theta^{*} := \\underset{\\theta \\in \\Theta}{\\arg\\min} \\int_{X \\times Y} \\ell(f(\\theta, x), y) \\, d\\mu(x, y) \\,.$$\n",
    "\n",
    "One of the standard strategies to minimise differentiable functions is using gradient descent, where the gradient\n",
    "$$g^{t} := \\nabla_{\\theta} \\left( \\int_{X \\times Y} \\ell(f(\\theta^{t}, x), y) \\, d\\mu(x, y) \\right)$$\n",
    "plays the role of the learning signal.\n",
    "<!--- Under certain hypothesis, exchangeability theorems (e.g., Fubini's theorem) can be applied to bring the gradient under the integral sign:\n",
    "$$g^{t} = \\eta \\int_{X \\times Y} \\left( \\nabla_{\\theta} \\ell(f(\\theta^{t}, x), y) \\right) \\, d\\mu(x, y) \\,.$$ --->\n",
    "\n",
    "Apart from technicalities, the most important issue of this formalism lies in the fact that the true data distribution $\\mu$ is unknown.\n",
    "In practical cases, the true measure is approximated by the empirical measure associated with an available, finite data set $\\mathcal{D} = ((x^{(1)}, y^{(1)}), \\dots, (x^{(N)}, y^{(N)}))$:\n",
    "$$\\mu \\approx m := \\frac{1}{N} \\sum_{n=1}^{N} \\delta_{(x^{(n)}, y^{(n)})} \\,.$$\n",
    "\n",
    "When plugged into the loss functional, this empirical measure yields the following learning signal:\n",
    "\\begin{align*}\n",
    "    g^{t}\n",
    "    &= \\nabla_{\\theta} \\left( \\frac{1}{N} \\sum_{n=1}^{N} \\ell(f(\\theta^{t}, x^{(n)}), y^{(n)}) \\right) \\\\\n",
    "    &= \\frac{1}{N} \\sum_{n=1}^{N} \\left( \\nabla_{\\theta} \\ell(f(\\theta^{t}, x^{(n)}), y^{(n)}) \\right) \\,.\n",
    "\\end{align*}\n",
    "\n",
    "Note that when the data set $\\mathcal{D}$ is fixed, the value $g^{t}$ of the gradient computed with respect to a specific value $\\theta^{t}$ of the variable is deterministic; however, the values of $g$ computed for different values of $\\theta$ can still differ.\n",
    "\n",
    "\n",
    "#### Learning rules for DNNs\n",
    "\n",
    "Gradient descent optimisation algorithms are usually iterative, meaning that the target variable is optimised from an initial condition $\\theta^{0}$ to a final condition $\\theta^{T}$ with the hope that $\\theta^{T} \\approx \\theta^{*}$.\n",
    "In the scope of machine learning, the update rule that describes how to go from $\\theta^{t}$ to $\\theta^{t+1}$ (i.e., the one-step evolution of the system), is called the learning rule.\n",
    "\n",
    "Several gradient-based learning rules have been proposed to train DNNs.\n",
    "The simplest one is stochastic gradient descent (SGD).\n",
    "In this case, the idea is to approximate $\\mu$ with a different empirical distribution at each iteration:\n",
    "$$\\mu \\approx m^{t} := \\frac{1}{B} \\sum_{b=1}^{B} \\delta_{(x^{(t, b)}, y^{(t, b)})} \\,;$$\n",
    "the collection $((x^{(t, 1)}, y^{(t, 1)}), \\dots, (x^{(t, B)}, y^{(t, B)}))$ is called the $t$-th mini-batch, and it is sampled from the complete data set $\\mathcal{D}$ ($1 \\leq B \\ll N$).\n",
    "Due to this mini-batch sampling, $g$ can take on different values even when computed for the same value of the variable $\\theta$; in other words, the dependency of $g^{t}$ on $\\theta^{t}$ is stochastic, not deterministic, hence the method's name.\n",
    "The update rule then is as simple as\n",
    "$$\\theta^{t+1} = \\theta^{t} - \\eta g^{t} \\,;$$\n",
    "accordingly, the update is $\\Delta \\theta^{t} := \\theta^{t+1} - \\theta^{t} = - \\eta g^{t}$.\n",
    "\n",
    "Note that gradients and updates are two different concepts: updates are computed using gradients, but gradients do not define updates by themselves.\n",
    "\n",
    "\n",
    "#### Optimisation in Pytorch\n",
    "\n",
    "PyTorch implements loss functions as specific `Module`s.\n",
    "Some loss `Module`s automatically compute the average value of the loss function with respect to all the data points in a mini-batch; others do not do it automatically.\n",
    "\n",
    "It is possible to compute gradients using reverse-mode automatic differentiation (also known as back-propagation) with a simple call to the `backward` method of the `Tensor` holding the aggregated loss value.\n",
    "The gradients of the parameter and feature `Tensor`s are stored in specific `grad` attributes attached to the corresponding `Tensor` objects.\n",
    "\n",
    "The `torch.optim` module implements `Optimizer` objects associated with the most popular SGD variants (vanilla SGD, RMSProp, Adam, ...).\n",
    "Remember: `Optimizer`s do not compute gradients; `Optimizer`s have a `step` method that uses gradients $g^{t}$ to compute updates $\\Delta \\theta^{t}$ and apply them to the parameter `Tensor`s.\n",
    "\n",
    "Multiple calls to `backward` on the `Tensor` holding the loss value accumulate gradients in the `grad` attributes of parameter and feature `Tensor`s.\n",
    "Therefore, it is important to remember to zero-out the gradients stored in `grad` attributes after every call to the `step` method of the chosen `Optimizer`, otherwise the \"gradient\" used by the `Optimizer` will not correspond to the correct gradient that it is supposed to use.\n",
    "\n",
    "<!--- Sometimes, forgetting to reset the gradients to zero might prevent the network from learning: indeed, if we suppose that gradients follow a multi-dimensional normal distribution, their sum will eventually zero out, leading to vanishing gradients in the span of a few training iterations. --->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "084edbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "loss_fn   = nn.CrossEntropyLoss()\n",
    "optimiser = optim.Adam(network.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6989b3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(device:       torch.device,\n",
    "        train_loader: torch.utils.data.DataLoader,\n",
    "        valid_loader: torch.utils.data.DataLoader,\n",
    "        network:      nn.Module,\n",
    "        loss_fn:      nn.Module,\n",
    "        optimiser:    torch.optim.Optimizer,\n",
    "        n_epochs:     int) -> None:\n",
    "\n",
    "    for i_epoch in range(0, n_epochs):\n",
    "    \n",
    "        # training pass\n",
    "        network.train()  # REMEMBER: before training a network, release the batch-normalisation and dropout parameters\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        for i_batch, (x, y_gt_int) in enumerate(train_loader):\n",
    "\n",
    "            x        = x.to(device=device)\n",
    "            y_gt_int = y_gt_int.to(device=device)\n",
    "\n",
    "            y_pr       = network(x)\n",
    "            loss_value = loss_fn(y_pr, y_gt_int)\n",
    "\n",
    "            loss_value.backward()  # back-propagation using torch's `autograd`\n",
    "            optimiser.step()\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "            print(\"Epoch[{:02d}/{:02d}] | Iteration[{:04d}/{:04d}] - Loss value: {}\".format(i_epoch, n_epochs, i_batch, len(train_loader), loss_value.item()))\n",
    "\n",
    "        # validation pass\n",
    "        network.eval()  # REMEMBER: before evaluating a network, freeze the batch-normalisation and dropout parameters\n",
    "        correct = 0\n",
    "\n",
    "        for x, y_gt_int in valid_loader:\n",
    "            \n",
    "            x        = x.to(device=device)\n",
    "            y_gt_int = y_gt_int.to(device=device)\n",
    "\n",
    "            y_pr     = network(x)\n",
    "            y_pr_int = y_pr.argmax(axis=1)  # the position of the neuron with the highest score encodes the predicted class\n",
    "\n",
    "            correct += torch.sum(y_pr_int == y_gt_int).item()\n",
    "\n",
    "        print(\"Epoch[{:02d}/{:02d}] - Validation accuracy: {:6.2f}%\".format(i_epoch, n_epochs, 100.0 * correct / len(valid_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe72474",
   "metadata": {},
   "source": [
    "We can now train our first training epoch!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b4c6e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[00/01] | Iteration[0000/0782] - Loss value: 2.2902233600616455\n",
      "Epoch[00/01] | Iteration[0001/0782] - Loss value: 2.168121576309204\n",
      "Epoch[00/01] | Iteration[0002/0782] - Loss value: 1.9841877222061157\n",
      "Epoch[00/01] | Iteration[0003/0782] - Loss value: 2.174591541290283\n",
      "Epoch[00/01] | Iteration[0004/0782] - Loss value: 2.1853742599487305\n",
      "Epoch[00/01] | Iteration[0005/0782] - Loss value: 1.8999896049499512\n",
      "Epoch[00/01] | Iteration[0006/0782] - Loss value: 1.9005472660064697\n",
      "Epoch[00/01] | Iteration[0007/0782] - Loss value: 1.9861276149749756\n",
      "Epoch[00/01] | Iteration[0008/0782] - Loss value: 2.0047085285186768\n",
      "Epoch[00/01] | Iteration[0009/0782] - Loss value: 1.9111617803573608\n",
      "Epoch[00/01] | Iteration[0010/0782] - Loss value: 1.7197151184082031\n",
      "Epoch[00/01] | Iteration[0011/0782] - Loss value: 1.9780986309051514\n",
      "Epoch[00/01] | Iteration[0012/0782] - Loss value: 1.7935079336166382\n",
      "Epoch[00/01] | Iteration[0013/0782] - Loss value: 1.9482579231262207\n",
      "Epoch[00/01] | Iteration[0014/0782] - Loss value: 1.960370659828186\n",
      "Epoch[00/01] | Iteration[0015/0782] - Loss value: 1.7399582862854004\n",
      "Epoch[00/01] | Iteration[0016/0782] - Loss value: 1.5310410261154175\n",
      "Epoch[00/01] | Iteration[0017/0782] - Loss value: 1.7245302200317383\n",
      "Epoch[00/01] | Iteration[0018/0782] - Loss value: 1.802449107170105\n",
      "Epoch[00/01] | Iteration[0019/0782] - Loss value: 1.7662326097488403\n",
      "Epoch[00/01] | Iteration[0020/0782] - Loss value: 1.7120733261108398\n",
      "Epoch[00/01] | Iteration[0021/0782] - Loss value: 1.623373031616211\n",
      "Epoch[00/01] | Iteration[0022/0782] - Loss value: 1.8620452880859375\n",
      "Epoch[00/01] | Iteration[0023/0782] - Loss value: 1.4978761672973633\n",
      "Epoch[00/01] | Iteration[0024/0782] - Loss value: 1.8165411949157715\n",
      "Epoch[00/01] | Iteration[0025/0782] - Loss value: 1.780747890472412\n",
      "Epoch[00/01] | Iteration[0026/0782] - Loss value: 1.7955228090286255\n",
      "Epoch[00/01] | Iteration[0027/0782] - Loss value: 1.7542083263397217\n",
      "Epoch[00/01] | Iteration[0028/0782] - Loss value: 1.8621023893356323\n",
      "Epoch[00/01] | Iteration[0029/0782] - Loss value: 1.6638085842132568\n",
      "Epoch[00/01] | Iteration[0030/0782] - Loss value: 1.738323450088501\n",
      "Epoch[00/01] | Iteration[0031/0782] - Loss value: 1.6799499988555908\n",
      "Epoch[00/01] | Iteration[0032/0782] - Loss value: 1.4420130252838135\n",
      "Epoch[00/01] | Iteration[0033/0782] - Loss value: 1.7209248542785645\n",
      "Epoch[00/01] | Iteration[0034/0782] - Loss value: 1.5370498895645142\n",
      "Epoch[00/01] | Iteration[0035/0782] - Loss value: 1.544639229774475\n",
      "Epoch[00/01] | Iteration[0036/0782] - Loss value: 1.5087238550186157\n",
      "Epoch[00/01] | Iteration[0037/0782] - Loss value: 1.6300421953201294\n",
      "Epoch[00/01] | Iteration[0038/0782] - Loss value: 1.5535911321640015\n",
      "Epoch[00/01] | Iteration[0039/0782] - Loss value: 1.5513522624969482\n",
      "Epoch[00/01] | Iteration[0040/0782] - Loss value: 1.7762751579284668\n",
      "Epoch[00/01] | Iteration[0041/0782] - Loss value: 1.5973875522613525\n",
      "Epoch[00/01] | Iteration[0042/0782] - Loss value: 1.5923258066177368\n",
      "Epoch[00/01] | Iteration[0043/0782] - Loss value: 1.7604141235351562\n",
      "Epoch[00/01] | Iteration[0044/0782] - Loss value: 1.6399924755096436\n",
      "Epoch[00/01] | Iteration[0045/0782] - Loss value: 1.4980815649032593\n",
      "Epoch[00/01] | Iteration[0046/0782] - Loss value: 1.6588013172149658\n",
      "Epoch[00/01] | Iteration[0047/0782] - Loss value: 1.5973389148712158\n",
      "Epoch[00/01] | Iteration[0048/0782] - Loss value: 1.463904857635498\n",
      "Epoch[00/01] | Iteration[0049/0782] - Loss value: 1.4954804182052612\n",
      "Epoch[00/01] | Iteration[0050/0782] - Loss value: 1.5787901878356934\n",
      "Epoch[00/01] | Iteration[0051/0782] - Loss value: 1.5684901475906372\n",
      "Epoch[00/01] | Iteration[0052/0782] - Loss value: 1.4727809429168701\n",
      "Epoch[00/01] | Iteration[0053/0782] - Loss value: 1.4987579584121704\n",
      "Epoch[00/01] | Iteration[0054/0782] - Loss value: 1.811376690864563\n",
      "Epoch[00/01] | Iteration[0055/0782] - Loss value: 1.6454668045043945\n",
      "Epoch[00/01] | Iteration[0056/0782] - Loss value: 1.7017197608947754\n",
      "Epoch[00/01] | Iteration[0057/0782] - Loss value: 1.5681869983673096\n",
      "Epoch[00/01] | Iteration[0058/0782] - Loss value: 1.4486831426620483\n",
      "Epoch[00/01] | Iteration[0059/0782] - Loss value: 1.4045944213867188\n",
      "Epoch[00/01] | Iteration[0060/0782] - Loss value: 1.5950416326522827\n",
      "Epoch[00/01] | Iteration[0061/0782] - Loss value: 1.642713189125061\n",
      "Epoch[00/01] | Iteration[0062/0782] - Loss value: 1.6853073835372925\n",
      "Epoch[00/01] | Iteration[0063/0782] - Loss value: 1.491997241973877\n",
      "Epoch[00/01] | Iteration[0064/0782] - Loss value: 1.3474425077438354\n",
      "Epoch[00/01] | Iteration[0065/0782] - Loss value: 1.4883933067321777\n",
      "Epoch[00/01] | Iteration[0066/0782] - Loss value: 1.5942800045013428\n",
      "Epoch[00/01] | Iteration[0067/0782] - Loss value: 1.3872270584106445\n",
      "Epoch[00/01] | Iteration[0068/0782] - Loss value: 1.4356623888015747\n",
      "Epoch[00/01] | Iteration[0069/0782] - Loss value: 1.4477039575576782\n",
      "Epoch[00/01] | Iteration[0070/0782] - Loss value: 1.6070225238800049\n",
      "Epoch[00/01] | Iteration[0071/0782] - Loss value: 1.3143589496612549\n",
      "Epoch[00/01] | Iteration[0072/0782] - Loss value: 1.6038846969604492\n",
      "Epoch[00/01] | Iteration[0073/0782] - Loss value: 1.6710143089294434\n",
      "Epoch[00/01] | Iteration[0074/0782] - Loss value: 1.415352463722229\n",
      "Epoch[00/01] | Iteration[0075/0782] - Loss value: 1.6914390325546265\n",
      "Epoch[00/01] | Iteration[0076/0782] - Loss value: 1.4037935733795166\n",
      "Epoch[00/01] | Iteration[0077/0782] - Loss value: 1.504136323928833\n",
      "Epoch[00/01] | Iteration[0078/0782] - Loss value: 1.6162054538726807\n",
      "Epoch[00/01] | Iteration[0079/0782] - Loss value: 1.472984790802002\n",
      "Epoch[00/01] | Iteration[0080/0782] - Loss value: 1.1911815404891968\n",
      "Epoch[00/01] | Iteration[0081/0782] - Loss value: 1.6559544801712036\n",
      "Epoch[00/01] | Iteration[0082/0782] - Loss value: 1.516160249710083\n",
      "Epoch[00/01] | Iteration[0083/0782] - Loss value: 1.4759304523468018\n",
      "Epoch[00/01] | Iteration[0084/0782] - Loss value: 1.721327543258667\n",
      "Epoch[00/01] | Iteration[0085/0782] - Loss value: 1.5764925479888916\n",
      "Epoch[00/01] | Iteration[0086/0782] - Loss value: 1.2608226537704468\n",
      "Epoch[00/01] | Iteration[0087/0782] - Loss value: 1.4905954599380493\n",
      "Epoch[00/01] | Iteration[0088/0782] - Loss value: 1.4818681478500366\n",
      "Epoch[00/01] | Iteration[0089/0782] - Loss value: 1.4211692810058594\n",
      "Epoch[00/01] | Iteration[0090/0782] - Loss value: 1.5657373666763306\n",
      "Epoch[00/01] | Iteration[0091/0782] - Loss value: 1.566580891609192\n",
      "Epoch[00/01] | Iteration[0092/0782] - Loss value: 1.7957220077514648\n",
      "Epoch[00/01] | Iteration[0093/0782] - Loss value: 1.4370973110198975\n",
      "Epoch[00/01] | Iteration[0094/0782] - Loss value: 1.2424581050872803\n",
      "Epoch[00/01] | Iteration[0095/0782] - Loss value: 1.508760690689087\n",
      "Epoch[00/01] | Iteration[0096/0782] - Loss value: 1.7648404836654663\n",
      "Epoch[00/01] | Iteration[0097/0782] - Loss value: 1.503571629524231\n",
      "Epoch[00/01] | Iteration[0098/0782] - Loss value: 1.4882516860961914\n",
      "Epoch[00/01] | Iteration[0099/0782] - Loss value: 1.3417558670043945\n",
      "Epoch[00/01] | Iteration[0100/0782] - Loss value: 1.3392419815063477\n",
      "Epoch[00/01] | Iteration[0101/0782] - Loss value: 1.3697116374969482\n",
      "Epoch[00/01] | Iteration[0102/0782] - Loss value: 1.3829454183578491\n",
      "Epoch[00/01] | Iteration[0103/0782] - Loss value: 1.6397963762283325\n",
      "Epoch[00/01] | Iteration[0104/0782] - Loss value: 1.454973816871643\n",
      "Epoch[00/01] | Iteration[0105/0782] - Loss value: 1.5127995014190674\n",
      "Epoch[00/01] | Iteration[0106/0782] - Loss value: 1.4206229448318481\n",
      "Epoch[00/01] | Iteration[0107/0782] - Loss value: 1.3047629594802856\n",
      "Epoch[00/01] | Iteration[0108/0782] - Loss value: 1.283932089805603\n",
      "Epoch[00/01] | Iteration[0109/0782] - Loss value: 1.6419261693954468\n",
      "Epoch[00/01] | Iteration[0110/0782] - Loss value: 1.6180140972137451\n",
      "Epoch[00/01] | Iteration[0111/0782] - Loss value: 1.4172396659851074\n",
      "Epoch[00/01] | Iteration[0112/0782] - Loss value: 1.5141011476516724\n",
      "Epoch[00/01] | Iteration[0113/0782] - Loss value: 1.3772770166397095\n",
      "Epoch[00/01] | Iteration[0114/0782] - Loss value: 1.401593565940857\n",
      "Epoch[00/01] | Iteration[0115/0782] - Loss value: 1.3638895750045776\n",
      "Epoch[00/01] | Iteration[0116/0782] - Loss value: 1.2635456323623657\n",
      "Epoch[00/01] | Iteration[0117/0782] - Loss value: 1.5831185579299927\n",
      "Epoch[00/01] | Iteration[0118/0782] - Loss value: 1.3925421237945557\n",
      "Epoch[00/01] | Iteration[0119/0782] - Loss value: 1.3584516048431396\n",
      "Epoch[00/01] | Iteration[0120/0782] - Loss value: 1.2947636842727661\n",
      "Epoch[00/01] | Iteration[0121/0782] - Loss value: 1.1434952020645142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[00/01] | Iteration[0122/0782] - Loss value: 1.613304853439331\n",
      "Epoch[00/01] | Iteration[0123/0782] - Loss value: 1.3722941875457764\n",
      "Epoch[00/01] | Iteration[0124/0782] - Loss value: 1.3144620656967163\n",
      "Epoch[00/01] | Iteration[0125/0782] - Loss value: 1.570178747177124\n",
      "Epoch[00/01] | Iteration[0126/0782] - Loss value: 1.1697155237197876\n",
      "Epoch[00/01] | Iteration[0127/0782] - Loss value: 1.1758521795272827\n",
      "Epoch[00/01] | Iteration[0128/0782] - Loss value: 1.388213038444519\n",
      "Epoch[00/01] | Iteration[0129/0782] - Loss value: 1.460591435432434\n",
      "Epoch[00/01] | Iteration[0130/0782] - Loss value: 1.2097171545028687\n",
      "Epoch[00/01] | Iteration[0131/0782] - Loss value: 1.2693395614624023\n",
      "Epoch[00/01] | Iteration[0132/0782] - Loss value: 1.2584985494613647\n",
      "Epoch[00/01] | Iteration[0133/0782] - Loss value: 1.5160207748413086\n",
      "Epoch[00/01] | Iteration[0134/0782] - Loss value: 1.3214702606201172\n",
      "Epoch[00/01] | Iteration[0135/0782] - Loss value: 1.091464877128601\n",
      "Epoch[00/01] | Iteration[0136/0782] - Loss value: 1.4245187044143677\n",
      "Epoch[00/01] | Iteration[0137/0782] - Loss value: 1.6843875646591187\n",
      "Epoch[00/01] | Iteration[0138/0782] - Loss value: 1.2924329042434692\n",
      "Epoch[00/01] | Iteration[0139/0782] - Loss value: 1.5442761182785034\n",
      "Epoch[00/01] | Iteration[0140/0782] - Loss value: 1.1210546493530273\n",
      "Epoch[00/01] | Iteration[0141/0782] - Loss value: 1.425471544265747\n",
      "Epoch[00/01] | Iteration[0142/0782] - Loss value: 1.579139232635498\n",
      "Epoch[00/01] | Iteration[0143/0782] - Loss value: 1.330237865447998\n",
      "Epoch[00/01] | Iteration[0144/0782] - Loss value: 1.362266182899475\n",
      "Epoch[00/01] | Iteration[0145/0782] - Loss value: 1.4057320356369019\n",
      "Epoch[00/01] | Iteration[0146/0782] - Loss value: 1.2704263925552368\n",
      "Epoch[00/01] | Iteration[0147/0782] - Loss value: 1.3702093362808228\n",
      "Epoch[00/01] | Iteration[0148/0782] - Loss value: 1.2618993520736694\n",
      "Epoch[00/01] | Iteration[0149/0782] - Loss value: 1.3453094959259033\n",
      "Epoch[00/01] | Iteration[0150/0782] - Loss value: 1.2559975385665894\n",
      "Epoch[00/01] | Iteration[0151/0782] - Loss value: 1.3986132144927979\n",
      "Epoch[00/01] | Iteration[0152/0782] - Loss value: 1.102402925491333\n",
      "Epoch[00/01] | Iteration[0153/0782] - Loss value: 1.2764226198196411\n",
      "Epoch[00/01] | Iteration[0154/0782] - Loss value: 1.7660020589828491\n",
      "Epoch[00/01] | Iteration[0155/0782] - Loss value: 1.30837082862854\n",
      "Epoch[00/01] | Iteration[0156/0782] - Loss value: 1.391696810722351\n",
      "Epoch[00/01] | Iteration[0157/0782] - Loss value: 1.2304282188415527\n",
      "Epoch[00/01] | Iteration[0158/0782] - Loss value: 1.3598403930664062\n",
      "Epoch[00/01] | Iteration[0159/0782] - Loss value: 1.202338695526123\n",
      "Epoch[00/01] | Iteration[0160/0782] - Loss value: 1.3537441492080688\n",
      "Epoch[00/01] | Iteration[0161/0782] - Loss value: 1.1658315658569336\n",
      "Epoch[00/01] | Iteration[0162/0782] - Loss value: 1.0444456338882446\n",
      "Epoch[00/01] | Iteration[0163/0782] - Loss value: 1.3847392797470093\n",
      "Epoch[00/01] | Iteration[0164/0782] - Loss value: 1.4175605773925781\n",
      "Epoch[00/01] | Iteration[0165/0782] - Loss value: 1.5754921436309814\n",
      "Epoch[00/01] | Iteration[0166/0782] - Loss value: 1.387872338294983\n",
      "Epoch[00/01] | Iteration[0167/0782] - Loss value: 1.3603992462158203\n",
      "Epoch[00/01] | Iteration[0168/0782] - Loss value: 1.3557227849960327\n",
      "Epoch[00/01] | Iteration[0169/0782] - Loss value: 1.376852035522461\n",
      "Epoch[00/01] | Iteration[0170/0782] - Loss value: 1.1946603059768677\n",
      "Epoch[00/01] | Iteration[0171/0782] - Loss value: 1.4294761419296265\n",
      "Epoch[00/01] | Iteration[0172/0782] - Loss value: 1.3933093547821045\n",
      "Epoch[00/01] | Iteration[0173/0782] - Loss value: 1.4710707664489746\n",
      "Epoch[00/01] | Iteration[0174/0782] - Loss value: 1.2037837505340576\n",
      "Epoch[00/01] | Iteration[0175/0782] - Loss value: 1.4103031158447266\n",
      "Epoch[00/01] | Iteration[0176/0782] - Loss value: 1.3357794284820557\n",
      "Epoch[00/01] | Iteration[0177/0782] - Loss value: 1.222473382949829\n",
      "Epoch[00/01] | Iteration[0178/0782] - Loss value: 1.232722520828247\n",
      "Epoch[00/01] | Iteration[0179/0782] - Loss value: 1.1106427907943726\n",
      "Epoch[00/01] | Iteration[0180/0782] - Loss value: 1.351365566253662\n",
      "Epoch[00/01] | Iteration[0181/0782] - Loss value: 1.5359983444213867\n",
      "Epoch[00/01] | Iteration[0182/0782] - Loss value: 1.302659273147583\n",
      "Epoch[00/01] | Iteration[0183/0782] - Loss value: 1.2575061321258545\n",
      "Epoch[00/01] | Iteration[0184/0782] - Loss value: 1.3703612089157104\n",
      "Epoch[00/01] | Iteration[0185/0782] - Loss value: 1.322836995124817\n",
      "Epoch[00/01] | Iteration[0186/0782] - Loss value: 1.0309118032455444\n",
      "Epoch[00/01] | Iteration[0187/0782] - Loss value: 1.1559189558029175\n",
      "Epoch[00/01] | Iteration[0188/0782] - Loss value: 0.9840484261512756\n",
      "Epoch[00/01] | Iteration[0189/0782] - Loss value: 1.2614058256149292\n",
      "Epoch[00/01] | Iteration[0190/0782] - Loss value: 1.1125376224517822\n",
      "Epoch[00/01] | Iteration[0191/0782] - Loss value: 1.194525957107544\n",
      "Epoch[00/01] | Iteration[0192/0782] - Loss value: 1.2001324892044067\n",
      "Epoch[00/01] | Iteration[0193/0782] - Loss value: 1.1652199029922485\n",
      "Epoch[00/01] | Iteration[0194/0782] - Loss value: 1.1964541673660278\n",
      "Epoch[00/01] | Iteration[0195/0782] - Loss value: 1.2619327306747437\n",
      "Epoch[00/01] | Iteration[0196/0782] - Loss value: 1.0896955728530884\n",
      "Epoch[00/01] | Iteration[0197/0782] - Loss value: 1.0907378196716309\n",
      "Epoch[00/01] | Iteration[0198/0782] - Loss value: 1.7015759944915771\n",
      "Epoch[00/01] | Iteration[0199/0782] - Loss value: 1.1822401285171509\n",
      "Epoch[00/01] | Iteration[0200/0782] - Loss value: 1.4141484498977661\n",
      "Epoch[00/01] | Iteration[0201/0782] - Loss value: 1.3775746822357178\n",
      "Epoch[00/01] | Iteration[0202/0782] - Loss value: 1.2856261730194092\n",
      "Epoch[00/01] | Iteration[0203/0782] - Loss value: 1.05235755443573\n",
      "Epoch[00/01] | Iteration[0204/0782] - Loss value: 1.202087163925171\n",
      "Epoch[00/01] | Iteration[0205/0782] - Loss value: 1.4334474802017212\n",
      "Epoch[00/01] | Iteration[0206/0782] - Loss value: 1.3440234661102295\n",
      "Epoch[00/01] | Iteration[0207/0782] - Loss value: 1.420494794845581\n",
      "Epoch[00/01] | Iteration[0208/0782] - Loss value: 1.2009824514389038\n",
      "Epoch[00/01] | Iteration[0209/0782] - Loss value: 1.1443145275115967\n",
      "Epoch[00/01] | Iteration[0210/0782] - Loss value: 1.1777775287628174\n",
      "Epoch[00/01] | Iteration[0211/0782] - Loss value: 1.0342060327529907\n",
      "Epoch[00/01] | Iteration[0212/0782] - Loss value: 1.154759168624878\n",
      "Epoch[00/01] | Iteration[0213/0782] - Loss value: 1.1441289186477661\n",
      "Epoch[00/01] | Iteration[0214/0782] - Loss value: 1.2158859968185425\n",
      "Epoch[00/01] | Iteration[0215/0782] - Loss value: 1.2500115633010864\n",
      "Epoch[00/01] | Iteration[0216/0782] - Loss value: 1.0193604230880737\n",
      "Epoch[00/01] | Iteration[0217/0782] - Loss value: 1.4896149635314941\n",
      "Epoch[00/01] | Iteration[0218/0782] - Loss value: 1.128745198249817\n",
      "Epoch[00/01] | Iteration[0219/0782] - Loss value: 1.2642011642456055\n",
      "Epoch[00/01] | Iteration[0220/0782] - Loss value: 1.4031474590301514\n",
      "Epoch[00/01] | Iteration[0221/0782] - Loss value: 1.4369064569473267\n",
      "Epoch[00/01] | Iteration[0222/0782] - Loss value: 1.3108947277069092\n",
      "Epoch[00/01] | Iteration[0223/0782] - Loss value: 1.0427154302597046\n",
      "Epoch[00/01] | Iteration[0224/0782] - Loss value: 1.38935124874115\n",
      "Epoch[00/01] | Iteration[0225/0782] - Loss value: 1.214936375617981\n",
      "Epoch[00/01] | Iteration[0226/0782] - Loss value: 1.264586329460144\n",
      "Epoch[00/01] | Iteration[0227/0782] - Loss value: 1.1975219249725342\n",
      "Epoch[00/01] | Iteration[0228/0782] - Loss value: 1.3880035877227783\n",
      "Epoch[00/01] | Iteration[0229/0782] - Loss value: 1.1548901796340942\n",
      "Epoch[00/01] | Iteration[0230/0782] - Loss value: 1.1937425136566162\n",
      "Epoch[00/01] | Iteration[0231/0782] - Loss value: 1.3115570545196533\n",
      "Epoch[00/01] | Iteration[0232/0782] - Loss value: 1.1109838485717773\n",
      "Epoch[00/01] | Iteration[0233/0782] - Loss value: 1.1782071590423584\n",
      "Epoch[00/01] | Iteration[0234/0782] - Loss value: 1.1021438837051392\n",
      "Epoch[00/01] | Iteration[0235/0782] - Loss value: 0.9863009452819824\n",
      "Epoch[00/01] | Iteration[0236/0782] - Loss value: 1.0949227809906006\n",
      "Epoch[00/01] | Iteration[0237/0782] - Loss value: 1.0608115196228027\n",
      "Epoch[00/01] | Iteration[0238/0782] - Loss value: 1.0663076639175415\n",
      "Epoch[00/01] | Iteration[0239/0782] - Loss value: 1.1807619333267212\n",
      "Epoch[00/01] | Iteration[0240/0782] - Loss value: 1.123281717300415\n",
      "Epoch[00/01] | Iteration[0241/0782] - Loss value: 1.18453848361969\n",
      "Epoch[00/01] | Iteration[0242/0782] - Loss value: 1.0383583307266235\n",
      "Epoch[00/01] | Iteration[0243/0782] - Loss value: 0.980688750743866\n",
      "Epoch[00/01] | Iteration[0244/0782] - Loss value: 1.1448140144348145\n",
      "Epoch[00/01] | Iteration[0245/0782] - Loss value: 1.230129599571228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[00/01] | Iteration[0246/0782] - Loss value: 1.3161791563034058\n",
      "Epoch[00/01] | Iteration[0247/0782] - Loss value: 1.1851446628570557\n",
      "Epoch[00/01] | Iteration[0248/0782] - Loss value: 1.01583731174469\n",
      "Epoch[00/01] | Iteration[0249/0782] - Loss value: 1.029046893119812\n",
      "Epoch[00/01] | Iteration[0250/0782] - Loss value: 1.2590526342391968\n",
      "Epoch[00/01] | Iteration[0251/0782] - Loss value: 1.2923450469970703\n",
      "Epoch[00/01] | Iteration[0252/0782] - Loss value: 1.1694283485412598\n",
      "Epoch[00/01] | Iteration[0253/0782] - Loss value: 1.2467738389968872\n",
      "Epoch[00/01] | Iteration[0254/0782] - Loss value: 1.2115020751953125\n",
      "Epoch[00/01] | Iteration[0255/0782] - Loss value: 1.0140451192855835\n",
      "Epoch[00/01] | Iteration[0256/0782] - Loss value: 1.1373069286346436\n",
      "Epoch[00/01] | Iteration[0257/0782] - Loss value: 1.1203020811080933\n",
      "Epoch[00/01] | Iteration[0258/0782] - Loss value: 1.3025286197662354\n",
      "Epoch[00/01] | Iteration[0259/0782] - Loss value: 1.174484133720398\n",
      "Epoch[00/01] | Iteration[0260/0782] - Loss value: 1.1174671649932861\n",
      "Epoch[00/01] | Iteration[0261/0782] - Loss value: 1.091606855392456\n",
      "Epoch[00/01] | Iteration[0262/0782] - Loss value: 1.4223065376281738\n",
      "Epoch[00/01] | Iteration[0263/0782] - Loss value: 1.2685281038284302\n",
      "Epoch[00/01] | Iteration[0264/0782] - Loss value: 1.073538064956665\n",
      "Epoch[00/01] | Iteration[0265/0782] - Loss value: 1.1849161386489868\n",
      "Epoch[00/01] | Iteration[0266/0782] - Loss value: 1.1496957540512085\n",
      "Epoch[00/01] | Iteration[0267/0782] - Loss value: 1.3422515392303467\n",
      "Epoch[00/01] | Iteration[0268/0782] - Loss value: 1.107456922531128\n",
      "Epoch[00/01] | Iteration[0269/0782] - Loss value: 1.050656795501709\n",
      "Epoch[00/01] | Iteration[0270/0782] - Loss value: 1.0008437633514404\n",
      "Epoch[00/01] | Iteration[0271/0782] - Loss value: 1.1751117706298828\n",
      "Epoch[00/01] | Iteration[0272/0782] - Loss value: 1.123531699180603\n",
      "Epoch[00/01] | Iteration[0273/0782] - Loss value: 1.5066008567810059\n",
      "Epoch[00/01] | Iteration[0274/0782] - Loss value: 1.0630282163619995\n",
      "Epoch[00/01] | Iteration[0275/0782] - Loss value: 1.0896096229553223\n",
      "Epoch[00/01] | Iteration[0276/0782] - Loss value: 1.041184663772583\n",
      "Epoch[00/01] | Iteration[0277/0782] - Loss value: 1.3128875494003296\n",
      "Epoch[00/01] | Iteration[0278/0782] - Loss value: 1.3355039358139038\n",
      "Epoch[00/01] | Iteration[0279/0782] - Loss value: 1.3948500156402588\n",
      "Epoch[00/01] | Iteration[0280/0782] - Loss value: 0.9789447784423828\n",
      "Epoch[00/01] | Iteration[0281/0782] - Loss value: 1.2481929063796997\n",
      "Epoch[00/01] | Iteration[0282/0782] - Loss value: 1.1418516635894775\n",
      "Epoch[00/01] | Iteration[0283/0782] - Loss value: 1.3055309057235718\n",
      "Epoch[00/01] | Iteration[0284/0782] - Loss value: 1.1095244884490967\n",
      "Epoch[00/01] | Iteration[0285/0782] - Loss value: 1.1968990564346313\n",
      "Epoch[00/01] | Iteration[0286/0782] - Loss value: 0.7825113534927368\n",
      "Epoch[00/01] | Iteration[0287/0782] - Loss value: 1.134741187095642\n",
      "Epoch[00/01] | Iteration[0288/0782] - Loss value: 1.1269714832305908\n",
      "Epoch[00/01] | Iteration[0289/0782] - Loss value: 1.2162156105041504\n",
      "Epoch[00/01] | Iteration[0290/0782] - Loss value: 1.1317998170852661\n",
      "Epoch[00/01] | Iteration[0291/0782] - Loss value: 1.2275499105453491\n",
      "Epoch[00/01] | Iteration[0292/0782] - Loss value: 1.0908255577087402\n",
      "Epoch[00/01] | Iteration[0293/0782] - Loss value: 1.2292970418930054\n",
      "Epoch[00/01] | Iteration[0294/0782] - Loss value: 1.1309274435043335\n",
      "Epoch[00/01] | Iteration[0295/0782] - Loss value: 1.0328400135040283\n",
      "Epoch[00/01] | Iteration[0296/0782] - Loss value: 1.066848635673523\n",
      "Epoch[00/01] | Iteration[0297/0782] - Loss value: 1.0702531337738037\n",
      "Epoch[00/01] | Iteration[0298/0782] - Loss value: 1.0787221193313599\n",
      "Epoch[00/01] | Iteration[0299/0782] - Loss value: 1.2798631191253662\n",
      "Epoch[00/01] | Iteration[0300/0782] - Loss value: 1.2645890712738037\n",
      "Epoch[00/01] | Iteration[0301/0782] - Loss value: 0.9692733883857727\n",
      "Epoch[00/01] | Iteration[0302/0782] - Loss value: 0.9985451698303223\n",
      "Epoch[00/01] | Iteration[0303/0782] - Loss value: 1.169493317604065\n",
      "Epoch[00/01] | Iteration[0304/0782] - Loss value: 1.0647720098495483\n",
      "Epoch[00/01] | Iteration[0305/0782] - Loss value: 1.2233086824417114\n",
      "Epoch[00/01] | Iteration[0306/0782] - Loss value: 1.094674825668335\n",
      "Epoch[00/01] | Iteration[0307/0782] - Loss value: 1.0916680097579956\n",
      "Epoch[00/01] | Iteration[0308/0782] - Loss value: 0.9407891035079956\n",
      "Epoch[00/01] | Iteration[0309/0782] - Loss value: 1.0889089107513428\n",
      "Epoch[00/01] | Iteration[0310/0782] - Loss value: 0.9355674982070923\n",
      "Epoch[00/01] | Iteration[0311/0782] - Loss value: 0.9691718220710754\n",
      "Epoch[00/01] | Iteration[0312/0782] - Loss value: 1.1314414739608765\n",
      "Epoch[00/01] | Iteration[0313/0782] - Loss value: 1.0386484861373901\n",
      "Epoch[00/01] | Iteration[0314/0782] - Loss value: 1.0335760116577148\n",
      "Epoch[00/01] | Iteration[0315/0782] - Loss value: 0.8145447373390198\n",
      "Epoch[00/01] | Iteration[0316/0782] - Loss value: 0.9516611099243164\n",
      "Epoch[00/01] | Iteration[0317/0782] - Loss value: 1.0448052883148193\n",
      "Epoch[00/01] | Iteration[0318/0782] - Loss value: 0.7752177715301514\n",
      "Epoch[00/01] | Iteration[0319/0782] - Loss value: 1.0720744132995605\n",
      "Epoch[00/01] | Iteration[0320/0782] - Loss value: 0.8784814476966858\n",
      "Epoch[00/01] | Iteration[0321/0782] - Loss value: 1.2969114780426025\n",
      "Epoch[00/01] | Iteration[0322/0782] - Loss value: 1.1470143795013428\n",
      "Epoch[00/01] | Iteration[0323/0782] - Loss value: 1.2448034286499023\n",
      "Epoch[00/01] | Iteration[0324/0782] - Loss value: 0.992767333984375\n",
      "Epoch[00/01] | Iteration[0325/0782] - Loss value: 0.9194493889808655\n",
      "Epoch[00/01] | Iteration[0326/0782] - Loss value: 0.8783035278320312\n",
      "Epoch[00/01] | Iteration[0327/0782] - Loss value: 0.9582481384277344\n",
      "Epoch[00/01] | Iteration[0328/0782] - Loss value: 1.1099331378936768\n",
      "Epoch[00/01] | Iteration[0329/0782] - Loss value: 1.258360505104065\n",
      "Epoch[00/01] | Iteration[0330/0782] - Loss value: 0.9313654899597168\n",
      "Epoch[00/01] | Iteration[0331/0782] - Loss value: 1.230733871459961\n",
      "Epoch[00/01] | Iteration[0332/0782] - Loss value: 1.2136205434799194\n",
      "Epoch[00/01] | Iteration[0333/0782] - Loss value: 1.3012341260910034\n",
      "Epoch[00/01] | Iteration[0334/0782] - Loss value: 0.8534731864929199\n",
      "Epoch[00/01] | Iteration[0335/0782] - Loss value: 1.4289944171905518\n",
      "Epoch[00/01] | Iteration[0336/0782] - Loss value: 0.7947697043418884\n",
      "Epoch[00/01] | Iteration[0337/0782] - Loss value: 0.9616779685020447\n",
      "Epoch[00/01] | Iteration[0338/0782] - Loss value: 1.1875805854797363\n",
      "Epoch[00/01] | Iteration[0339/0782] - Loss value: 1.0810619592666626\n",
      "Epoch[00/01] | Iteration[0340/0782] - Loss value: 1.1436395645141602\n",
      "Epoch[00/01] | Iteration[0341/0782] - Loss value: 0.9904590249061584\n",
      "Epoch[00/01] | Iteration[0342/0782] - Loss value: 1.0529454946517944\n",
      "Epoch[00/01] | Iteration[0343/0782] - Loss value: 1.1718195676803589\n",
      "Epoch[00/01] | Iteration[0344/0782] - Loss value: 1.0753815174102783\n",
      "Epoch[00/01] | Iteration[0345/0782] - Loss value: 0.9813401699066162\n",
      "Epoch[00/01] | Iteration[0346/0782] - Loss value: 0.9239857792854309\n",
      "Epoch[00/01] | Iteration[0347/0782] - Loss value: 1.0151656866073608\n",
      "Epoch[00/01] | Iteration[0348/0782] - Loss value: 1.3541107177734375\n",
      "Epoch[00/01] | Iteration[0349/0782] - Loss value: 0.8174185752868652\n",
      "Epoch[00/01] | Iteration[0350/0782] - Loss value: 0.929830014705658\n",
      "Epoch[00/01] | Iteration[0351/0782] - Loss value: 0.7632151246070862\n",
      "Epoch[00/01] | Iteration[0352/0782] - Loss value: 0.8659297227859497\n",
      "Epoch[00/01] | Iteration[0353/0782] - Loss value: 0.9839761257171631\n",
      "Epoch[00/01] | Iteration[0354/0782] - Loss value: 0.7871929407119751\n",
      "Epoch[00/01] | Iteration[0355/0782] - Loss value: 1.3255746364593506\n",
      "Epoch[00/01] | Iteration[0356/0782] - Loss value: 0.9749582409858704\n",
      "Epoch[00/01] | Iteration[0357/0782] - Loss value: 1.1985405683517456\n",
      "Epoch[00/01] | Iteration[0358/0782] - Loss value: 1.1283069849014282\n",
      "Epoch[00/01] | Iteration[0359/0782] - Loss value: 0.979239821434021\n",
      "Epoch[00/01] | Iteration[0360/0782] - Loss value: 0.9952703714370728\n",
      "Epoch[00/01] | Iteration[0361/0782] - Loss value: 1.1436136960983276\n",
      "Epoch[00/01] | Iteration[0362/0782] - Loss value: 1.0175549983978271\n",
      "Epoch[00/01] | Iteration[0363/0782] - Loss value: 1.2883919477462769\n",
      "Epoch[00/01] | Iteration[0364/0782] - Loss value: 1.127291202545166\n",
      "Epoch[00/01] | Iteration[0365/0782] - Loss value: 1.1040608882904053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[00/01] | Iteration[0366/0782] - Loss value: 1.360675573348999\n",
      "Epoch[00/01] | Iteration[0367/0782] - Loss value: 0.8469610214233398\n",
      "Epoch[00/01] | Iteration[0368/0782] - Loss value: 1.0519758462905884\n",
      "Epoch[00/01] | Iteration[0369/0782] - Loss value: 0.7542871236801147\n",
      "Epoch[00/01] | Iteration[0370/0782] - Loss value: 0.9744110107421875\n",
      "Epoch[00/01] | Iteration[0371/0782] - Loss value: 1.0060029029846191\n",
      "Epoch[00/01] | Iteration[0372/0782] - Loss value: 1.2373478412628174\n",
      "Epoch[00/01] | Iteration[0373/0782] - Loss value: 1.0689982175827026\n",
      "Epoch[00/01] | Iteration[0374/0782] - Loss value: 0.906174898147583\n",
      "Epoch[00/01] | Iteration[0375/0782] - Loss value: 1.2258981466293335\n",
      "Epoch[00/01] | Iteration[0376/0782] - Loss value: 0.8119885921478271\n",
      "Epoch[00/01] | Iteration[0377/0782] - Loss value: 0.9848148226737976\n",
      "Epoch[00/01] | Iteration[0378/0782] - Loss value: 1.1006886959075928\n",
      "Epoch[00/01] | Iteration[0379/0782] - Loss value: 1.036145567893982\n",
      "Epoch[00/01] | Iteration[0380/0782] - Loss value: 1.181003212928772\n",
      "Epoch[00/01] | Iteration[0381/0782] - Loss value: 0.939333438873291\n",
      "Epoch[00/01] | Iteration[0382/0782] - Loss value: 1.11045241355896\n",
      "Epoch[00/01] | Iteration[0383/0782] - Loss value: 1.275020956993103\n",
      "Epoch[00/01] | Iteration[0384/0782] - Loss value: 1.0054751634597778\n",
      "Epoch[00/01] | Iteration[0385/0782] - Loss value: 0.9007235169410706\n",
      "Epoch[00/01] | Iteration[0386/0782] - Loss value: 1.07819402217865\n",
      "Epoch[00/01] | Iteration[0387/0782] - Loss value: 0.8215667605400085\n",
      "Epoch[00/01] | Iteration[0388/0782] - Loss value: 1.1357073783874512\n",
      "Epoch[00/01] | Iteration[0389/0782] - Loss value: 1.1482945680618286\n",
      "Epoch[00/01] | Iteration[0390/0782] - Loss value: 1.1081362962722778\n",
      "Epoch[00/01] | Iteration[0391/0782] - Loss value: 1.2266290187835693\n",
      "Epoch[00/01] | Iteration[0392/0782] - Loss value: 1.0771291255950928\n",
      "Epoch[00/01] | Iteration[0393/0782] - Loss value: 0.8693161606788635\n",
      "Epoch[00/01] | Iteration[0394/0782] - Loss value: 1.1468641757965088\n",
      "Epoch[00/01] | Iteration[0395/0782] - Loss value: 1.0814582109451294\n",
      "Epoch[00/01] | Iteration[0396/0782] - Loss value: 1.0644872188568115\n",
      "Epoch[00/01] | Iteration[0397/0782] - Loss value: 0.9767166972160339\n",
      "Epoch[00/01] | Iteration[0398/0782] - Loss value: 1.1340882778167725\n",
      "Epoch[00/01] | Iteration[0399/0782] - Loss value: 1.1501092910766602\n",
      "Epoch[00/01] | Iteration[0400/0782] - Loss value: 1.061953067779541\n",
      "Epoch[00/01] | Iteration[0401/0782] - Loss value: 0.9759380221366882\n",
      "Epoch[00/01] | Iteration[0402/0782] - Loss value: 1.0363348722457886\n",
      "Epoch[00/01] | Iteration[0403/0782] - Loss value: 1.0735101699829102\n",
      "Epoch[00/01] | Iteration[0404/0782] - Loss value: 0.7772850394248962\n",
      "Epoch[00/01] | Iteration[0405/0782] - Loss value: 1.0101381540298462\n",
      "Epoch[00/01] | Iteration[0406/0782] - Loss value: 1.0940401554107666\n",
      "Epoch[00/01] | Iteration[0407/0782] - Loss value: 1.0531340837478638\n",
      "Epoch[00/01] | Iteration[0408/0782] - Loss value: 1.1876963376998901\n",
      "Epoch[00/01] | Iteration[0409/0782] - Loss value: 0.8124776482582092\n",
      "Epoch[00/01] | Iteration[0410/0782] - Loss value: 0.904842734336853\n",
      "Epoch[00/01] | Iteration[0411/0782] - Loss value: 1.0485520362854004\n",
      "Epoch[00/01] | Iteration[0412/0782] - Loss value: 0.7342634201049805\n",
      "Epoch[00/01] | Iteration[0413/0782] - Loss value: 1.1233699321746826\n",
      "Epoch[00/01] | Iteration[0414/0782] - Loss value: 1.1444109678268433\n",
      "Epoch[00/01] | Iteration[0415/0782] - Loss value: 1.157361388206482\n",
      "Epoch[00/01] | Iteration[0416/0782] - Loss value: 1.0966285467147827\n",
      "Epoch[00/01] | Iteration[0417/0782] - Loss value: 1.0087486505508423\n",
      "Epoch[00/01] | Iteration[0418/0782] - Loss value: 0.954342782497406\n",
      "Epoch[00/01] | Iteration[0419/0782] - Loss value: 1.009228229522705\n",
      "Epoch[00/01] | Iteration[0420/0782] - Loss value: 0.9339841604232788\n",
      "Epoch[00/01] | Iteration[0421/0782] - Loss value: 0.7487262487411499\n",
      "Epoch[00/01] | Iteration[0422/0782] - Loss value: 1.30060613155365\n",
      "Epoch[00/01] | Iteration[0423/0782] - Loss value: 1.0353926420211792\n",
      "Epoch[00/01] | Iteration[0424/0782] - Loss value: 1.267366647720337\n",
      "Epoch[00/01] | Iteration[0425/0782] - Loss value: 1.0608891248703003\n",
      "Epoch[00/01] | Iteration[0426/0782] - Loss value: 0.9817437529563904\n",
      "Epoch[00/01] | Iteration[0427/0782] - Loss value: 1.0041463375091553\n",
      "Epoch[00/01] | Iteration[0428/0782] - Loss value: 0.9713025689125061\n",
      "Epoch[00/01] | Iteration[0429/0782] - Loss value: 1.0107516050338745\n",
      "Epoch[00/01] | Iteration[0430/0782] - Loss value: 0.9796500205993652\n",
      "Epoch[00/01] | Iteration[0431/0782] - Loss value: 1.0307081937789917\n",
      "Epoch[00/01] | Iteration[0432/0782] - Loss value: 0.826015830039978\n",
      "Epoch[00/01] | Iteration[0433/0782] - Loss value: 1.0707899332046509\n",
      "Epoch[00/01] | Iteration[0434/0782] - Loss value: 0.8955119848251343\n",
      "Epoch[00/01] | Iteration[0435/0782] - Loss value: 1.2676030397415161\n",
      "Epoch[00/01] | Iteration[0436/0782] - Loss value: 1.036404013633728\n",
      "Epoch[00/01] | Iteration[0437/0782] - Loss value: 0.7596375942230225\n",
      "Epoch[00/01] | Iteration[0438/0782] - Loss value: 0.8466038703918457\n",
      "Epoch[00/01] | Iteration[0439/0782] - Loss value: 0.8845447301864624\n",
      "Epoch[00/01] | Iteration[0440/0782] - Loss value: 0.8108018040657043\n",
      "Epoch[00/01] | Iteration[0441/0782] - Loss value: 1.3412690162658691\n",
      "Epoch[00/01] | Iteration[0442/0782] - Loss value: 0.8809263706207275\n",
      "Epoch[00/01] | Iteration[0443/0782] - Loss value: 0.9678123593330383\n",
      "Epoch[00/01] | Iteration[0444/0782] - Loss value: 0.7818798422813416\n",
      "Epoch[00/01] | Iteration[0445/0782] - Loss value: 0.8047850131988525\n",
      "Epoch[00/01] | Iteration[0446/0782] - Loss value: 1.1876863241195679\n",
      "Epoch[00/01] | Iteration[0447/0782] - Loss value: 0.9658499956130981\n",
      "Epoch[00/01] | Iteration[0448/0782] - Loss value: 0.9841486215591431\n",
      "Epoch[00/01] | Iteration[0449/0782] - Loss value: 0.8851131200790405\n",
      "Epoch[00/01] | Iteration[0450/0782] - Loss value: 0.8500031232833862\n",
      "Epoch[00/01] | Iteration[0451/0782] - Loss value: 0.8107489347457886\n",
      "Epoch[00/01] | Iteration[0452/0782] - Loss value: 0.8209668397903442\n",
      "Epoch[00/01] | Iteration[0453/0782] - Loss value: 1.1252918243408203\n",
      "Epoch[00/01] | Iteration[0454/0782] - Loss value: 0.7217077612876892\n",
      "Epoch[00/01] | Iteration[0455/0782] - Loss value: 0.8805137872695923\n",
      "Epoch[00/01] | Iteration[0456/0782] - Loss value: 1.0707638263702393\n",
      "Epoch[00/01] | Iteration[0457/0782] - Loss value: 0.9063265919685364\n",
      "Epoch[00/01] | Iteration[0458/0782] - Loss value: 0.8269302845001221\n",
      "Epoch[00/01] | Iteration[0459/0782] - Loss value: 1.0031379461288452\n",
      "Epoch[00/01] | Iteration[0460/0782] - Loss value: 1.1517837047576904\n",
      "Epoch[00/01] | Iteration[0461/0782] - Loss value: 0.6596863865852356\n",
      "Epoch[00/01] | Iteration[0462/0782] - Loss value: 0.8229061365127563\n",
      "Epoch[00/01] | Iteration[0463/0782] - Loss value: 1.0395547151565552\n",
      "Epoch[00/01] | Iteration[0464/0782] - Loss value: 0.759028434753418\n",
      "Epoch[00/01] | Iteration[0465/0782] - Loss value: 1.1773847341537476\n",
      "Epoch[00/01] | Iteration[0466/0782] - Loss value: 1.1377384662628174\n",
      "Epoch[00/01] | Iteration[0467/0782] - Loss value: 0.9484192132949829\n",
      "Epoch[00/01] | Iteration[0468/0782] - Loss value: 0.9386293888092041\n",
      "Epoch[00/01] | Iteration[0469/0782] - Loss value: 0.8218252658843994\n",
      "Epoch[00/01] | Iteration[0470/0782] - Loss value: 1.015097737312317\n",
      "Epoch[00/01] | Iteration[0471/0782] - Loss value: 0.9648720026016235\n",
      "Epoch[00/01] | Iteration[0472/0782] - Loss value: 0.9693491458892822\n",
      "Epoch[00/01] | Iteration[0473/0782] - Loss value: 0.9263641834259033\n",
      "Epoch[00/01] | Iteration[0474/0782] - Loss value: 0.9193131923675537\n",
      "Epoch[00/01] | Iteration[0475/0782] - Loss value: 0.7910720705986023\n",
      "Epoch[00/01] | Iteration[0476/0782] - Loss value: 1.1344292163848877\n",
      "Epoch[00/01] | Iteration[0477/0782] - Loss value: 0.9825833439826965\n",
      "Epoch[00/01] | Iteration[0478/0782] - Loss value: 0.7284043431282043\n",
      "Epoch[00/01] | Iteration[0479/0782] - Loss value: 1.0113413333892822\n",
      "Epoch[00/01] | Iteration[0480/0782] - Loss value: 0.762558102607727\n",
      "Epoch[00/01] | Iteration[0481/0782] - Loss value: 0.9698677659034729\n",
      "Epoch[00/01] | Iteration[0482/0782] - Loss value: 0.8503175377845764\n",
      "Epoch[00/01] | Iteration[0483/0782] - Loss value: 0.7558768391609192\n",
      "Epoch[00/01] | Iteration[0484/0782] - Loss value: 0.9608561992645264\n",
      "Epoch[00/01] | Iteration[0485/0782] - Loss value: 0.9277378916740417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[00/01] | Iteration[0486/0782] - Loss value: 0.9530360698699951\n",
      "Epoch[00/01] | Iteration[0487/0782] - Loss value: 1.2505714893341064\n",
      "Epoch[00/01] | Iteration[0488/0782] - Loss value: 1.1143171787261963\n",
      "Epoch[00/01] | Iteration[0489/0782] - Loss value: 0.6872540712356567\n",
      "Epoch[00/01] | Iteration[0490/0782] - Loss value: 1.2129409313201904\n",
      "Epoch[00/01] | Iteration[0491/0782] - Loss value: 1.3736075162887573\n",
      "Epoch[00/01] | Iteration[0492/0782] - Loss value: 0.9756830334663391\n",
      "Epoch[00/01] | Iteration[0493/0782] - Loss value: 0.8802105188369751\n",
      "Epoch[00/01] | Iteration[0494/0782] - Loss value: 0.9272868037223816\n",
      "Epoch[00/01] | Iteration[0495/0782] - Loss value: 0.7245615124702454\n",
      "Epoch[00/01] | Iteration[0496/0782] - Loss value: 1.0717071294784546\n",
      "Epoch[00/01] | Iteration[0497/0782] - Loss value: 1.055528163909912\n",
      "Epoch[00/01] | Iteration[0498/0782] - Loss value: 0.8742111325263977\n",
      "Epoch[00/01] | Iteration[0499/0782] - Loss value: 0.8708487749099731\n",
      "Epoch[00/01] | Iteration[0500/0782] - Loss value: 0.9544166922569275\n",
      "Epoch[00/01] | Iteration[0501/0782] - Loss value: 1.1534849405288696\n",
      "Epoch[00/01] | Iteration[0502/0782] - Loss value: 0.9146031737327576\n",
      "Epoch[00/01] | Iteration[0503/0782] - Loss value: 1.1488933563232422\n",
      "Epoch[00/01] | Iteration[0504/0782] - Loss value: 0.7390914559364319\n",
      "Epoch[00/01] | Iteration[0505/0782] - Loss value: 1.0149352550506592\n",
      "Epoch[00/01] | Iteration[0506/0782] - Loss value: 0.8005584478378296\n",
      "Epoch[00/01] | Iteration[0507/0782] - Loss value: 0.8405711054801941\n",
      "Epoch[00/01] | Iteration[0508/0782] - Loss value: 0.850729763507843\n",
      "Epoch[00/01] | Iteration[0509/0782] - Loss value: 1.0022183656692505\n",
      "Epoch[00/01] | Iteration[0510/0782] - Loss value: 0.8216354846954346\n",
      "Epoch[00/01] | Iteration[0511/0782] - Loss value: 0.8508538603782654\n",
      "Epoch[00/01] | Iteration[0512/0782] - Loss value: 0.7923957109451294\n",
      "Epoch[00/01] | Iteration[0513/0782] - Loss value: 0.6994203925132751\n",
      "Epoch[00/01] | Iteration[0514/0782] - Loss value: 0.7993788123130798\n",
      "Epoch[00/01] | Iteration[0515/0782] - Loss value: 1.0507903099060059\n",
      "Epoch[00/01] | Iteration[0516/0782] - Loss value: 0.8788996338844299\n",
      "Epoch[00/01] | Iteration[0517/0782] - Loss value: 1.0370328426361084\n",
      "Epoch[00/01] | Iteration[0518/0782] - Loss value: 0.8475902080535889\n",
      "Epoch[00/01] | Iteration[0519/0782] - Loss value: 0.7741764187812805\n",
      "Epoch[00/01] | Iteration[0520/0782] - Loss value: 0.770385205745697\n",
      "Epoch[00/01] | Iteration[0521/0782] - Loss value: 0.9451907277107239\n",
      "Epoch[00/01] | Iteration[0522/0782] - Loss value: 1.0887620449066162\n",
      "Epoch[00/01] | Iteration[0523/0782] - Loss value: 1.0448206663131714\n",
      "Epoch[00/01] | Iteration[0524/0782] - Loss value: 0.8429091572761536\n",
      "Epoch[00/01] | Iteration[0525/0782] - Loss value: 0.6870040893554688\n",
      "Epoch[00/01] | Iteration[0526/0782] - Loss value: 1.077732801437378\n",
      "Epoch[00/01] | Iteration[0527/0782] - Loss value: 0.9307586550712585\n",
      "Epoch[00/01] | Iteration[0528/0782] - Loss value: 1.1365649700164795\n",
      "Epoch[00/01] | Iteration[0529/0782] - Loss value: 0.9637633562088013\n",
      "Epoch[00/01] | Iteration[0530/0782] - Loss value: 0.9571025371551514\n",
      "Epoch[00/01] | Iteration[0531/0782] - Loss value: 0.774700939655304\n",
      "Epoch[00/01] | Iteration[0532/0782] - Loss value: 0.9185844659805298\n",
      "Epoch[00/01] | Iteration[0533/0782] - Loss value: 0.7861674427986145\n",
      "Epoch[00/01] | Iteration[0534/0782] - Loss value: 1.0501126050949097\n",
      "Epoch[00/01] | Iteration[0535/0782] - Loss value: 1.1697379350662231\n",
      "Epoch[00/01] | Iteration[0536/0782] - Loss value: 0.7680975794792175\n",
      "Epoch[00/01] | Iteration[0537/0782] - Loss value: 0.88083815574646\n",
      "Epoch[00/01] | Iteration[0538/0782] - Loss value: 0.9978627562522888\n",
      "Epoch[00/01] | Iteration[0539/0782] - Loss value: 0.8008301258087158\n",
      "Epoch[00/01] | Iteration[0540/0782] - Loss value: 0.9130581021308899\n",
      "Epoch[00/01] | Iteration[0541/0782] - Loss value: 0.8862724304199219\n",
      "Epoch[00/01] | Iteration[0542/0782] - Loss value: 0.8557522296905518\n",
      "Epoch[00/01] | Iteration[0543/0782] - Loss value: 0.882881760597229\n",
      "Epoch[00/01] | Iteration[0544/0782] - Loss value: 0.6757485270500183\n",
      "Epoch[00/01] | Iteration[0545/0782] - Loss value: 0.9151849150657654\n",
      "Epoch[00/01] | Iteration[0546/0782] - Loss value: 1.1965056657791138\n",
      "Epoch[00/01] | Iteration[0547/0782] - Loss value: 0.7781537771224976\n",
      "Epoch[00/01] | Iteration[0548/0782] - Loss value: 1.128867268562317\n",
      "Epoch[00/01] | Iteration[0549/0782] - Loss value: 0.8449219465255737\n",
      "Epoch[00/01] | Iteration[0550/0782] - Loss value: 0.8906944990158081\n",
      "Epoch[00/01] | Iteration[0551/0782] - Loss value: 0.6568778157234192\n",
      "Epoch[00/01] | Iteration[0552/0782] - Loss value: 0.7609981894493103\n",
      "Epoch[00/01] | Iteration[0553/0782] - Loss value: 0.8281716108322144\n",
      "Epoch[00/01] | Iteration[0554/0782] - Loss value: 0.9588211178779602\n",
      "Epoch[00/01] | Iteration[0555/0782] - Loss value: 0.922185480594635\n",
      "Epoch[00/01] | Iteration[0556/0782] - Loss value: 0.8630892634391785\n",
      "Epoch[00/01] | Iteration[0557/0782] - Loss value: 0.9513049125671387\n",
      "Epoch[00/01] | Iteration[0558/0782] - Loss value: 0.8242853879928589\n",
      "Epoch[00/01] | Iteration[0559/0782] - Loss value: 0.882379412651062\n",
      "Epoch[00/01] | Iteration[0560/0782] - Loss value: 0.9737673997879028\n",
      "Epoch[00/01] | Iteration[0561/0782] - Loss value: 0.8102666735649109\n",
      "Epoch[00/01] | Iteration[0562/0782] - Loss value: 1.3306525945663452\n",
      "Epoch[00/01] | Iteration[0563/0782] - Loss value: 0.697664737701416\n",
      "Epoch[00/01] | Iteration[0564/0782] - Loss value: 0.7199586629867554\n",
      "Epoch[00/01] | Iteration[0565/0782] - Loss value: 0.8697636723518372\n",
      "Epoch[00/01] | Iteration[0566/0782] - Loss value: 0.8840843439102173\n",
      "Epoch[00/01] | Iteration[0567/0782] - Loss value: 1.048904299736023\n",
      "Epoch[00/01] | Iteration[0568/0782] - Loss value: 0.8034945726394653\n",
      "Epoch[00/01] | Iteration[0569/0782] - Loss value: 0.8013084530830383\n",
      "Epoch[00/01] | Iteration[0570/0782] - Loss value: 0.8685575723648071\n",
      "Epoch[00/01] | Iteration[0571/0782] - Loss value: 0.641099750995636\n",
      "Epoch[00/01] | Iteration[0572/0782] - Loss value: 0.9495974779129028\n",
      "Epoch[00/01] | Iteration[0573/0782] - Loss value: 1.075968623161316\n",
      "Epoch[00/01] | Iteration[0574/0782] - Loss value: 0.8524580597877502\n",
      "Epoch[00/01] | Iteration[0575/0782] - Loss value: 0.633574366569519\n",
      "Epoch[00/01] | Iteration[0576/0782] - Loss value: 0.8967476487159729\n",
      "Epoch[00/01] | Iteration[0577/0782] - Loss value: 0.9573989510536194\n",
      "Epoch[00/01] | Iteration[0578/0782] - Loss value: 1.0059304237365723\n",
      "Epoch[00/01] | Iteration[0579/0782] - Loss value: 0.9633243083953857\n",
      "Epoch[00/01] | Iteration[0580/0782] - Loss value: 1.010802984237671\n",
      "Epoch[00/01] | Iteration[0581/0782] - Loss value: 0.7204972505569458\n",
      "Epoch[00/01] | Iteration[0582/0782] - Loss value: 0.5883359909057617\n",
      "Epoch[00/01] | Iteration[0583/0782] - Loss value: 1.0043879747390747\n",
      "Epoch[00/01] | Iteration[0584/0782] - Loss value: 0.9278017282485962\n",
      "Epoch[00/01] | Iteration[0585/0782] - Loss value: 1.1149948835372925\n",
      "Epoch[00/01] | Iteration[0586/0782] - Loss value: 0.941635251045227\n",
      "Epoch[00/01] | Iteration[0587/0782] - Loss value: 0.9968035221099854\n",
      "Epoch[00/01] | Iteration[0588/0782] - Loss value: 0.6840665340423584\n",
      "Epoch[00/01] | Iteration[0589/0782] - Loss value: 0.5789809823036194\n",
      "Epoch[00/01] | Iteration[0590/0782] - Loss value: 0.9726834893226624\n",
      "Epoch[00/01] | Iteration[0591/0782] - Loss value: 1.08763587474823\n",
      "Epoch[00/01] | Iteration[0592/0782] - Loss value: 1.2500243186950684\n",
      "Epoch[00/01] | Iteration[0593/0782] - Loss value: 0.8378092050552368\n",
      "Epoch[00/01] | Iteration[0594/0782] - Loss value: 0.6094707250595093\n",
      "Epoch[00/01] | Iteration[0595/0782] - Loss value: 0.5940663814544678\n",
      "Epoch[00/01] | Iteration[0596/0782] - Loss value: 1.0217899084091187\n",
      "Epoch[00/01] | Iteration[0597/0782] - Loss value: 0.8236596584320068\n",
      "Epoch[00/01] | Iteration[0598/0782] - Loss value: 1.0218315124511719\n",
      "Epoch[00/01] | Iteration[0599/0782] - Loss value: 0.7549508810043335\n",
      "Epoch[00/01] | Iteration[0600/0782] - Loss value: 1.2581342458724976\n",
      "Epoch[00/01] | Iteration[0601/0782] - Loss value: 0.7572948336601257\n",
      "Epoch[00/01] | Iteration[0602/0782] - Loss value: 0.6906768679618835\n",
      "Epoch[00/01] | Iteration[0603/0782] - Loss value: 0.8368899822235107\n",
      "Epoch[00/01] | Iteration[0604/0782] - Loss value: 0.8641274571418762\n",
      "Epoch[00/01] | Iteration[0605/0782] - Loss value: 0.7410238981246948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[00/01] | Iteration[0606/0782] - Loss value: 0.8532950282096863\n",
      "Epoch[00/01] | Iteration[0607/0782] - Loss value: 0.6622495651245117\n",
      "Epoch[00/01] | Iteration[0608/0782] - Loss value: 0.9543533325195312\n",
      "Epoch[00/01] | Iteration[0609/0782] - Loss value: 0.9257883429527283\n",
      "Epoch[00/01] | Iteration[0610/0782] - Loss value: 1.029453992843628\n",
      "Epoch[00/01] | Iteration[0611/0782] - Loss value: 0.7820538282394409\n",
      "Epoch[00/01] | Iteration[0612/0782] - Loss value: 0.8289909958839417\n",
      "Epoch[00/01] | Iteration[0613/0782] - Loss value: 0.8287452459335327\n",
      "Epoch[00/01] | Iteration[0614/0782] - Loss value: 0.8846283555030823\n",
      "Epoch[00/01] | Iteration[0615/0782] - Loss value: 1.2816106081008911\n",
      "Epoch[00/01] | Iteration[0616/0782] - Loss value: 1.0636005401611328\n",
      "Epoch[00/01] | Iteration[0617/0782] - Loss value: 0.8416983485221863\n",
      "Epoch[00/01] | Iteration[0618/0782] - Loss value: 0.9637599587440491\n",
      "Epoch[00/01] | Iteration[0619/0782] - Loss value: 0.6707130074501038\n",
      "Epoch[00/01] | Iteration[0620/0782] - Loss value: 0.8343450427055359\n",
      "Epoch[00/01] | Iteration[0621/0782] - Loss value: 0.8275704383850098\n",
      "Epoch[00/01] | Iteration[0622/0782] - Loss value: 0.6932608485221863\n",
      "Epoch[00/01] | Iteration[0623/0782] - Loss value: 0.8821351528167725\n",
      "Epoch[00/01] | Iteration[0624/0782] - Loss value: 0.9948022961616516\n",
      "Epoch[00/01] | Iteration[0625/0782] - Loss value: 0.8619665503501892\n",
      "Epoch[00/01] | Iteration[0626/0782] - Loss value: 1.09972083568573\n",
      "Epoch[00/01] | Iteration[0627/0782] - Loss value: 0.9487794637680054\n",
      "Epoch[00/01] | Iteration[0628/0782] - Loss value: 0.8067458868026733\n",
      "Epoch[00/01] | Iteration[0629/0782] - Loss value: 0.9466972947120667\n",
      "Epoch[00/01] | Iteration[0630/0782] - Loss value: 0.6444234251976013\n",
      "Epoch[00/01] | Iteration[0631/0782] - Loss value: 1.0626475811004639\n",
      "Epoch[00/01] | Iteration[0632/0782] - Loss value: 0.9792002439498901\n",
      "Epoch[00/01] | Iteration[0633/0782] - Loss value: 1.0307631492614746\n",
      "Epoch[00/01] | Iteration[0634/0782] - Loss value: 0.9079276323318481\n",
      "Epoch[00/01] | Iteration[0635/0782] - Loss value: 0.8502231240272522\n",
      "Epoch[00/01] | Iteration[0636/0782] - Loss value: 0.8317015767097473\n",
      "Epoch[00/01] | Iteration[0637/0782] - Loss value: 0.7176665663719177\n",
      "Epoch[00/01] | Iteration[0638/0782] - Loss value: 0.7295246124267578\n",
      "Epoch[00/01] | Iteration[0639/0782] - Loss value: 0.8711203932762146\n",
      "Epoch[00/01] | Iteration[0640/0782] - Loss value: 0.9656394124031067\n",
      "Epoch[00/01] | Iteration[0641/0782] - Loss value: 0.660807728767395\n",
      "Epoch[00/01] | Iteration[0642/0782] - Loss value: 0.8304161429405212\n",
      "Epoch[00/01] | Iteration[0643/0782] - Loss value: 0.8137872219085693\n",
      "Epoch[00/01] | Iteration[0644/0782] - Loss value: 0.7934772372245789\n",
      "Epoch[00/01] | Iteration[0645/0782] - Loss value: 0.7171226739883423\n",
      "Epoch[00/01] | Iteration[0646/0782] - Loss value: 1.0199450254440308\n",
      "Epoch[00/01] | Iteration[0647/0782] - Loss value: 0.9706981182098389\n",
      "Epoch[00/01] | Iteration[0648/0782] - Loss value: 0.89488285779953\n",
      "Epoch[00/01] | Iteration[0649/0782] - Loss value: 0.7940962910652161\n",
      "Epoch[00/01] | Iteration[0650/0782] - Loss value: 0.8665798306465149\n",
      "Epoch[00/01] | Iteration[0651/0782] - Loss value: 0.7169051170349121\n",
      "Epoch[00/01] | Iteration[0652/0782] - Loss value: 0.9539819359779358\n",
      "Epoch[00/01] | Iteration[0653/0782] - Loss value: 0.7493488192558289\n",
      "Epoch[00/01] | Iteration[0654/0782] - Loss value: 0.6083411574363708\n",
      "Epoch[00/01] | Iteration[0655/0782] - Loss value: 0.7263598442077637\n",
      "Epoch[00/01] | Iteration[0656/0782] - Loss value: 0.9439873695373535\n",
      "Epoch[00/01] | Iteration[0657/0782] - Loss value: 1.0670572519302368\n",
      "Epoch[00/01] | Iteration[0658/0782] - Loss value: 0.8144957423210144\n",
      "Epoch[00/01] | Iteration[0659/0782] - Loss value: 0.8967698812484741\n",
      "Epoch[00/01] | Iteration[0660/0782] - Loss value: 0.8859553933143616\n",
      "Epoch[00/01] | Iteration[0661/0782] - Loss value: 1.0167237520217896\n",
      "Epoch[00/01] | Iteration[0662/0782] - Loss value: 0.8321219682693481\n",
      "Epoch[00/01] | Iteration[0663/0782] - Loss value: 0.9853996634483337\n",
      "Epoch[00/01] | Iteration[0664/0782] - Loss value: 0.7400511503219604\n",
      "Epoch[00/01] | Iteration[0665/0782] - Loss value: 0.8523483872413635\n",
      "Epoch[00/01] | Iteration[0666/0782] - Loss value: 0.8997141122817993\n",
      "Epoch[00/01] | Iteration[0667/0782] - Loss value: 0.8363571763038635\n",
      "Epoch[00/01] | Iteration[0668/0782] - Loss value: 0.9372344613075256\n",
      "Epoch[00/01] | Iteration[0669/0782] - Loss value: 0.9489721655845642\n",
      "Epoch[00/01] | Iteration[0670/0782] - Loss value: 0.6578627824783325\n",
      "Epoch[00/01] | Iteration[0671/0782] - Loss value: 0.9556615948677063\n",
      "Epoch[00/01] | Iteration[0672/0782] - Loss value: 0.8365646600723267\n",
      "Epoch[00/01] | Iteration[0673/0782] - Loss value: 0.8049390912055969\n",
      "Epoch[00/01] | Iteration[0674/0782] - Loss value: 1.2348034381866455\n",
      "Epoch[00/01] | Iteration[0675/0782] - Loss value: 0.816335916519165\n",
      "Epoch[00/01] | Iteration[0676/0782] - Loss value: 0.810716986656189\n",
      "Epoch[00/01] | Iteration[0677/0782] - Loss value: 0.8245818018913269\n",
      "Epoch[00/01] | Iteration[0678/0782] - Loss value: 0.8067882657051086\n",
      "Epoch[00/01] | Iteration[0679/0782] - Loss value: 0.8614395260810852\n",
      "Epoch[00/01] | Iteration[0680/0782] - Loss value: 0.8232342004776001\n",
      "Epoch[00/01] | Iteration[0681/0782] - Loss value: 0.9574099183082581\n",
      "Epoch[00/01] | Iteration[0682/0782] - Loss value: 0.8849166631698608\n",
      "Epoch[00/01] | Iteration[0683/0782] - Loss value: 0.766223669052124\n",
      "Epoch[00/01] | Iteration[0684/0782] - Loss value: 0.7813954949378967\n",
      "Epoch[00/01] | Iteration[0685/0782] - Loss value: 0.8839775323867798\n",
      "Epoch[00/01] | Iteration[0686/0782] - Loss value: 0.8887689709663391\n",
      "Epoch[00/01] | Iteration[0687/0782] - Loss value: 1.033684253692627\n",
      "Epoch[00/01] | Iteration[0688/0782] - Loss value: 0.8496692776679993\n",
      "Epoch[00/01] | Iteration[0689/0782] - Loss value: 1.012455701828003\n",
      "Epoch[00/01] | Iteration[0690/0782] - Loss value: 1.0407119989395142\n",
      "Epoch[00/01] | Iteration[0691/0782] - Loss value: 0.9210261106491089\n",
      "Epoch[00/01] | Iteration[0692/0782] - Loss value: 0.6885454058647156\n",
      "Epoch[00/01] | Iteration[0693/0782] - Loss value: 0.7431314587593079\n",
      "Epoch[00/01] | Iteration[0694/0782] - Loss value: 1.0348390340805054\n",
      "Epoch[00/01] | Iteration[0695/0782] - Loss value: 0.9603832960128784\n",
      "Epoch[00/01] | Iteration[0696/0782] - Loss value: 0.9484309554100037\n",
      "Epoch[00/01] | Iteration[0697/0782] - Loss value: 1.1558687686920166\n",
      "Epoch[00/01] | Iteration[0698/0782] - Loss value: 0.881742000579834\n",
      "Epoch[00/01] | Iteration[0699/0782] - Loss value: 0.6623952984809875\n",
      "Epoch[00/01] | Iteration[0700/0782] - Loss value: 0.9974543452262878\n",
      "Epoch[00/01] | Iteration[0701/0782] - Loss value: 1.0928517580032349\n",
      "Epoch[00/01] | Iteration[0702/0782] - Loss value: 0.9221088290214539\n",
      "Epoch[00/01] | Iteration[0703/0782] - Loss value: 0.7846910953521729\n",
      "Epoch[00/01] | Iteration[0704/0782] - Loss value: 0.7324693202972412\n",
      "Epoch[00/01] | Iteration[0705/0782] - Loss value: 0.795290470123291\n",
      "Epoch[00/01] | Iteration[0706/0782] - Loss value: 0.6005098819732666\n",
      "Epoch[00/01] | Iteration[0707/0782] - Loss value: 0.8259113430976868\n",
      "Epoch[00/01] | Iteration[0708/0782] - Loss value: 0.8298726081848145\n",
      "Epoch[00/01] | Iteration[0709/0782] - Loss value: 1.2426594495773315\n",
      "Epoch[00/01] | Iteration[0710/0782] - Loss value: 0.6161248087882996\n",
      "Epoch[00/01] | Iteration[0711/0782] - Loss value: 0.7106800675392151\n",
      "Epoch[00/01] | Iteration[0712/0782] - Loss value: 0.8927405476570129\n",
      "Epoch[00/01] | Iteration[0713/0782] - Loss value: 0.8260782957077026\n",
      "Epoch[00/01] | Iteration[0714/0782] - Loss value: 0.9668548107147217\n",
      "Epoch[00/01] | Iteration[0715/0782] - Loss value: 1.1027623414993286\n",
      "Epoch[00/01] | Iteration[0716/0782] - Loss value: 0.9484200477600098\n",
      "Epoch[00/01] | Iteration[0717/0782] - Loss value: 0.7700321674346924\n",
      "Epoch[00/01] | Iteration[0718/0782] - Loss value: 0.7683197855949402\n",
      "Epoch[00/01] | Iteration[0719/0782] - Loss value: 0.6389105916023254\n",
      "Epoch[00/01] | Iteration[0720/0782] - Loss value: 0.848926842212677\n",
      "Epoch[00/01] | Iteration[0721/0782] - Loss value: 0.7781853675842285\n",
      "Epoch[00/01] | Iteration[0722/0782] - Loss value: 0.8075898289680481\n",
      "Epoch[00/01] | Iteration[0723/0782] - Loss value: 0.7356679439544678\n",
      "Epoch[00/01] | Iteration[0724/0782] - Loss value: 0.917621910572052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[00/01] | Iteration[0725/0782] - Loss value: 0.8373074531555176\n",
      "Epoch[00/01] | Iteration[0726/0782] - Loss value: 0.7368305325508118\n",
      "Epoch[00/01] | Iteration[0727/0782] - Loss value: 0.8069965243339539\n",
      "Epoch[00/01] | Iteration[0728/0782] - Loss value: 0.8201723098754883\n",
      "Epoch[00/01] | Iteration[0729/0782] - Loss value: 0.7319982647895813\n",
      "Epoch[00/01] | Iteration[0730/0782] - Loss value: 0.965031087398529\n",
      "Epoch[00/01] | Iteration[0731/0782] - Loss value: 0.8965201377868652\n",
      "Epoch[00/01] | Iteration[0732/0782] - Loss value: 0.7268739342689514\n",
      "Epoch[00/01] | Iteration[0733/0782] - Loss value: 0.6800922751426697\n",
      "Epoch[00/01] | Iteration[0734/0782] - Loss value: 1.1945714950561523\n",
      "Epoch[00/01] | Iteration[0735/0782] - Loss value: 0.8962424397468567\n",
      "Epoch[00/01] | Iteration[0736/0782] - Loss value: 0.7739585638046265\n",
      "Epoch[00/01] | Iteration[0737/0782] - Loss value: 0.7546209096908569\n",
      "Epoch[00/01] | Iteration[0738/0782] - Loss value: 0.7043880820274353\n",
      "Epoch[00/01] | Iteration[0739/0782] - Loss value: 0.7725162506103516\n",
      "Epoch[00/01] | Iteration[0740/0782] - Loss value: 0.6452879309654236\n",
      "Epoch[00/01] | Iteration[0741/0782] - Loss value: 0.7905505895614624\n",
      "Epoch[00/01] | Iteration[0742/0782] - Loss value: 0.8794203400611877\n",
      "Epoch[00/01] | Iteration[0743/0782] - Loss value: 0.6230156421661377\n",
      "Epoch[00/01] | Iteration[0744/0782] - Loss value: 0.8298487663269043\n",
      "Epoch[00/01] | Iteration[0745/0782] - Loss value: 0.6633932590484619\n",
      "Epoch[00/01] | Iteration[0746/0782] - Loss value: 0.6459571123123169\n",
      "Epoch[00/01] | Iteration[0747/0782] - Loss value: 0.7708215713500977\n",
      "Epoch[00/01] | Iteration[0748/0782] - Loss value: 0.6600053310394287\n",
      "Epoch[00/01] | Iteration[0749/0782] - Loss value: 1.0112628936767578\n",
      "Epoch[00/01] | Iteration[0750/0782] - Loss value: 0.8876920342445374\n",
      "Epoch[00/01] | Iteration[0751/0782] - Loss value: 0.7534881830215454\n",
      "Epoch[00/01] | Iteration[0752/0782] - Loss value: 0.857169508934021\n",
      "Epoch[00/01] | Iteration[0753/0782] - Loss value: 0.692609965801239\n",
      "Epoch[00/01] | Iteration[0754/0782] - Loss value: 0.9572352766990662\n",
      "Epoch[00/01] | Iteration[0755/0782] - Loss value: 0.930468738079071\n",
      "Epoch[00/01] | Iteration[0756/0782] - Loss value: 0.990000307559967\n",
      "Epoch[00/01] | Iteration[0757/0782] - Loss value: 1.1069198846817017\n",
      "Epoch[00/01] | Iteration[0758/0782] - Loss value: 0.8621045351028442\n",
      "Epoch[00/01] | Iteration[0759/0782] - Loss value: 0.6014823317527771\n",
      "Epoch[00/01] | Iteration[0760/0782] - Loss value: 1.054980993270874\n",
      "Epoch[00/01] | Iteration[0761/0782] - Loss value: 0.5997458100318909\n",
      "Epoch[00/01] | Iteration[0762/0782] - Loss value: 0.9685895442962646\n",
      "Epoch[00/01] | Iteration[0763/0782] - Loss value: 0.8934987783432007\n",
      "Epoch[00/01] | Iteration[0764/0782] - Loss value: 1.0081558227539062\n",
      "Epoch[00/01] | Iteration[0765/0782] - Loss value: 0.888365626335144\n",
      "Epoch[00/01] | Iteration[0766/0782] - Loss value: 0.8160898685455322\n",
      "Epoch[00/01] | Iteration[0767/0782] - Loss value: 0.6094212532043457\n",
      "Epoch[00/01] | Iteration[0768/0782] - Loss value: 0.766941487789154\n",
      "Epoch[00/01] | Iteration[0769/0782] - Loss value: 0.6278762817382812\n",
      "Epoch[00/01] | Iteration[0770/0782] - Loss value: 0.758054792881012\n",
      "Epoch[00/01] | Iteration[0771/0782] - Loss value: 0.6809074878692627\n",
      "Epoch[00/01] | Iteration[0772/0782] - Loss value: 0.7607909440994263\n",
      "Epoch[00/01] | Iteration[0773/0782] - Loss value: 0.9071497917175293\n",
      "Epoch[00/01] | Iteration[0774/0782] - Loss value: 0.8595938086509705\n",
      "Epoch[00/01] | Iteration[0775/0782] - Loss value: 0.6374883651733398\n",
      "Epoch[00/01] | Iteration[0776/0782] - Loss value: 0.8627766966819763\n",
      "Epoch[00/01] | Iteration[0777/0782] - Loss value: 0.8565099835395813\n",
      "Epoch[00/01] | Iteration[0778/0782] - Loss value: 0.794439435005188\n",
      "Epoch[00/01] | Iteration[0779/0782] - Loss value: 0.782074511051178\n",
      "Epoch[00/01] | Iteration[0780/0782] - Loss value: 0.9932976961135864\n",
      "Epoch[00/01] | Iteration[0781/0782] - Loss value: 0.5811229348182678\n",
      "Epoch[00/01] - Validation accuracy:  59.53%\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1\n",
    "fit(device, train_loader, valid_loader, network, loss_fn, optimiser, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a17799",
   "metadata": {},
   "source": [
    "<a id='sec:float2fake'></a>\n",
    "## Training QNNs with the PACT/SAWB algorithm\n",
    "\n",
    "We are now ready to introduce the quantisation problem.\n",
    "In the rest of the notebook, we will use the `quantlib` package to manipulate and train QNNs.\n",
    "We will focus on the PACT/SAWB quantisation-aware training (QAT) algorithm.\n",
    "\n",
    "In our specific case, we will not train the network from scratch, but introduce quantisers in the pre-trained computational graph and run a few iterations of quantisation-aware fine-tuning (QAFT).\n",
    "Starting from a pre-trained model is common practice in machine learning, since it alleviates the cost of the most resource-demanding part of the development process: training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c1fa3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vgg_dir_logs() -> os.PathLike:\n",
    "    \"\"\"Create a directory where PyTorch checkpoints of the VGG network can be saved.\"\"\"\n",
    "    dir_logs = os.path.join(os.curdir, 'logs_vgg')\n",
    "    if not os.path.isdir(dir_logs):\n",
    "        os.makedirs(dir_logs, exist_ok=True)\n",
    "        \n",
    "    return dir_logs\n",
    "\n",
    "\n",
    "def load_ckpt(network: nn.Module, device: torch.device, dir_logs: os.PathLike, ckpt_name: str) -> None:\n",
    "    ckpt_file = os.path.join(dir_logs, ckpt_name)\n",
    "    ckpt      = torch.load(ckpt_file, map_location=device)\n",
    "    network.load_state_dict(ckpt)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11943700",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = VGG('VGG9')\n",
    "network = network.to(device=device)\n",
    "\n",
    "dir_logs = create_vgg_dir_logs()\n",
    "load_ckpt(network, device, dir_logs, 'pretrained_vgg9_fp.ckpt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f9aab7",
   "metadata": {},
   "source": [
    "If we run our model on the points provided by the current `Dataset` objects, we can see that the performance is not great.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b65e3fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(device:       torch.device,\n",
    "             valid_loader: torch.utils.data.DataLoader,\n",
    "             network:      nn.Module) -> None:\n",
    "\n",
    "    network.eval()\n",
    "    correct = 0\n",
    "\n",
    "    for x, y_gt_int in valid_loader:\n",
    "        \n",
    "        x        = x.to(device=device)\n",
    "        y_gt_int = y_gt_int.to(device=device)\n",
    "        \n",
    "        y_pr     = network(x)\n",
    "        y_pr_int = y_pr.argmax(axis=1)\n",
    "        \n",
    "        correct += torch.sum(y_pr_int == y_gt_int).item()\n",
    "    \n",
    "    print(\"Validation accuracy: {:6.2f}%\".format(100.0 * correct / len(valid_loader.dataset)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a60b78bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy:  38.23%\n"
     ]
    }
   ],
   "source": [
    "validate(device, valid_loader, network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551d0c5f",
   "metadata": {},
   "source": [
    "Why does this happen?\n",
    "The pre-trained model that we just loaded has been trained on CIFAR-10 data points which were normalised, i.e., transformed in such a way that the mean of the pixel components over all the training set is zero and their variance is one.\n",
    "If we apply the same normalisation transform before feeding the data point to the network, we see that the performance is much more in line with what we expect from a well-trained model solving CIFAR-10.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cabd9da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CIFAR10_STATS_NORMALISE = \\\n",
    "{\n",
    "    'mean': (0.4914, 0.4822, 0.4465),\n",
    "    'std':  (0.2470, 0.2430, 0.2610)\n",
    "}\n",
    "\n",
    "\n",
    "def add_normalisation_transform(data_loader: torch.utils.data.DataLoader) -> None:\n",
    "    \n",
    "    transform_list  = []\n",
    "    transform_list += [data_loader.dataset.transform]\n",
    "    transform_list += [transforms.Normalize(**CIFAR10_STATS_NORMALISE)]\n",
    "    \n",
    "    data_loader.dataset.transform = transforms.Compose(transform_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c286663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy:  93.05%\n"
     ]
    }
   ],
   "source": [
    "add_normalisation_transform(train_loader)\n",
    "add_normalisation_transform(valid_loader)\n",
    "\n",
    "validate(device, valid_loader, network)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0017f1d",
   "metadata": {},
   "source": [
    "The computational graph of our VGG network uses floating-point operands, i.e., it is a floating-point computational graph.\n",
    "\n",
    "As a first step towards quantisation, we need to introduce `Module`s that support PACT/SAWB.\n",
    "This step is called the **float-to-fake** (F2F) conversion: it introduces linear quantisers into the computational graph, but at the same time continues to operate with floating-point operands.\n",
    "Therefore, at this stage quantisation is only simulated (i.e., *faked*), and the computational graph is said to be **fake quantised** (FQ).\n",
    "\n",
    "However, at a later stage it will be possible to apply elementary arithmetic properties and some clever approximation rules to turn the computational graph into a program operating on integer operands and using only hardware-friendly integer operations, i.e., a **true quantised** (TQ) computational graph.\n",
    "This step is called teh **fake-to-true** (F2T) conversion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b667096d",
   "metadata": {},
   "source": [
    "To perform F2F conversion, you need to replace standard PyTorch `Module`s with corresponding FQ counterparts.\n",
    "Since this process usually involves one-to-one replacements, the required graph manipulation is relatively simple (\"lightweight\").\n",
    "The `quantlib.editing.lightweight` sub-package implements the abstractions required to perform the process:\n",
    "* `LightweightGraph`, a wrapper object that encapsulates a `Module` and computes a list of `LightweightNode`s (its composing elementary `Module`s); it can return the name and type of the `Module` objects, as well as pointers to them;\n",
    "* `LightweightRule`s, objects that take in input a `Filter` and can apply it to a list of `LightweightNode`s, then replace the `Module`s associated to the filtered nodes with user-defined `Module`s;\n",
    "* `LightweightEditor`, a manager object which takes in input a `LighweightGraph` and oranises the application of `LightweightRule`s to modify the encapsulated `Module`.\n",
    "\n",
    "F2F conversion can be solved programmatically thanks to the so-called **quantisation recipes**.\n",
    "A quantisation recipe is a function taking in input a floating-point `Module` and producing an FQ `Module`.\n",
    "To achieve the goal, it uses a `LightweightGraph` and a `LightweightEditor`, as well as user-defined `LightweightRule`s (and potentially additional parameters).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d2ac64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import quantlib.editing.lightweight as qlw\n",
    "\n",
    "\n",
    "def all_pact_f2f_recipe(network: nn.Module, name2config: Dict[str, Dict]) -> nn.Module:\n",
    "\n",
    "    lwg = qlw.LightweightGraph(network)\n",
    "    name2type = {n.name: n.module.__class__.__name__ for n in lwg.nodes_list}\n",
    "\n",
    "    # generate lightweight (i.e., atomic) replacement rules\n",
    "    assert set(name2config.keys()).issubset(set(name2type.keys()))\n",
    "    type2rule = \\\n",
    "    {\n",
    "        'Conv2d': qlw.rules.pact.ReplaceConvLinearPACTRule,\n",
    "        'Linear': qlw.rules.pact.ReplaceConvLinearPACTRule,\n",
    "        'ReLU':   qlw.rules.pact.ReplaceActPACTRule\n",
    "    }\n",
    "    rhos = list(map(lambda n: type2rule[name2type[n]](qlw.rules.NameFilter(n), **name2config[n]), name2config.keys()))\n",
    "    \n",
    "    # boot lightweight editor and apply atomic rules\n",
    "    lwe = qlw.LightweightEditor(lwg)\n",
    "    lwe.startup()\n",
    "    for rho in rhos:\n",
    "        lwe.set_lwr(rho)\n",
    "        lwe.apply()\n",
    "    lwe.shutdown()\n",
    "    \n",
    "    return lwe.graph.net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a78b276",
   "metadata": {},
   "source": [
    "To simplify the exploration of different quantisation policies, we aim at defining the configurations of quantised nodes in a programmatic way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ffd6470f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def all_pact_create_configs(network: nn.Module, patches: Dict[str, Dict]) -> Dict[str, Dict]:\n",
    "\n",
    "    lwg = qlw.LightweightGraph(network)\n",
    "    conv2d_nodes = set([n.name for n in lwg.nodes_list if n.module.__class__.__name__ == 'Conv2d'])\n",
    "    linear_nodes = set([n.name for n in lwg.nodes_list if n.module.__class__.__name__ == 'Linear'])\n",
    "    relu_nodes   = set([n.name for n in lwg.nodes_list if n.module.__class__.__name__ == 'ReLU'])\n",
    "    assert set(patches.keys()).issubset(conv2d_nodes | linear_nodes | relu_nodes)\n",
    "\n",
    "    # configure convolutional nodes\n",
    "    conv2d_default = \\\n",
    "    {\n",
    "        'quantize':   'per_channel',\n",
    "        'init_clip':  'sawb_asymm',\n",
    "        'learn_clip': False,\n",
    "        'symm_wts':   True,\n",
    "        'tqt':        False,\n",
    "        'n_levels':   4\n",
    "    }\n",
    "\n",
    "    conv2d_config = defaultdict(lambda: conv2d_default.copy())  # it is EXTREMELY important that we return a copy of the dictionary and not the dictionary itself; otherwise, all configs would point to the same (possibly updated) object\n",
    "    for n in conv2d_nodes:\n",
    "        conv2d_config[n].update(patches[n] if n in patches.keys() else {})  # patches have higher priority than default configurations\n",
    "\n",
    "    # configure linear nodes\n",
    "    linear_default = \\\n",
    "    {\n",
    "        'quantize':   'per_layer',\n",
    "        'init_clip':  'sawb_asymm',\n",
    "        'learn_clip': False,\n",
    "        'symm_wts':   True,\n",
    "        'tqt':        False,\n",
    "        'n_levels':   4\n",
    "    }\n",
    "\n",
    "    linear_config = defaultdict(lambda: linear_default.copy())\n",
    "    for n in linear_nodes:\n",
    "        linear_config[n].update(patches[n] if n in patches.keys() else {})  # patches have higher priority than default configurations\n",
    "\n",
    "    # configure ReLU nodes\n",
    "    relu_default = \\\n",
    "    {\n",
    "        'init_clip':  'std',\n",
    "        'learn_clip': True,\n",
    "        'nb_std':     3,\n",
    "        'rounding':   False,\n",
    "        'tqt':        False,\n",
    "        'n_levels':   4\n",
    "    }\n",
    "\n",
    "    relu_config = defaultdict(lambda: relu_default.copy())\n",
    "    for n in relu_nodes:\n",
    "        relu_config[n].update(patches[n] if n in patches.keys() else {})  # patches have higher priority than default configurations\n",
    "\n",
    "    # create complete configuration\n",
    "    config = {**conv2d_config, **linear_config, **relu_config}\n",
    "\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2e5917",
   "metadata": {},
   "source": [
    "We can see that the default configuration for each PACT node sets $2^{2} = 4$ quantisation levels, in such a way that the operands can be encoded using only two bits.\n",
    "\n",
    "However, the pixels of RGB images are encoded using three 8-bit bytes each.\n",
    "Moreover, both theoretical and experimental research on QNNs has shown that keeping the last layers operands at high precision benefits accuracy.\n",
    "For these reasons, we increase the precision of the weights of the first convolutional node, of the last feature array, and of the weights of the last linear node to $2^{8} = 256$ quantisation levels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e68d990f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (pilot): Sequential(\n",
       "    (0): PACTConv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, n_levels=256, quantize='per_channel', init_clip='sawb_asymm', learn_clip=False, symm_wts=True, nb_std=3, tqt=False, tqt_beta=0.90, tqt_clip_grad=True)\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): PACTUnsignedAct(n_levels=4, init_clip='std', learn_clip=True, act_kind='relu', leaky=0.1, nb_std=3, tqt=False, tqt_beta=0.90, tqt_clip_grad=True)\n",
       "  )\n",
       "  (features): Sequential(\n",
       "    (0): PACTConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, n_levels=4, quantize='per_channel', init_clip='sawb_asymm', learn_clip=False, symm_wts=True, nb_std=3, tqt=False, tqt_beta=0.90, tqt_clip_grad=True)\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): PACTUnsignedAct(n_levels=4, init_clip='std', learn_clip=True, act_kind='relu', leaky=0.1, nb_std=3, tqt=False, tqt_beta=0.90, tqt_clip_grad=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): PACTConv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, n_levels=4, quantize='per_channel', init_clip='sawb_asymm', learn_clip=False, symm_wts=True, nb_std=3, tqt=False, tqt_beta=0.90, tqt_clip_grad=True)\n",
       "    (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): PACTUnsignedAct(n_levels=4, init_clip='std', learn_clip=True, act_kind='relu', leaky=0.1, nb_std=3, tqt=False, tqt_beta=0.90, tqt_clip_grad=True)\n",
       "    (7): PACTConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, n_levels=4, quantize='per_channel', init_clip='sawb_asymm', learn_clip=False, symm_wts=True, nb_std=3, tqt=False, tqt_beta=0.90, tqt_clip_grad=True)\n",
       "    (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): PACTUnsignedAct(n_levels=4, init_clip='std', learn_clip=True, act_kind='relu', leaky=0.1, nb_std=3, tqt=False, tqt_beta=0.90, tqt_clip_grad=True)\n",
       "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (11): PACTConv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, n_levels=4, quantize='per_channel', init_clip='sawb_asymm', learn_clip=False, symm_wts=True, nb_std=3, tqt=False, tqt_beta=0.90, tqt_clip_grad=True)\n",
       "    (12): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (13): PACTUnsignedAct(n_levels=4, init_clip='std', learn_clip=True, act_kind='relu', leaky=0.1, nb_std=3, tqt=False, tqt_beta=0.90, tqt_clip_grad=True)\n",
       "    (14): PACTConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, n_levels=4, quantize='per_channel', init_clip='sawb_asymm', learn_clip=False, symm_wts=True, nb_std=3, tqt=False, tqt_beta=0.90, tqt_clip_grad=True)\n",
       "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): PACTUnsignedAct(n_levels=4, init_clip='std', learn_clip=True, act_kind='relu', leaky=0.1, nb_std=3, tqt=False, tqt_beta=0.90, tqt_clip_grad=True)\n",
       "    (17): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(4, 4))\n",
       "  (classifier): Sequential(\n",
       "    (0): PACTLinear(in_features=8192, out_features=1024, bias=False, n_levels=4, quantize='per_layer', init_clip='sawb_asymm', learn_clip=False, symm_wts=True, nb_std=3, tqt=False, tqt_beta=0.90, tqt_clip_grad=True)\n",
       "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): PACTUnsignedAct(n_levels=4, init_clip='std', learn_clip=True, act_kind='relu', leaky=0.1, nb_std=3, tqt=False, tqt_beta=0.90, tqt_clip_grad=True)\n",
       "    (3): PACTLinear(in_features=1024, out_features=1024, bias=False, n_levels=4, quantize='per_layer', init_clip='sawb_asymm', learn_clip=False, symm_wts=True, nb_std=3, tqt=False, tqt_beta=0.90, tqt_clip_grad=True)\n",
       "    (4): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): PACTUnsignedAct(n_levels=256, init_clip='std', learn_clip=True, act_kind='relu', leaky=0.1, nb_std=3, tqt=False, tqt_beta=0.90, tqt_clip_grad=True)\n",
       "    (6): PACTLinear(in_features=1024, out_features=10, bias=True, n_levels=256, quantize='per_layer', init_clip='sawb_asymm', learn_clip=False, symm_wts=True, nb_std=3, tqt=False, tqt_beta=0.90, tqt_clip_grad=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create configuration for PACT float-to-fake conversion\n",
    "patches = \\\n",
    "{\n",
    "    'pilot.0':      {'n_levels': 256},\n",
    "    'classifier.5': {'n_levels': 256},\n",
    "    'classifier.6': {'n_levels': 256}\n",
    "}\n",
    "name2config = all_pact_create_configs(network, patches)\n",
    "\n",
    "# apply PACT float-to-fake conversion\n",
    "pact_network = all_pact_f2f_recipe(network, name2config)\n",
    "pact_network.to(device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e26d33",
   "metadata": {},
   "source": [
    "Many training algorithms for quantised neural networks allow users to specify hyper-parameters that drive the dynamics of the quantisation process.\n",
    "In `quantlib`, these hyper-parameters are set and controlled by `Controller`s.\n",
    "We implemented `Controller`s for all the supported QAFT/QAT algorithms.\n",
    "Given a specific QAFT/QAT algorithm, we follow the convention of naming the class of the associated `Controller` by prefixing `Controller` with the acronym of the algorithm (e.g., `PACTController` for PACT/SAWB).\n",
    "\n",
    "Each `Controller` class has a specific `get_modules` static method that can be applied to filter a FQ `Module` and return all the elementary FQ `Module`s supporting the corresponding QAFT/QAT algorithm.\n",
    "\n",
    "Analogously to the concept of F2F recipe, we define a **controller recipe** to be a function that takes in input a `Module` and a description of the hyper-parameters of the associated QAFT/QAT training algorithms, and returns `Controller`s that implement the desired training strategy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df4830b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import quantlib.algorithms as qa\n",
    "\n",
    "\n",
    "def all_ana_get_controllers(network:         nn.Module,\n",
    "                            schedule_linear: Dict[int, Union[str, List[str]]],\n",
    "                            schedule_act:    Dict[int, Union[str, List[str]]],\n",
    "                            kwargs_linear:   Dict = {},\n",
    "                            kwargs_act:      Dict = {}) -> Tuple[qa.pact.PACTLinearController, qa.pact.PACTActController]:\n",
    "    \n",
    "    modules_linear    = qa.pact.PACTLinearController.get_modules(network)\n",
    "    controller_linear = qa.pact.PACTLinearController(modules_linear, schedule_linear, **kwargs_linear)\n",
    "    \n",
    "    modules_act    = qa.pact.PACTActController.get_modules(network)\n",
    "    controller_act = qa.pact.PACTActController(modules_act, schedule_act, **kwargs_act)\n",
    "\n",
    "    return controller_linear, controller_act\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07d4d992",
   "metadata": {},
   "outputs": [],
   "source": [
    "pact_schedule_linear = \\\n",
    "{\n",
    "    0: ['verbose_on', 'start']\n",
    "}\n",
    "\n",
    "pact_schedule_act = \\\n",
    "{\n",
    "    0: ['verbose_on', 'start']\n",
    "}\n",
    "\n",
    "pact_controller_linear, pact_controller_act = all_ana_get_controllers(pact_network, pact_schedule_linear, pact_schedule_act)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e8342c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn   = nn.CrossEntropyLoss()\n",
    "optimiser = qa.pact.PACTAdam(pact_network, pact_decay=0.001, lr=0.004)\n",
    "lr_sched  = optim.lr_scheduler.CosineAnnealingLR(optimiser, T_max=250, eta_min=0.00001)  # in this 'LRScheduler', the value of 'T_max' should be set equal to the number of epochs for which you plan to train the network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "42f18f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_quantised(device:          torch.device,\n",
    "                  train_loader:    torch.utils.data.DataLoader,\n",
    "                  valid_loader:    torch.utils.data.DataLoader,\n",
    "                  network:         nn.Module,\n",
    "                  qnt_controllers: List[Union[qa.pact.PACTLinearController, qa.pact.PACTActController]],\n",
    "                  loss_fn:         nn.Module,\n",
    "                  optimiser:       torch.optim.Optimizer,\n",
    "                  lr_sched:        torch.optim.lr_scheduler._LRScheduler,\n",
    "                  n_epochs:        int) -> None:\n",
    "    \n",
    "    for i_epoch in range(0, n_epochs):\n",
    "        \n",
    "        # training pass\n",
    "        network.train()\n",
    "        optimiser.zero_grad()\n",
    "        \n",
    "        for qnt_ctrl in qnt_controllers:\n",
    "            qnt_ctrl.step_pre_training_epoch(i_epoch)  # NOTE: the hyper-parameters of the QAT algorithm could be updated at each epoch\n",
    "        \n",
    "        for i_batch, (x, y_gt_int) in enumerate(train_loader):\n",
    "            \n",
    "            x        = x.to(device=device)\n",
    "            y_gt_int = y_gt_int.to(device=device)\n",
    "            \n",
    "            y_pr       = network(x)\n",
    "            loss_value = loss_fn(y_pr, y_gt_int)\n",
    "            \n",
    "            loss_value.backward()\n",
    "            optimiser.step()\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "            print(\"Epoch[{:02d}/{:02d}] | Iteration[{:04d}/{:04d}] - Loss value: {}\".format(i_epoch, n_epochs, i_batch, len(train_loader), loss_value.item()))\n",
    "\n",
    "        lr_sched.step()\n",
    "        \n",
    "        # validation pass\n",
    "        network.eval()\n",
    "        correct = 0\n",
    "        \n",
    "        for qnt_ctrl in qnt_controllers:\n",
    "            qnt_ctrl.step_pre_validation_epoch(i_epoch)  # NOTE: the hyper-parameters of the QAT algorithm could be updated at each epoch\n",
    "        \n",
    "        for x, y_gt_int in valid_loader:\n",
    "            \n",
    "            x        = x.to(device=device)\n",
    "            y_gt_int = y_gt_int.to(device=device)\n",
    "            \n",
    "            y_pr     = network(x)\n",
    "            y_pr_int = y_pr.argmax(axis=1)\n",
    "            \n",
    "            correct += torch.sum(y_pr_int == y_gt_int).item()\n",
    "\n",
    "        print(\"Epoch[{:02d}/{:02d}] - Validation accuracy: {:6.2f}%\".format(i_epoch, n_epochs, 100.0 * correct / len(valid_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "292f0440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PACTLinearController]    Verbose mode enabled!\n",
      "[PACTLinearController]    Epoch 0 - running command start\n",
      "[PACTLinearController]    Started quantization!\n",
      "[PACTActController]    Verbose mode enabled!\n",
      "[PACTActController]    Epoch 0 - running command start\n",
      "[PACTActController]    Started activation quantization!\n",
      "Epoch[00/01] | Iteration[0000/0782] - Loss value: 1.7380707263946533\n",
      "Epoch[00/01] | Iteration[0001/0782] - Loss value: 1.2041376829147339\n",
      "Epoch[00/01] | Iteration[0002/0782] - Loss value: 1.0330677032470703\n",
      "Epoch[00/01] | Iteration[0003/0782] - Loss value: 1.1727932691574097\n",
      "Epoch[00/01] | Iteration[0004/0782] - Loss value: 1.43255615234375\n",
      "Epoch[00/01] | Iteration[0005/0782] - Loss value: 1.1405421495437622\n",
      "Epoch[00/01] | Iteration[0006/0782] - Loss value: 0.474385142326355\n",
      "Epoch[00/01] | Iteration[0007/0782] - Loss value: 1.0654104948043823\n",
      "Epoch[00/01] | Iteration[0008/0782] - Loss value: 1.414267659187317\n",
      "Epoch[00/01] | Iteration[0009/0782] - Loss value: 0.9874011278152466\n",
      "Epoch[00/01] | Iteration[0010/0782] - Loss value: 0.5658419728279114\n",
      "Epoch[00/01] | Iteration[0011/0782] - Loss value: 0.8777334094047546\n",
      "Epoch[00/01] | Iteration[0012/0782] - Loss value: 0.9322207570075989\n",
      "Epoch[00/01] | Iteration[0013/0782] - Loss value: 0.6831080317497253\n",
      "Epoch[00/01] | Iteration[0014/0782] - Loss value: 1.0565035343170166\n",
      "Epoch[00/01] | Iteration[0015/0782] - Loss value: 0.538179874420166\n",
      "Epoch[00/01] | Iteration[0016/0782] - Loss value: 0.8329780697822571\n",
      "Epoch[00/01] | Iteration[0017/0782] - Loss value: 0.9827203154563904\n",
      "Epoch[00/01] | Iteration[0018/0782] - Loss value: 0.7906885147094727\n",
      "Epoch[00/01] | Iteration[0019/0782] - Loss value: 0.7763180732727051\n",
      "Epoch[00/01] | Iteration[0020/0782] - Loss value: 0.7123925685882568\n",
      "Epoch[00/01] | Iteration[0021/0782] - Loss value: 0.9383823275566101\n",
      "Epoch[00/01] | Iteration[0022/0782] - Loss value: 1.360259771347046\n",
      "Epoch[00/01] | Iteration[0023/0782] - Loss value: 0.9616940021514893\n",
      "Epoch[00/01] | Iteration[0024/0782] - Loss value: 0.6326376795768738\n",
      "Epoch[00/01] | Iteration[0025/0782] - Loss value: 0.9377609491348267\n",
      "Epoch[00/01] | Iteration[0026/0782] - Loss value: 0.6322749257087708\n",
      "Epoch[00/01] | Iteration[0027/0782] - Loss value: 0.8848229646682739\n",
      "Epoch[00/01] | Iteration[0028/0782] - Loss value: 0.5163272619247437\n",
      "Epoch[00/01] | Iteration[0029/0782] - Loss value: 0.4271543622016907\n",
      "Epoch[00/01] | Iteration[0030/0782] - Loss value: 0.7430713176727295\n",
      "Epoch[00/01] | Iteration[0031/0782] - Loss value: 0.5109795928001404\n",
      "Epoch[00/01] | Iteration[0032/0782] - Loss value: 0.6666463017463684\n",
      "Epoch[00/01] | Iteration[0033/0782] - Loss value: 1.2212002277374268\n",
      "Epoch[00/01] | Iteration[0034/0782] - Loss value: 0.34528762102127075\n",
      "Epoch[00/01] | Iteration[0035/0782] - Loss value: 0.845998227596283\n",
      "Epoch[00/01] | Iteration[0036/0782] - Loss value: 0.6096300482749939\n",
      "Epoch[00/01] | Iteration[0037/0782] - Loss value: 1.3681143522262573\n",
      "Epoch[00/01] | Iteration[0038/0782] - Loss value: 0.4233797788619995\n",
      "Epoch[00/01] | Iteration[0039/0782] - Loss value: 0.438552588224411\n",
      "Epoch[00/01] | Iteration[0040/0782] - Loss value: 0.615071713924408\n",
      "Epoch[00/01] | Iteration[0041/0782] - Loss value: 0.44416627287864685\n",
      "Epoch[00/01] | Iteration[0042/0782] - Loss value: 0.34501221776008606\n",
      "Epoch[00/01] | Iteration[0043/0782] - Loss value: 1.1039422750473022\n",
      "Epoch[00/01] | Iteration[0044/0782] - Loss value: 0.3890734612941742\n",
      "Epoch[00/01] | Iteration[0045/0782] - Loss value: 0.43382057547569275\n",
      "Epoch[00/01] | Iteration[0046/0782] - Loss value: 0.5875651836395264\n",
      "Epoch[00/01] | Iteration[0047/0782] - Loss value: 0.6785071492195129\n",
      "Epoch[00/01] | Iteration[0048/0782] - Loss value: 0.4750010669231415\n",
      "Epoch[00/01] | Iteration[0049/0782] - Loss value: 1.0462101697921753\n",
      "Epoch[00/01] | Iteration[0050/0782] - Loss value: 0.3497520685195923\n",
      "Epoch[00/01] | Iteration[0051/0782] - Loss value: 0.6388516426086426\n",
      "Epoch[00/01] | Iteration[0052/0782] - Loss value: 0.6562386751174927\n",
      "Epoch[00/01] | Iteration[0053/0782] - Loss value: 0.590853750705719\n",
      "Epoch[00/01] | Iteration[0054/0782] - Loss value: 0.3812140226364136\n",
      "Epoch[00/01] | Iteration[0055/0782] - Loss value: 0.6924201250076294\n",
      "Epoch[00/01] | Iteration[0056/0782] - Loss value: 0.728759765625\n",
      "Epoch[00/01] | Iteration[0057/0782] - Loss value: 1.1150333881378174\n",
      "Epoch[00/01] | Iteration[0058/0782] - Loss value: 0.7914858460426331\n",
      "Epoch[00/01] | Iteration[0059/0782] - Loss value: 0.3399055302143097\n",
      "Epoch[00/01] | Iteration[0060/0782] - Loss value: 0.5007908344268799\n",
      "Epoch[00/01] | Iteration[0061/0782] - Loss value: 0.8553759455680847\n",
      "Epoch[00/01] | Iteration[0062/0782] - Loss value: 0.6886035799980164\n",
      "Epoch[00/01] | Iteration[0063/0782] - Loss value: 0.7178710103034973\n",
      "Epoch[00/01] | Iteration[0064/0782] - Loss value: 0.8206791877746582\n",
      "Epoch[00/01] | Iteration[0065/0782] - Loss value: 0.2750120460987091\n",
      "Epoch[00/01] | Iteration[0066/0782] - Loss value: 0.45589831471443176\n",
      "Epoch[00/01] | Iteration[0067/0782] - Loss value: 0.6244702339172363\n",
      "Epoch[00/01] | Iteration[0068/0782] - Loss value: 0.19239552319049835\n",
      "Epoch[00/01] | Iteration[0069/0782] - Loss value: 0.4281699061393738\n",
      "Epoch[00/01] | Iteration[0070/0782] - Loss value: 0.3436840772628784\n",
      "Epoch[00/01] | Iteration[0071/0782] - Loss value: 0.19697631895542145\n",
      "Epoch[00/01] | Iteration[0072/0782] - Loss value: 0.6706699728965759\n",
      "Epoch[00/01] | Iteration[0073/0782] - Loss value: 0.38855379819869995\n",
      "Epoch[00/01] | Iteration[0074/0782] - Loss value: 0.69141685962677\n",
      "Epoch[00/01] | Iteration[0075/0782] - Loss value: 0.5920093059539795\n",
      "Epoch[00/01] | Iteration[0076/0782] - Loss value: 0.6120023727416992\n",
      "Epoch[00/01] | Iteration[0077/0782] - Loss value: 0.3718120753765106\n",
      "Epoch[00/01] | Iteration[0078/0782] - Loss value: 0.2369479387998581\n",
      "Epoch[00/01] | Iteration[0079/0782] - Loss value: 0.42590734362602234\n",
      "Epoch[00/01] | Iteration[0080/0782] - Loss value: 0.40443822741508484\n",
      "Epoch[00/01] | Iteration[0081/0782] - Loss value: 0.19853845238685608\n",
      "Epoch[00/01] | Iteration[0082/0782] - Loss value: 0.6543741226196289\n",
      "Epoch[00/01] | Iteration[0083/0782] - Loss value: 0.14933282136917114\n",
      "Epoch[00/01] | Iteration[0084/0782] - Loss value: 0.4346858859062195\n",
      "Epoch[00/01] | Iteration[0085/0782] - Loss value: 0.53831946849823\n",
      "Epoch[00/01] | Iteration[0086/0782] - Loss value: 0.200677290558815\n",
      "Epoch[00/01] | Iteration[0087/0782] - Loss value: 0.1657814085483551\n",
      "Epoch[00/01] | Iteration[0088/0782] - Loss value: 0.3281031548976898\n",
      "Epoch[00/01] | Iteration[0089/0782] - Loss value: 0.4772869646549225\n",
      "Epoch[00/01] | Iteration[0090/0782] - Loss value: 0.3924279808998108\n",
      "Epoch[00/01] | Iteration[0091/0782] - Loss value: 0.47941818833351135\n",
      "Epoch[00/01] | Iteration[0092/0782] - Loss value: 0.74217289686203\n",
      "Epoch[00/01] | Iteration[0093/0782] - Loss value: 0.3170628249645233\n",
      "Epoch[00/01] | Iteration[0094/0782] - Loss value: 0.4156700372695923\n",
      "Epoch[00/01] | Iteration[0095/0782] - Loss value: 0.4187917709350586\n",
      "Epoch[00/01] | Iteration[0096/0782] - Loss value: 0.41752758622169495\n",
      "Epoch[00/01] | Iteration[0097/0782] - Loss value: 0.5419849753379822\n",
      "Epoch[00/01] | Iteration[0098/0782] - Loss value: 0.4246186316013336\n",
      "Epoch[00/01] | Iteration[0099/0782] - Loss value: 0.4702204465866089\n",
      "Epoch[00/01] | Iteration[0100/0782] - Loss value: 0.4544181525707245\n",
      "Epoch[00/01] | Iteration[0101/0782] - Loss value: 0.47650447487831116\n",
      "Epoch[00/01] | Iteration[0102/0782] - Loss value: 0.2927001416683197\n",
      "Epoch[00/01] | Iteration[0103/0782] - Loss value: 0.5128070116043091\n",
      "Epoch[00/01] | Iteration[0104/0782] - Loss value: 0.6928447484970093\n",
      "Epoch[00/01] | Iteration[0105/0782] - Loss value: 0.41080206632614136\n",
      "Epoch[00/01] | Iteration[0106/0782] - Loss value: 0.5554909110069275\n",
      "Epoch[00/01] | Iteration[0107/0782] - Loss value: 0.615061342716217\n",
      "Epoch[00/01] | Iteration[0108/0782] - Loss value: 0.27105212211608887\n",
      "Epoch[00/01] | Iteration[0109/0782] - Loss value: 0.20341138541698456\n",
      "Epoch[00/01] | Iteration[0110/0782] - Loss value: 0.22716090083122253\n",
      "Epoch[00/01] | Iteration[0111/0782] - Loss value: 0.41461479663848877\n",
      "Epoch[00/01] | Iteration[0112/0782] - Loss value: 0.2647911608219147\n",
      "Epoch[00/01] | Iteration[0113/0782] - Loss value: 0.3941856920719147\n",
      "Epoch[00/01] | Iteration[0114/0782] - Loss value: 0.2785757780075073\n",
      "Epoch[00/01] | Iteration[0115/0782] - Loss value: 0.11780840903520584\n",
      "Epoch[00/01] | Iteration[0116/0782] - Loss value: 0.26457929611206055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[00/01] | Iteration[0117/0782] - Loss value: 0.19633059203624725\n",
      "Epoch[00/01] | Iteration[0118/0782] - Loss value: 0.313126802444458\n",
      "Epoch[00/01] | Iteration[0119/0782] - Loss value: 0.14431557059288025\n",
      "Epoch[00/01] | Iteration[0120/0782] - Loss value: 0.31877267360687256\n",
      "Epoch[00/01] | Iteration[0121/0782] - Loss value: 0.27420708537101746\n",
      "Epoch[00/01] | Iteration[0122/0782] - Loss value: 0.35605883598327637\n",
      "Epoch[00/01] | Iteration[0123/0782] - Loss value: 0.3858283460140228\n",
      "Epoch[00/01] | Iteration[0124/0782] - Loss value: 0.10388392210006714\n",
      "Epoch[00/01] | Iteration[0125/0782] - Loss value: 0.26887714862823486\n",
      "Epoch[00/01] | Iteration[0126/0782] - Loss value: 0.35733410716056824\n",
      "Epoch[00/01] | Iteration[0127/0782] - Loss value: 0.2982226014137268\n",
      "Epoch[00/01] | Iteration[0128/0782] - Loss value: 0.22134673595428467\n",
      "Epoch[00/01] | Iteration[0129/0782] - Loss value: 0.2548665702342987\n",
      "Epoch[00/01] | Iteration[0130/0782] - Loss value: 0.3901757001876831\n",
      "Epoch[00/01] | Iteration[0131/0782] - Loss value: 0.23440340161323547\n",
      "Epoch[00/01] | Iteration[0132/0782] - Loss value: 0.3521035611629486\n",
      "Epoch[00/01] | Iteration[0133/0782] - Loss value: 0.4354948103427887\n",
      "Epoch[00/01] | Iteration[0134/0782] - Loss value: 0.40435951948165894\n",
      "Epoch[00/01] | Iteration[0135/0782] - Loss value: 0.21666492521762848\n",
      "Epoch[00/01] | Iteration[0136/0782] - Loss value: 0.1880350112915039\n",
      "Epoch[00/01] | Iteration[0137/0782] - Loss value: 0.4204643666744232\n",
      "Epoch[00/01] | Iteration[0138/0782] - Loss value: 0.24181565642356873\n",
      "Epoch[00/01] | Iteration[0139/0782] - Loss value: 0.4336572587490082\n",
      "Epoch[00/01] | Iteration[0140/0782] - Loss value: 0.35780107975006104\n",
      "Epoch[00/01] | Iteration[0141/0782] - Loss value: 0.6183390617370605\n",
      "Epoch[00/01] | Iteration[0142/0782] - Loss value: 0.21185244619846344\n",
      "Epoch[00/01] | Iteration[0143/0782] - Loss value: 0.4009525179862976\n",
      "Epoch[00/01] | Iteration[0144/0782] - Loss value: 0.3906087279319763\n",
      "Epoch[00/01] | Iteration[0145/0782] - Loss value: 0.19690200686454773\n",
      "Epoch[00/01] | Iteration[0146/0782] - Loss value: 0.12113939225673676\n",
      "Epoch[00/01] | Iteration[0147/0782] - Loss value: 0.2720166742801666\n",
      "Epoch[00/01] | Iteration[0148/0782] - Loss value: 0.3047046661376953\n",
      "Epoch[00/01] | Iteration[0149/0782] - Loss value: 0.26019036769866943\n",
      "Epoch[00/01] | Iteration[0150/0782] - Loss value: 0.3503078818321228\n",
      "Epoch[00/01] | Iteration[0151/0782] - Loss value: 0.41374337673187256\n",
      "Epoch[00/01] | Iteration[0152/0782] - Loss value: 0.165505051612854\n",
      "Epoch[00/01] | Iteration[0153/0782] - Loss value: 0.38935917615890503\n",
      "Epoch[00/01] | Iteration[0154/0782] - Loss value: 0.2054528295993805\n",
      "Epoch[00/01] | Iteration[0155/0782] - Loss value: 0.2373657524585724\n",
      "Epoch[00/01] | Iteration[0156/0782] - Loss value: 0.16463878750801086\n",
      "Epoch[00/01] | Iteration[0157/0782] - Loss value: 0.21786926686763763\n",
      "Epoch[00/01] | Iteration[0158/0782] - Loss value: 0.05653703957796097\n",
      "Epoch[00/01] | Iteration[0159/0782] - Loss value: 0.37760090827941895\n",
      "Epoch[00/01] | Iteration[0160/0782] - Loss value: 0.3559136390686035\n",
      "Epoch[00/01] | Iteration[0161/0782] - Loss value: 0.287795752286911\n",
      "Epoch[00/01] | Iteration[0162/0782] - Loss value: 0.21911710500717163\n",
      "Epoch[00/01] | Iteration[0163/0782] - Loss value: 0.5847874879837036\n",
      "Epoch[00/01] | Iteration[0164/0782] - Loss value: 0.17895033955574036\n",
      "Epoch[00/01] | Iteration[0165/0782] - Loss value: 0.47722700238227844\n",
      "Epoch[00/01] | Iteration[0166/0782] - Loss value: 0.15760523080825806\n",
      "Epoch[00/01] | Iteration[0167/0782] - Loss value: 0.4498429596424103\n",
      "Epoch[00/01] | Iteration[0168/0782] - Loss value: 0.3913126587867737\n",
      "Epoch[00/01] | Iteration[0169/0782] - Loss value: 0.20055632293224335\n",
      "Epoch[00/01] | Iteration[0170/0782] - Loss value: 0.14031332731246948\n",
      "Epoch[00/01] | Iteration[0171/0782] - Loss value: 0.1899069994688034\n",
      "Epoch[00/01] | Iteration[0172/0782] - Loss value: 0.2514878213405609\n",
      "Epoch[00/01] | Iteration[0173/0782] - Loss value: 0.35490190982818604\n",
      "Epoch[00/01] | Iteration[0174/0782] - Loss value: 0.3694024980068207\n",
      "Epoch[00/01] | Iteration[0175/0782] - Loss value: 0.2874382436275482\n",
      "Epoch[00/01] | Iteration[0176/0782] - Loss value: 0.3500796854496002\n",
      "Epoch[00/01] | Iteration[0177/0782] - Loss value: 0.30634236335754395\n",
      "Epoch[00/01] | Iteration[0178/0782] - Loss value: 0.13640761375427246\n",
      "Epoch[00/01] | Iteration[0179/0782] - Loss value: 0.4850032925605774\n",
      "Epoch[00/01] | Iteration[0180/0782] - Loss value: 0.2185600847005844\n",
      "Epoch[00/01] | Iteration[0181/0782] - Loss value: 0.2642573118209839\n",
      "Epoch[00/01] | Iteration[0182/0782] - Loss value: 0.39698082208633423\n",
      "Epoch[00/01] | Iteration[0183/0782] - Loss value: 0.3211084306240082\n",
      "Epoch[00/01] | Iteration[0184/0782] - Loss value: 0.24897417426109314\n",
      "Epoch[00/01] | Iteration[0185/0782] - Loss value: 0.2470369189977646\n",
      "Epoch[00/01] | Iteration[0186/0782] - Loss value: 0.31365111470222473\n",
      "Epoch[00/01] | Iteration[0187/0782] - Loss value: 0.3225894868373871\n",
      "Epoch[00/01] | Iteration[0188/0782] - Loss value: 0.2285051792860031\n",
      "Epoch[00/01] | Iteration[0189/0782] - Loss value: 0.4388522505760193\n",
      "Epoch[00/01] | Iteration[0190/0782] - Loss value: 0.29769575595855713\n",
      "Epoch[00/01] | Iteration[0191/0782] - Loss value: 0.12453621625900269\n",
      "Epoch[00/01] | Iteration[0192/0782] - Loss value: 0.17635959386825562\n",
      "Epoch[00/01] | Iteration[0193/0782] - Loss value: 0.31080278754234314\n",
      "Epoch[00/01] | Iteration[0194/0782] - Loss value: 0.5455107092857361\n",
      "Epoch[00/01] | Iteration[0195/0782] - Loss value: 0.4660448431968689\n",
      "Epoch[00/01] | Iteration[0196/0782] - Loss value: 0.16191749274730682\n",
      "Epoch[00/01] | Iteration[0197/0782] - Loss value: 0.35104987025260925\n",
      "Epoch[00/01] | Iteration[0198/0782] - Loss value: 0.23944279551506042\n",
      "Epoch[00/01] | Iteration[0199/0782] - Loss value: 0.278804212808609\n",
      "Epoch[00/01] | Iteration[0200/0782] - Loss value: 0.2088344842195511\n",
      "Epoch[00/01] | Iteration[0201/0782] - Loss value: 0.29963502287864685\n",
      "Epoch[00/01] | Iteration[0202/0782] - Loss value: 0.3277808427810669\n",
      "Epoch[00/01] | Iteration[0203/0782] - Loss value: 0.18597105145454407\n",
      "Epoch[00/01] | Iteration[0204/0782] - Loss value: 0.23455564677715302\n",
      "Epoch[00/01] | Iteration[0205/0782] - Loss value: 0.1998804658651352\n",
      "Epoch[00/01] | Iteration[0206/0782] - Loss value: 0.14953304827213287\n",
      "Epoch[00/01] | Iteration[0207/0782] - Loss value: 0.13119570910930634\n",
      "Epoch[00/01] | Iteration[0208/0782] - Loss value: 0.07790349423885345\n",
      "Epoch[00/01] | Iteration[0209/0782] - Loss value: 0.09071781486272812\n",
      "Epoch[00/01] | Iteration[0210/0782] - Loss value: 0.12134061753749847\n",
      "Epoch[00/01] | Iteration[0211/0782] - Loss value: 0.49557796120643616\n",
      "Epoch[00/01] | Iteration[0212/0782] - Loss value: 0.16992956399917603\n",
      "Epoch[00/01] | Iteration[0213/0782] - Loss value: 0.1067628264427185\n",
      "Epoch[00/01] | Iteration[0214/0782] - Loss value: 0.24890096485614777\n",
      "Epoch[00/01] | Iteration[0215/0782] - Loss value: 0.19735005497932434\n",
      "Epoch[00/01] | Iteration[0216/0782] - Loss value: 0.12119672447443008\n",
      "Epoch[00/01] | Iteration[0217/0782] - Loss value: 0.4341191351413727\n",
      "Epoch[00/01] | Iteration[0218/0782] - Loss value: 0.2894010841846466\n",
      "Epoch[00/01] | Iteration[0219/0782] - Loss value: 0.2857438027858734\n",
      "Epoch[00/01] | Iteration[0220/0782] - Loss value: 0.24098607897758484\n",
      "Epoch[00/01] | Iteration[0221/0782] - Loss value: 0.15939925611019135\n",
      "Epoch[00/01] | Iteration[0222/0782] - Loss value: 0.2396354079246521\n",
      "Epoch[00/01] | Iteration[0223/0782] - Loss value: 0.2868100702762604\n",
      "Epoch[00/01] | Iteration[0224/0782] - Loss value: 0.46545499563217163\n",
      "Epoch[00/01] | Iteration[0225/0782] - Loss value: 0.2615605294704437\n",
      "Epoch[00/01] | Iteration[0226/0782] - Loss value: 0.23569923639297485\n",
      "Epoch[00/01] | Iteration[0227/0782] - Loss value: 0.3882039785385132\n",
      "Epoch[00/01] | Iteration[0228/0782] - Loss value: 0.19764089584350586\n",
      "Epoch[00/01] | Iteration[0229/0782] - Loss value: 0.12572060525417328\n",
      "Epoch[00/01] | Iteration[0230/0782] - Loss value: 0.2296741008758545\n",
      "Epoch[00/01] | Iteration[0231/0782] - Loss value: 0.2186644971370697\n",
      "Epoch[00/01] | Iteration[0232/0782] - Loss value: 0.23475609719753265\n",
      "Epoch[00/01] | Iteration[0233/0782] - Loss value: 0.41064542531967163\n",
      "Epoch[00/01] | Iteration[0234/0782] - Loss value: 0.18861915171146393\n",
      "Epoch[00/01] | Iteration[0235/0782] - Loss value: 0.2875272035598755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[00/01] | Iteration[0236/0782] - Loss value: 0.11650477349758148\n",
      "Epoch[00/01] | Iteration[0237/0782] - Loss value: 0.2843380570411682\n",
      "Epoch[00/01] | Iteration[0238/0782] - Loss value: 0.28081533312797546\n",
      "Epoch[00/01] | Iteration[0239/0782] - Loss value: 0.1747223287820816\n",
      "Epoch[00/01] | Iteration[0240/0782] - Loss value: 0.22505030035972595\n",
      "Epoch[00/01] | Iteration[0241/0782] - Loss value: 0.22762756049633026\n",
      "Epoch[00/01] | Iteration[0242/0782] - Loss value: 0.26447176933288574\n",
      "Epoch[00/01] | Iteration[0243/0782] - Loss value: 0.22902682423591614\n",
      "Epoch[00/01] | Iteration[0244/0782] - Loss value: 0.14210176467895508\n",
      "Epoch[00/01] | Iteration[0245/0782] - Loss value: 0.32431161403656006\n",
      "Epoch[00/01] | Iteration[0246/0782] - Loss value: 0.31392359733581543\n",
      "Epoch[00/01] | Iteration[0247/0782] - Loss value: 0.14450713992118835\n",
      "Epoch[00/01] | Iteration[0248/0782] - Loss value: 0.20232875645160675\n",
      "Epoch[00/01] | Iteration[0249/0782] - Loss value: 0.20008070766925812\n",
      "Epoch[00/01] | Iteration[0250/0782] - Loss value: 0.1836475282907486\n",
      "Epoch[00/01] | Iteration[0251/0782] - Loss value: 0.16045436263084412\n",
      "Epoch[00/01] | Iteration[0252/0782] - Loss value: 0.0504847951233387\n",
      "Epoch[00/01] | Iteration[0253/0782] - Loss value: 0.22935660183429718\n",
      "Epoch[00/01] | Iteration[0254/0782] - Loss value: 0.34067267179489136\n",
      "Epoch[00/01] | Iteration[0255/0782] - Loss value: 0.24241603910923004\n",
      "Epoch[00/01] | Iteration[0256/0782] - Loss value: 0.1894388347864151\n",
      "Epoch[00/01] | Iteration[0257/0782] - Loss value: 0.2971175014972687\n",
      "Epoch[00/01] | Iteration[0258/0782] - Loss value: 0.08176660537719727\n",
      "Epoch[00/01] | Iteration[0259/0782] - Loss value: 0.19489674270153046\n",
      "Epoch[00/01] | Iteration[0260/0782] - Loss value: 0.10040479153394699\n",
      "Epoch[00/01] | Iteration[0261/0782] - Loss value: 0.16027511656284332\n",
      "Epoch[00/01] | Iteration[0262/0782] - Loss value: 0.31243497133255005\n",
      "Epoch[00/01] | Iteration[0263/0782] - Loss value: 0.24302169680595398\n",
      "Epoch[00/01] | Iteration[0264/0782] - Loss value: 0.05083969980478287\n",
      "Epoch[00/01] | Iteration[0265/0782] - Loss value: 0.3215367794036865\n",
      "Epoch[00/01] | Iteration[0266/0782] - Loss value: 0.1661195456981659\n",
      "Epoch[00/01] | Iteration[0267/0782] - Loss value: 0.12687353789806366\n",
      "Epoch[00/01] | Iteration[0268/0782] - Loss value: 0.23107004165649414\n",
      "Epoch[00/01] | Iteration[0269/0782] - Loss value: 0.1581214964389801\n",
      "Epoch[00/01] | Iteration[0270/0782] - Loss value: 0.1704513281583786\n",
      "Epoch[00/01] | Iteration[0271/0782] - Loss value: 0.23581178486347198\n",
      "Epoch[00/01] | Iteration[0272/0782] - Loss value: 0.1214696541428566\n",
      "Epoch[00/01] | Iteration[0273/0782] - Loss value: 0.18579913675785065\n",
      "Epoch[00/01] | Iteration[0274/0782] - Loss value: 0.19407890737056732\n",
      "Epoch[00/01] | Iteration[0275/0782] - Loss value: 0.1897910088300705\n",
      "Epoch[00/01] | Iteration[0276/0782] - Loss value: 0.3836268186569214\n",
      "Epoch[00/01] | Iteration[0277/0782] - Loss value: 0.2030780166387558\n",
      "Epoch[00/01] | Iteration[0278/0782] - Loss value: 0.2977791130542755\n",
      "Epoch[00/01] | Iteration[0279/0782] - Loss value: 0.19959600269794464\n",
      "Epoch[00/01] | Iteration[0280/0782] - Loss value: 0.1150609627366066\n",
      "Epoch[00/01] | Iteration[0281/0782] - Loss value: 0.3636179566383362\n",
      "Epoch[00/01] | Iteration[0282/0782] - Loss value: 0.23400188982486725\n",
      "Epoch[00/01] | Iteration[0283/0782] - Loss value: 0.2704107165336609\n",
      "Epoch[00/01] | Iteration[0284/0782] - Loss value: 0.26020827889442444\n",
      "Epoch[00/01] | Iteration[0285/0782] - Loss value: 0.2996835708618164\n",
      "Epoch[00/01] | Iteration[0286/0782] - Loss value: 0.2028428316116333\n",
      "Epoch[00/01] | Iteration[0287/0782] - Loss value: 0.13652345538139343\n",
      "Epoch[00/01] | Iteration[0288/0782] - Loss value: 0.22677946090698242\n",
      "Epoch[00/01] | Iteration[0289/0782] - Loss value: 0.14182382822036743\n",
      "Epoch[00/01] | Iteration[0290/0782] - Loss value: 0.14277151226997375\n",
      "Epoch[00/01] | Iteration[0291/0782] - Loss value: 0.1179870069026947\n",
      "Epoch[00/01] | Iteration[0292/0782] - Loss value: 0.13034841418266296\n",
      "Epoch[00/01] | Iteration[0293/0782] - Loss value: 0.3657671809196472\n",
      "Epoch[00/01] | Iteration[0294/0782] - Loss value: 0.16707782447338104\n",
      "Epoch[00/01] | Iteration[0295/0782] - Loss value: 0.2877672016620636\n",
      "Epoch[00/01] | Iteration[0296/0782] - Loss value: 0.10690391808748245\n",
      "Epoch[00/01] | Iteration[0297/0782] - Loss value: 0.30353665351867676\n",
      "Epoch[00/01] | Iteration[0298/0782] - Loss value: 0.1518358588218689\n",
      "Epoch[00/01] | Iteration[0299/0782] - Loss value: 0.07460451126098633\n",
      "Epoch[00/01] | Iteration[0300/0782] - Loss value: 0.15944822132587433\n",
      "Epoch[00/01] | Iteration[0301/0782] - Loss value: 0.12666761875152588\n",
      "Epoch[00/01] | Iteration[0302/0782] - Loss value: 0.26847559213638306\n",
      "Epoch[00/01] | Iteration[0303/0782] - Loss value: 0.12196048349142075\n",
      "Epoch[00/01] | Iteration[0304/0782] - Loss value: 0.09993037581443787\n",
      "Epoch[00/01] | Iteration[0305/0782] - Loss value: 0.35147789120674133\n",
      "Epoch[00/01] | Iteration[0306/0782] - Loss value: 0.1000700518488884\n",
      "Epoch[00/01] | Iteration[0307/0782] - Loss value: 0.04933624342083931\n",
      "Epoch[00/01] | Iteration[0308/0782] - Loss value: 0.168503537774086\n",
      "Epoch[00/01] | Iteration[0309/0782] - Loss value: 0.16452322900295258\n",
      "Epoch[00/01] | Iteration[0310/0782] - Loss value: 0.10387592762708664\n",
      "Epoch[00/01] | Iteration[0311/0782] - Loss value: 0.0632113441824913\n",
      "Epoch[00/01] | Iteration[0312/0782] - Loss value: 0.1956038624048233\n",
      "Epoch[00/01] | Iteration[0313/0782] - Loss value: 0.2766267955303192\n",
      "Epoch[00/01] | Iteration[0314/0782] - Loss value: 0.12060721218585968\n",
      "Epoch[00/01] | Iteration[0315/0782] - Loss value: 0.22336897253990173\n",
      "Epoch[00/01] | Iteration[0316/0782] - Loss value: 0.24173606932163239\n",
      "Epoch[00/01] | Iteration[0317/0782] - Loss value: 0.2031382918357849\n",
      "Epoch[00/01] | Iteration[0318/0782] - Loss value: 0.17081837356090546\n",
      "Epoch[00/01] | Iteration[0319/0782] - Loss value: 0.1736859381198883\n",
      "Epoch[00/01] | Iteration[0320/0782] - Loss value: 0.1235736757516861\n",
      "Epoch[00/01] | Iteration[0321/0782] - Loss value: 0.04424137622117996\n",
      "Epoch[00/01] | Iteration[0322/0782] - Loss value: 0.09445962309837341\n",
      "Epoch[00/01] | Iteration[0323/0782] - Loss value: 0.0757947489619255\n",
      "Epoch[00/01] | Iteration[0324/0782] - Loss value: 0.16251850128173828\n",
      "Epoch[00/01] | Iteration[0325/0782] - Loss value: 0.3442784547805786\n",
      "Epoch[00/01] | Iteration[0326/0782] - Loss value: 0.30631619691848755\n",
      "Epoch[00/01] | Iteration[0327/0782] - Loss value: 0.18194925785064697\n",
      "Epoch[00/01] | Iteration[0328/0782] - Loss value: 0.1627012938261032\n",
      "Epoch[00/01] | Iteration[0329/0782] - Loss value: 0.3049434721469879\n",
      "Epoch[00/01] | Iteration[0330/0782] - Loss value: 0.1278407871723175\n",
      "Epoch[00/01] | Iteration[0331/0782] - Loss value: 0.19996024668216705\n",
      "Epoch[00/01] | Iteration[0332/0782] - Loss value: 0.3891077935695648\n",
      "Epoch[00/01] | Iteration[0333/0782] - Loss value: 0.11815513670444489\n",
      "Epoch[00/01] | Iteration[0334/0782] - Loss value: 0.06958048045635223\n",
      "Epoch[00/01] | Iteration[0335/0782] - Loss value: 0.28371888399124146\n",
      "Epoch[00/01] | Iteration[0336/0782] - Loss value: 0.10253726691007614\n",
      "Epoch[00/01] | Iteration[0337/0782] - Loss value: 0.15349180996418\n",
      "Epoch[00/01] | Iteration[0338/0782] - Loss value: 0.22951236367225647\n",
      "Epoch[00/01] | Iteration[0339/0782] - Loss value: 0.2346220314502716\n",
      "Epoch[00/01] | Iteration[0340/0782] - Loss value: 0.23066526651382446\n",
      "Epoch[00/01] | Iteration[0341/0782] - Loss value: 0.11929573118686676\n",
      "Epoch[00/01] | Iteration[0342/0782] - Loss value: 0.1791149228811264\n",
      "Epoch[00/01] | Iteration[0343/0782] - Loss value: 0.10680489242076874\n",
      "Epoch[00/01] | Iteration[0344/0782] - Loss value: 0.16539961099624634\n",
      "Epoch[00/01] | Iteration[0345/0782] - Loss value: 0.14039839804172516\n",
      "Epoch[00/01] | Iteration[0346/0782] - Loss value: 0.236717090010643\n",
      "Epoch[00/01] | Iteration[0347/0782] - Loss value: 0.18645521998405457\n",
      "Epoch[00/01] | Iteration[0348/0782] - Loss value: 0.2455938756465912\n",
      "Epoch[00/01] | Iteration[0349/0782] - Loss value: 0.28000447154045105\n",
      "Epoch[00/01] | Iteration[0350/0782] - Loss value: 0.28764021396636963\n",
      "Epoch[00/01] | Iteration[0351/0782] - Loss value: 0.18918131291866302\n",
      "Epoch[00/01] | Iteration[0352/0782] - Loss value: 0.28935015201568604\n",
      "Epoch[00/01] | Iteration[0353/0782] - Loss value: 0.17768529057502747\n",
      "Epoch[00/01] | Iteration[0354/0782] - Loss value: 0.30282479524612427\n",
      "Epoch[00/01] | Iteration[0355/0782] - Loss value: 0.19310635328292847\n",
      "Epoch[00/01] | Iteration[0356/0782] - Loss value: 0.2263898253440857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[00/01] | Iteration[0357/0782] - Loss value: 0.06338228285312653\n",
      "Epoch[00/01] | Iteration[0358/0782] - Loss value: 0.021962545812129974\n",
      "Epoch[00/01] | Iteration[0359/0782] - Loss value: 0.08791898936033249\n",
      "Epoch[00/01] | Iteration[0360/0782] - Loss value: 0.10718850791454315\n",
      "Epoch[00/01] | Iteration[0361/0782] - Loss value: 0.0732051432132721\n",
      "Epoch[00/01] | Iteration[0362/0782] - Loss value: 0.1387227475643158\n",
      "Epoch[00/01] | Iteration[0363/0782] - Loss value: 0.1124342828989029\n",
      "Epoch[00/01] | Iteration[0364/0782] - Loss value: 0.15622839331626892\n",
      "Epoch[00/01] | Iteration[0365/0782] - Loss value: 0.115693598985672\n",
      "Epoch[00/01] | Iteration[0366/0782] - Loss value: 0.11421507596969604\n",
      "Epoch[00/01] | Iteration[0367/0782] - Loss value: 0.23538362979888916\n",
      "Epoch[00/01] | Iteration[0368/0782] - Loss value: 0.36898577213287354\n",
      "Epoch[00/01] | Iteration[0369/0782] - Loss value: 0.03460756316781044\n",
      "Epoch[00/01] | Iteration[0370/0782] - Loss value: 0.10948144644498825\n",
      "Epoch[00/01] | Iteration[0371/0782] - Loss value: 0.07773347198963165\n",
      "Epoch[00/01] | Iteration[0372/0782] - Loss value: 0.1461206078529358\n",
      "Epoch[00/01] | Iteration[0373/0782] - Loss value: 0.21183274686336517\n",
      "Epoch[00/01] | Iteration[0374/0782] - Loss value: 0.20265403389930725\n",
      "Epoch[00/01] | Iteration[0375/0782] - Loss value: 0.08843530714511871\n",
      "Epoch[00/01] | Iteration[0376/0782] - Loss value: 0.12379070371389389\n",
      "Epoch[00/01] | Iteration[0377/0782] - Loss value: 0.15355612337589264\n",
      "Epoch[00/01] | Iteration[0378/0782] - Loss value: 0.21345555782318115\n",
      "Epoch[00/01] | Iteration[0379/0782] - Loss value: 0.26535293459892273\n",
      "Epoch[00/01] | Iteration[0380/0782] - Loss value: 0.2002505362033844\n",
      "Epoch[00/01] | Iteration[0381/0782] - Loss value: 0.10322754085063934\n",
      "Epoch[00/01] | Iteration[0382/0782] - Loss value: 0.06741944700479507\n",
      "Epoch[00/01] | Iteration[0383/0782] - Loss value: 0.15753929316997528\n",
      "Epoch[00/01] | Iteration[0384/0782] - Loss value: 0.1447073370218277\n",
      "Epoch[00/01] | Iteration[0385/0782] - Loss value: 0.10182522982358932\n",
      "Epoch[00/01] | Iteration[0386/0782] - Loss value: 0.2505802810192108\n",
      "Epoch[00/01] | Iteration[0387/0782] - Loss value: 0.15156200528144836\n",
      "Epoch[00/01] | Iteration[0388/0782] - Loss value: 0.24999597668647766\n",
      "Epoch[00/01] | Iteration[0389/0782] - Loss value: 0.10937467962503433\n",
      "Epoch[00/01] | Iteration[0390/0782] - Loss value: 0.25629398226737976\n",
      "Epoch[00/01] | Iteration[0391/0782] - Loss value: 0.14991703629493713\n",
      "Epoch[00/01] | Iteration[0392/0782] - Loss value: 0.11398687213659286\n",
      "Epoch[00/01] | Iteration[0393/0782] - Loss value: 0.09674042463302612\n",
      "Epoch[00/01] | Iteration[0394/0782] - Loss value: 0.24595895409584045\n",
      "Epoch[00/01] | Iteration[0395/0782] - Loss value: 0.02115347608923912\n",
      "Epoch[00/01] | Iteration[0396/0782] - Loss value: 0.06583001464605331\n",
      "Epoch[00/01] | Iteration[0397/0782] - Loss value: 0.1408187448978424\n",
      "Epoch[00/01] | Iteration[0398/0782] - Loss value: 0.19155506789684296\n",
      "Epoch[00/01] | Iteration[0399/0782] - Loss value: 0.049903206527233124\n",
      "Epoch[00/01] | Iteration[0400/0782] - Loss value: 0.09075211733579636\n",
      "Epoch[00/01] | Iteration[0401/0782] - Loss value: 0.23486794531345367\n",
      "Epoch[00/01] | Iteration[0402/0782] - Loss value: 0.24428677558898926\n",
      "Epoch[00/01] | Iteration[0403/0782] - Loss value: 0.08405707031488419\n",
      "Epoch[00/01] | Iteration[0404/0782] - Loss value: 0.3769568204879761\n",
      "Epoch[00/01] | Iteration[0405/0782] - Loss value: 0.2618517279624939\n",
      "Epoch[00/01] | Iteration[0406/0782] - Loss value: 0.12240401655435562\n",
      "Epoch[00/01] | Iteration[0407/0782] - Loss value: 0.16522477567195892\n",
      "Epoch[00/01] | Iteration[0408/0782] - Loss value: 0.17315177619457245\n",
      "Epoch[00/01] | Iteration[0409/0782] - Loss value: 0.09353713691234589\n",
      "Epoch[00/01] | Iteration[0410/0782] - Loss value: 0.07564803957939148\n",
      "Epoch[00/01] | Iteration[0411/0782] - Loss value: 0.09274929761886597\n",
      "Epoch[00/01] | Iteration[0412/0782] - Loss value: 0.12772606313228607\n",
      "Epoch[00/01] | Iteration[0413/0782] - Loss value: 0.13706143200397491\n",
      "Epoch[00/01] | Iteration[0414/0782] - Loss value: 0.1404300034046173\n",
      "Epoch[00/01] | Iteration[0415/0782] - Loss value: 0.06952526420354843\n",
      "Epoch[00/01] | Iteration[0416/0782] - Loss value: 0.13907252252101898\n",
      "Epoch[00/01] | Iteration[0417/0782] - Loss value: 0.05650763586163521\n",
      "Epoch[00/01] | Iteration[0418/0782] - Loss value: 0.05726267397403717\n",
      "Epoch[00/01] | Iteration[0419/0782] - Loss value: 0.06705138087272644\n",
      "Epoch[00/01] | Iteration[0420/0782] - Loss value: 0.10164348781108856\n",
      "Epoch[00/01] | Iteration[0421/0782] - Loss value: 0.1493792086839676\n",
      "Epoch[00/01] | Iteration[0422/0782] - Loss value: 0.28873318433761597\n",
      "Epoch[00/01] | Iteration[0423/0782] - Loss value: 0.3707180321216583\n",
      "Epoch[00/01] | Iteration[0424/0782] - Loss value: 0.14563962817192078\n",
      "Epoch[00/01] | Iteration[0425/0782] - Loss value: 0.06691115349531174\n",
      "Epoch[00/01] | Iteration[0426/0782] - Loss value: 0.06573673337697983\n",
      "Epoch[00/01] | Iteration[0427/0782] - Loss value: 0.0509822852909565\n",
      "Epoch[00/01] | Iteration[0428/0782] - Loss value: 0.17271053791046143\n",
      "Epoch[00/01] | Iteration[0429/0782] - Loss value: 0.09622024744749069\n",
      "Epoch[00/01] | Iteration[0430/0782] - Loss value: 0.15684498846530914\n",
      "Epoch[00/01] | Iteration[0431/0782] - Loss value: 0.05349729582667351\n",
      "Epoch[00/01] | Iteration[0432/0782] - Loss value: 0.21706920862197876\n",
      "Epoch[00/01] | Iteration[0433/0782] - Loss value: 0.05407016724348068\n",
      "Epoch[00/01] | Iteration[0434/0782] - Loss value: 0.15319834649562836\n",
      "Epoch[00/01] | Iteration[0435/0782] - Loss value: 0.16880108416080475\n",
      "Epoch[00/01] | Iteration[0436/0782] - Loss value: 0.15523661673069\n",
      "Epoch[00/01] | Iteration[0437/0782] - Loss value: 0.05033151060342789\n",
      "Epoch[00/01] | Iteration[0438/0782] - Loss value: 0.35098764300346375\n",
      "Epoch[00/01] | Iteration[0439/0782] - Loss value: 0.12128086388111115\n",
      "Epoch[00/01] | Iteration[0440/0782] - Loss value: 0.0922795906662941\n",
      "Epoch[00/01] | Iteration[0441/0782] - Loss value: 0.13927040994167328\n",
      "Epoch[00/01] | Iteration[0442/0782] - Loss value: 0.0576363168656826\n",
      "Epoch[00/01] | Iteration[0443/0782] - Loss value: 0.05735207349061966\n",
      "Epoch[00/01] | Iteration[0444/0782] - Loss value: 0.38530123233795166\n",
      "Epoch[00/01] | Iteration[0445/0782] - Loss value: 0.19228820502758026\n",
      "Epoch[00/01] | Iteration[0446/0782] - Loss value: 0.11559227854013443\n",
      "Epoch[00/01] | Iteration[0447/0782] - Loss value: 0.1805909126996994\n",
      "Epoch[00/01] | Iteration[0448/0782] - Loss value: 0.02731783501803875\n",
      "Epoch[00/01] | Iteration[0449/0782] - Loss value: 0.04794631525874138\n",
      "Epoch[00/01] | Iteration[0450/0782] - Loss value: 0.1219135969877243\n",
      "Epoch[00/01] | Iteration[0451/0782] - Loss value: 0.07129240781068802\n",
      "Epoch[00/01] | Iteration[0452/0782] - Loss value: 0.1057974174618721\n",
      "Epoch[00/01] | Iteration[0453/0782] - Loss value: 0.06441757827997208\n",
      "Epoch[00/01] | Iteration[0454/0782] - Loss value: 0.24300624430179596\n",
      "Epoch[00/01] | Iteration[0455/0782] - Loss value: 0.18185222148895264\n",
      "Epoch[00/01] | Iteration[0456/0782] - Loss value: 0.08452332019805908\n",
      "Epoch[00/01] | Iteration[0457/0782] - Loss value: 0.15904918313026428\n",
      "Epoch[00/01] | Iteration[0458/0782] - Loss value: 0.2372330129146576\n",
      "Epoch[00/01] | Iteration[0459/0782] - Loss value: 0.10227993875741959\n",
      "Epoch[00/01] | Iteration[0460/0782] - Loss value: 0.17103685438632965\n",
      "Epoch[00/01] | Iteration[0461/0782] - Loss value: 0.10491545498371124\n",
      "Epoch[00/01] | Iteration[0462/0782] - Loss value: 0.23124320805072784\n",
      "Epoch[00/01] | Iteration[0463/0782] - Loss value: 0.3043600916862488\n",
      "Epoch[00/01] | Iteration[0464/0782] - Loss value: 0.04269488900899887\n",
      "Epoch[00/01] | Iteration[0465/0782] - Loss value: 0.05473450943827629\n",
      "Epoch[00/01] | Iteration[0466/0782] - Loss value: 0.09738411009311676\n",
      "Epoch[00/01] | Iteration[0467/0782] - Loss value: 0.1476418524980545\n",
      "Epoch[00/01] | Iteration[0468/0782] - Loss value: 0.3447955250740051\n",
      "Epoch[00/01] | Iteration[0469/0782] - Loss value: 0.14680097997188568\n",
      "Epoch[00/01] | Iteration[0470/0782] - Loss value: 0.1347145289182663\n",
      "Epoch[00/01] | Iteration[0471/0782] - Loss value: 0.07758192718029022\n",
      "Epoch[00/01] | Iteration[0472/0782] - Loss value: 0.13748684525489807\n",
      "Epoch[00/01] | Iteration[0473/0782] - Loss value: 0.1310185045003891\n",
      "Epoch[00/01] | Iteration[0474/0782] - Loss value: 0.21643196046352386\n",
      "Epoch[00/01] | Iteration[0475/0782] - Loss value: 0.1771979033946991\n",
      "Epoch[00/01] | Iteration[0476/0782] - Loss value: 0.0716167464852333\n",
      "Epoch[00/01] | Iteration[0477/0782] - Loss value: 0.0838998556137085\n",
      "Epoch[00/01] | Iteration[0478/0782] - Loss value: 0.15981079638004303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[00/01] | Iteration[0479/0782] - Loss value: 0.3632441759109497\n",
      "Epoch[00/01] | Iteration[0480/0782] - Loss value: 0.04249963164329529\n",
      "Epoch[00/01] | Iteration[0481/0782] - Loss value: 0.1690824031829834\n",
      "Epoch[00/01] | Iteration[0482/0782] - Loss value: 0.10366643220186234\n",
      "Epoch[00/01] | Iteration[0483/0782] - Loss value: 0.1370903104543686\n",
      "Epoch[00/01] | Iteration[0484/0782] - Loss value: 0.08903853595256805\n",
      "Epoch[00/01] | Iteration[0485/0782] - Loss value: 0.08948647230863571\n",
      "Epoch[00/01] | Iteration[0486/0782] - Loss value: 0.18447211384773254\n",
      "Epoch[00/01] | Iteration[0487/0782] - Loss value: 0.1906207650899887\n",
      "Epoch[00/01] | Iteration[0488/0782] - Loss value: 0.05942648649215698\n",
      "Epoch[00/01] | Iteration[0489/0782] - Loss value: 0.1748296320438385\n",
      "Epoch[00/01] | Iteration[0490/0782] - Loss value: 0.08412200212478638\n",
      "Epoch[00/01] | Iteration[0491/0782] - Loss value: 0.09991000592708588\n",
      "Epoch[00/01] | Iteration[0492/0782] - Loss value: 0.1409684419631958\n",
      "Epoch[00/01] | Iteration[0493/0782] - Loss value: 0.17749592661857605\n",
      "Epoch[00/01] | Iteration[0494/0782] - Loss value: 0.14010249078273773\n",
      "Epoch[00/01] | Iteration[0495/0782] - Loss value: 0.1841026246547699\n",
      "Epoch[00/01] | Iteration[0496/0782] - Loss value: 0.19120877981185913\n",
      "Epoch[00/01] | Iteration[0497/0782] - Loss value: 0.10005198419094086\n",
      "Epoch[00/01] | Iteration[0498/0782] - Loss value: 0.04370312765240669\n",
      "Epoch[00/01] | Iteration[0499/0782] - Loss value: 0.09987036138772964\n",
      "Epoch[00/01] | Iteration[0500/0782] - Loss value: 0.21496444940567017\n",
      "Epoch[00/01] | Iteration[0501/0782] - Loss value: 0.16754145920276642\n",
      "Epoch[00/01] | Iteration[0502/0782] - Loss value: 0.09749197214841843\n",
      "Epoch[00/01] | Iteration[0503/0782] - Loss value: 0.36925435066223145\n",
      "Epoch[00/01] | Iteration[0504/0782] - Loss value: 0.11506789177656174\n",
      "Epoch[00/01] | Iteration[0505/0782] - Loss value: 0.05196718871593475\n",
      "Epoch[00/01] | Iteration[0506/0782] - Loss value: 0.05020124465227127\n",
      "Epoch[00/01] | Iteration[0507/0782] - Loss value: 0.08728339523077011\n",
      "Epoch[00/01] | Iteration[0508/0782] - Loss value: 0.12906670570373535\n",
      "Epoch[00/01] | Iteration[0509/0782] - Loss value: 0.056774698197841644\n",
      "Epoch[00/01] | Iteration[0510/0782] - Loss value: 0.0902036502957344\n",
      "Epoch[00/01] | Iteration[0511/0782] - Loss value: 0.25496265292167664\n",
      "Epoch[00/01] | Iteration[0512/0782] - Loss value: 0.0421154610812664\n",
      "Epoch[00/01] | Iteration[0513/0782] - Loss value: 0.19943749904632568\n",
      "Epoch[00/01] | Iteration[0514/0782] - Loss value: 0.528497576713562\n",
      "Epoch[00/01] | Iteration[0515/0782] - Loss value: 0.06097807362675667\n",
      "Epoch[00/01] | Iteration[0516/0782] - Loss value: 0.03722084313631058\n",
      "Epoch[00/01] | Iteration[0517/0782] - Loss value: 0.20737914741039276\n",
      "Epoch[00/01] | Iteration[0518/0782] - Loss value: 0.16236907243728638\n",
      "Epoch[00/01] | Iteration[0519/0782] - Loss value: 0.09611790627241135\n",
      "Epoch[00/01] | Iteration[0520/0782] - Loss value: 0.269851952791214\n",
      "Epoch[00/01] | Iteration[0521/0782] - Loss value: 0.038641322404146194\n",
      "Epoch[00/01] | Iteration[0522/0782] - Loss value: 0.10924699902534485\n",
      "Epoch[00/01] | Iteration[0523/0782] - Loss value: 0.2748899459838867\n",
      "Epoch[00/01] | Iteration[0524/0782] - Loss value: 0.03854530304670334\n",
      "Epoch[00/01] | Iteration[0525/0782] - Loss value: 0.08089926838874817\n",
      "Epoch[00/01] | Iteration[0526/0782] - Loss value: 0.1450379639863968\n",
      "Epoch[00/01] | Iteration[0527/0782] - Loss value: 0.1151762306690216\n",
      "Epoch[00/01] | Iteration[0528/0782] - Loss value: 0.027251098304986954\n",
      "Epoch[00/01] | Iteration[0529/0782] - Loss value: 0.17628774046897888\n",
      "Epoch[00/01] | Iteration[0530/0782] - Loss value: 0.17765171825885773\n",
      "Epoch[00/01] | Iteration[0531/0782] - Loss value: 0.10797657817602158\n",
      "Epoch[00/01] | Iteration[0532/0782] - Loss value: 0.15768849849700928\n",
      "Epoch[00/01] | Iteration[0533/0782] - Loss value: 0.1450059413909912\n",
      "Epoch[00/01] | Iteration[0534/0782] - Loss value: 0.16878975927829742\n",
      "Epoch[00/01] | Iteration[0535/0782] - Loss value: 0.1966295838356018\n",
      "Epoch[00/01] | Iteration[0536/0782] - Loss value: 0.026946576312184334\n",
      "Epoch[00/01] | Iteration[0537/0782] - Loss value: 0.18782322108745575\n",
      "Epoch[00/01] | Iteration[0538/0782] - Loss value: 0.16901935636997223\n",
      "Epoch[00/01] | Iteration[0539/0782] - Loss value: 0.31108853220939636\n",
      "Epoch[00/01] | Iteration[0540/0782] - Loss value: 0.028751807287335396\n",
      "Epoch[00/01] | Iteration[0541/0782] - Loss value: 0.12792177498340607\n",
      "Epoch[00/01] | Iteration[0542/0782] - Loss value: 0.11894425004720688\n",
      "Epoch[00/01] | Iteration[0543/0782] - Loss value: 0.17491969466209412\n",
      "Epoch[00/01] | Iteration[0544/0782] - Loss value: 0.1445537805557251\n",
      "Epoch[00/01] | Iteration[0545/0782] - Loss value: 0.11959695816040039\n",
      "Epoch[00/01] | Iteration[0546/0782] - Loss value: 0.09540047496557236\n",
      "Epoch[00/01] | Iteration[0547/0782] - Loss value: 0.0373072512447834\n",
      "Epoch[00/01] | Iteration[0548/0782] - Loss value: 0.06135978177189827\n",
      "Epoch[00/01] | Iteration[0549/0782] - Loss value: 0.11936289072036743\n",
      "Epoch[00/01] | Iteration[0550/0782] - Loss value: 0.16014616191387177\n",
      "Epoch[00/01] | Iteration[0551/0782] - Loss value: 0.26940083503723145\n",
      "Epoch[00/01] | Iteration[0552/0782] - Loss value: 0.12192612141370773\n",
      "Epoch[00/01] | Iteration[0553/0782] - Loss value: 0.11085925251245499\n",
      "Epoch[00/01] | Iteration[0554/0782] - Loss value: 0.14638155698776245\n",
      "Epoch[00/01] | Iteration[0555/0782] - Loss value: 0.04576151818037033\n",
      "Epoch[00/01] | Iteration[0556/0782] - Loss value: 0.2273988425731659\n",
      "Epoch[00/01] | Iteration[0557/0782] - Loss value: 0.11622212827205658\n",
      "Epoch[00/01] | Iteration[0558/0782] - Loss value: 0.14397397637367249\n",
      "Epoch[00/01] | Iteration[0559/0782] - Loss value: 0.0978449210524559\n",
      "Epoch[00/01] | Iteration[0560/0782] - Loss value: 0.2755584418773651\n",
      "Epoch[00/01] | Iteration[0561/0782] - Loss value: 0.12338629364967346\n",
      "Epoch[00/01] | Iteration[0562/0782] - Loss value: 0.13660669326782227\n",
      "Epoch[00/01] | Iteration[0563/0782] - Loss value: 0.10782994329929352\n",
      "Epoch[00/01] | Iteration[0564/0782] - Loss value: 0.06291310489177704\n",
      "Epoch[00/01] | Iteration[0565/0782] - Loss value: 0.05085071921348572\n",
      "Epoch[00/01] | Iteration[0566/0782] - Loss value: 0.10096538066864014\n",
      "Epoch[00/01] | Iteration[0567/0782] - Loss value: 0.19959330558776855\n",
      "Epoch[00/01] | Iteration[0568/0782] - Loss value: 0.0683780163526535\n",
      "Epoch[00/01] | Iteration[0569/0782] - Loss value: 0.09022439271211624\n",
      "Epoch[00/01] | Iteration[0570/0782] - Loss value: 0.06814898550510406\n",
      "Epoch[00/01] | Iteration[0571/0782] - Loss value: 0.13246437907218933\n",
      "Epoch[00/01] | Iteration[0572/0782] - Loss value: 0.24214477837085724\n",
      "Epoch[00/01] | Iteration[0573/0782] - Loss value: 0.027835210785269737\n",
      "Epoch[00/01] | Iteration[0574/0782] - Loss value: 0.07358501106500626\n",
      "Epoch[00/01] | Iteration[0575/0782] - Loss value: 0.07465104758739471\n",
      "Epoch[00/01] | Iteration[0576/0782] - Loss value: 0.2541274428367615\n",
      "Epoch[00/01] | Iteration[0577/0782] - Loss value: 0.09764739125967026\n",
      "Epoch[00/01] | Iteration[0578/0782] - Loss value: 0.08421539515256882\n",
      "Epoch[00/01] | Iteration[0579/0782] - Loss value: 0.11735900491476059\n",
      "Epoch[00/01] | Iteration[0580/0782] - Loss value: 0.1382737010717392\n",
      "Epoch[00/01] | Iteration[0581/0782] - Loss value: 0.09845557063817978\n",
      "Epoch[00/01] | Iteration[0582/0782] - Loss value: 0.14515575766563416\n",
      "Epoch[00/01] | Iteration[0583/0782] - Loss value: 0.20289495587348938\n",
      "Epoch[00/01] | Iteration[0584/0782] - Loss value: 0.13823655247688293\n",
      "Epoch[00/01] | Iteration[0585/0782] - Loss value: 0.06667925417423248\n",
      "Epoch[00/01] | Iteration[0586/0782] - Loss value: 0.11287958920001984\n",
      "Epoch[00/01] | Iteration[0587/0782] - Loss value: 0.12715500593185425\n",
      "Epoch[00/01] | Iteration[0588/0782] - Loss value: 0.1905546337366104\n",
      "Epoch[00/01] | Iteration[0589/0782] - Loss value: 0.1582038253545761\n",
      "Epoch[00/01] | Iteration[0590/0782] - Loss value: 0.0940035805106163\n",
      "Epoch[00/01] | Iteration[0591/0782] - Loss value: 0.03404303267598152\n",
      "Epoch[00/01] | Iteration[0592/0782] - Loss value: 0.2007119208574295\n",
      "Epoch[00/01] | Iteration[0593/0782] - Loss value: 0.2727009057998657\n",
      "Epoch[00/01] | Iteration[0594/0782] - Loss value: 0.13194116950035095\n",
      "Epoch[00/01] | Iteration[0595/0782] - Loss value: 0.11454705148935318\n",
      "Epoch[00/01] | Iteration[0596/0782] - Loss value: 0.1312752515077591\n",
      "Epoch[00/01] | Iteration[0597/0782] - Loss value: 0.2196868509054184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[00/01] | Iteration[0598/0782] - Loss value: 0.12112788110971451\n",
      "Epoch[00/01] | Iteration[0599/0782] - Loss value: 0.033371616154909134\n",
      "Epoch[00/01] | Iteration[0600/0782] - Loss value: 0.09874971210956573\n",
      "Epoch[00/01] | Iteration[0601/0782] - Loss value: 0.08781082183122635\n",
      "Epoch[00/01] | Iteration[0602/0782] - Loss value: 0.01502265501767397\n",
      "Epoch[00/01] | Iteration[0603/0782] - Loss value: 0.13266238570213318\n",
      "Epoch[00/01] | Iteration[0604/0782] - Loss value: 0.12909895181655884\n",
      "Epoch[00/01] | Iteration[0605/0782] - Loss value: 0.037025220692157745\n",
      "Epoch[00/01] | Iteration[0606/0782] - Loss value: 0.0727071762084961\n",
      "Epoch[00/01] | Iteration[0607/0782] - Loss value: 0.055222176015377045\n",
      "Epoch[00/01] | Iteration[0608/0782] - Loss value: 0.1494063138961792\n",
      "Epoch[00/01] | Iteration[0609/0782] - Loss value: 0.16140711307525635\n",
      "Epoch[00/01] | Iteration[0610/0782] - Loss value: 0.0743255540728569\n",
      "Epoch[00/01] | Iteration[0611/0782] - Loss value: 0.036221105605363846\n",
      "Epoch[00/01] | Iteration[0612/0782] - Loss value: 0.038352664560079575\n",
      "Epoch[00/01] | Iteration[0613/0782] - Loss value: 0.04025828838348389\n",
      "Epoch[00/01] | Iteration[0614/0782] - Loss value: 0.07714296877384186\n",
      "Epoch[00/01] | Iteration[0615/0782] - Loss value: 0.21177148818969727\n",
      "Epoch[00/01] | Iteration[0616/0782] - Loss value: 0.039905473589897156\n",
      "Epoch[00/01] | Iteration[0617/0782] - Loss value: 0.07552903890609741\n",
      "Epoch[00/01] | Iteration[0618/0782] - Loss value: 0.10847703367471695\n",
      "Epoch[00/01] | Iteration[0619/0782] - Loss value: 0.06330881267786026\n",
      "Epoch[00/01] | Iteration[0620/0782] - Loss value: 0.08636558800935745\n",
      "Epoch[00/01] | Iteration[0621/0782] - Loss value: 0.11961857974529266\n",
      "Epoch[00/01] | Iteration[0622/0782] - Loss value: 0.21004390716552734\n",
      "Epoch[00/01] | Iteration[0623/0782] - Loss value: 0.1066000908613205\n",
      "Epoch[00/01] | Iteration[0624/0782] - Loss value: 0.08196351677179337\n",
      "Epoch[00/01] | Iteration[0625/0782] - Loss value: 0.1105131283402443\n",
      "Epoch[00/01] | Iteration[0626/0782] - Loss value: 0.10447550565004349\n",
      "Epoch[00/01] | Iteration[0627/0782] - Loss value: 0.057201970368623734\n",
      "Epoch[00/01] | Iteration[0628/0782] - Loss value: 0.2412474900484085\n",
      "Epoch[00/01] | Iteration[0629/0782] - Loss value: 0.11777883768081665\n",
      "Epoch[00/01] | Iteration[0630/0782] - Loss value: 0.1545661985874176\n",
      "Epoch[00/01] | Iteration[0631/0782] - Loss value: 0.05706619471311569\n",
      "Epoch[00/01] | Iteration[0632/0782] - Loss value: 0.040097542107105255\n",
      "Epoch[00/01] | Iteration[0633/0782] - Loss value: 0.13141009211540222\n",
      "Epoch[00/01] | Iteration[0634/0782] - Loss value: 0.09006128460168839\n",
      "Epoch[00/01] | Iteration[0635/0782] - Loss value: 0.11868059635162354\n",
      "Epoch[00/01] | Iteration[0636/0782] - Loss value: 0.09616473317146301\n",
      "Epoch[00/01] | Iteration[0637/0782] - Loss value: 0.03507249802350998\n",
      "Epoch[00/01] | Iteration[0638/0782] - Loss value: 0.13480940461158752\n",
      "Epoch[00/01] | Iteration[0639/0782] - Loss value: 0.041296642273664474\n",
      "Epoch[00/01] | Iteration[0640/0782] - Loss value: 0.12314537912607193\n",
      "Epoch[00/01] | Iteration[0641/0782] - Loss value: 0.05276380851864815\n",
      "Epoch[00/01] | Iteration[0642/0782] - Loss value: 0.19346317648887634\n",
      "Epoch[00/01] | Iteration[0643/0782] - Loss value: 0.053834155201911926\n",
      "Epoch[00/01] | Iteration[0644/0782] - Loss value: 0.20548631250858307\n",
      "Epoch[00/01] | Iteration[0645/0782] - Loss value: 0.19316431879997253\n",
      "Epoch[00/01] | Iteration[0646/0782] - Loss value: 0.04913408309221268\n",
      "Epoch[00/01] | Iteration[0647/0782] - Loss value: 0.12222453206777573\n",
      "Epoch[00/01] | Iteration[0648/0782] - Loss value: 0.21124544739723206\n",
      "Epoch[00/01] | Iteration[0649/0782] - Loss value: 0.10762090235948563\n",
      "Epoch[00/01] | Iteration[0650/0782] - Loss value: 0.01986113004386425\n",
      "Epoch[00/01] | Iteration[0651/0782] - Loss value: 0.048664312809705734\n",
      "Epoch[00/01] | Iteration[0652/0782] - Loss value: 0.09271103143692017\n",
      "Epoch[00/01] | Iteration[0653/0782] - Loss value: 0.10761259496212006\n",
      "Epoch[00/01] | Iteration[0654/0782] - Loss value: 0.23851390182971954\n",
      "Epoch[00/01] | Iteration[0655/0782] - Loss value: 0.13800162076950073\n",
      "Epoch[00/01] | Iteration[0656/0782] - Loss value: 0.3426118493080139\n",
      "Epoch[00/01] | Iteration[0657/0782] - Loss value: 0.17645564675331116\n",
      "Epoch[00/01] | Iteration[0658/0782] - Loss value: 0.1997557431459427\n",
      "Epoch[00/01] | Iteration[0659/0782] - Loss value: 0.10950472205877304\n",
      "Epoch[00/01] | Iteration[0660/0782] - Loss value: 0.11313781142234802\n",
      "Epoch[00/01] | Iteration[0661/0782] - Loss value: 0.06956097483634949\n",
      "Epoch[00/01] | Iteration[0662/0782] - Loss value: 0.09930147230625153\n",
      "Epoch[00/01] | Iteration[0663/0782] - Loss value: 0.0419028103351593\n",
      "Epoch[00/01] | Iteration[0664/0782] - Loss value: 0.08060619235038757\n",
      "Epoch[00/01] | Iteration[0665/0782] - Loss value: 0.037997063249349594\n",
      "Epoch[00/01] | Iteration[0666/0782] - Loss value: 0.07640934735536575\n",
      "Epoch[00/01] | Iteration[0667/0782] - Loss value: 0.18541845679283142\n",
      "Epoch[00/01] | Iteration[0668/0782] - Loss value: 0.08727722615003586\n",
      "Epoch[00/01] | Iteration[0669/0782] - Loss value: 0.0622507780790329\n",
      "Epoch[00/01] | Iteration[0670/0782] - Loss value: 0.11385469138622284\n",
      "Epoch[00/01] | Iteration[0671/0782] - Loss value: 0.1088494062423706\n",
      "Epoch[00/01] | Iteration[0672/0782] - Loss value: 0.13811369240283966\n",
      "Epoch[00/01] | Iteration[0673/0782] - Loss value: 0.11903399229049683\n",
      "Epoch[00/01] | Iteration[0674/0782] - Loss value: 0.18822821974754333\n",
      "Epoch[00/01] | Iteration[0675/0782] - Loss value: 0.15485680103302002\n",
      "Epoch[00/01] | Iteration[0676/0782] - Loss value: 0.05855465680360794\n",
      "Epoch[00/01] | Iteration[0677/0782] - Loss value: 0.04401463270187378\n",
      "Epoch[00/01] | Iteration[0678/0782] - Loss value: 0.06328004598617554\n",
      "Epoch[00/01] | Iteration[0679/0782] - Loss value: 0.15606309473514557\n",
      "Epoch[00/01] | Iteration[0680/0782] - Loss value: 0.04146040603518486\n",
      "Epoch[00/01] | Iteration[0681/0782] - Loss value: 0.09558981657028198\n",
      "Epoch[00/01] | Iteration[0682/0782] - Loss value: 0.08778750896453857\n",
      "Epoch[00/01] | Iteration[0683/0782] - Loss value: 0.11145971715450287\n",
      "Epoch[00/01] | Iteration[0684/0782] - Loss value: 0.08756770938634872\n",
      "Epoch[00/01] | Iteration[0685/0782] - Loss value: 0.12434292584657669\n",
      "Epoch[00/01] | Iteration[0686/0782] - Loss value: 0.125539168715477\n",
      "Epoch[00/01] | Iteration[0687/0782] - Loss value: 0.1228259950876236\n",
      "Epoch[00/01] | Iteration[0688/0782] - Loss value: 0.04386141523718834\n",
      "Epoch[00/01] | Iteration[0689/0782] - Loss value: 0.035450153052806854\n",
      "Epoch[00/01] | Iteration[0690/0782] - Loss value: 0.17655745148658752\n",
      "Epoch[00/01] | Iteration[0691/0782] - Loss value: 0.09672659635543823\n",
      "Epoch[00/01] | Iteration[0692/0782] - Loss value: 0.02875184267759323\n",
      "Epoch[00/01] | Iteration[0693/0782] - Loss value: 0.116884745657444\n",
      "Epoch[00/01] | Iteration[0694/0782] - Loss value: 0.0790550708770752\n",
      "Epoch[00/01] | Iteration[0695/0782] - Loss value: 0.15911264717578888\n",
      "Epoch[00/01] | Iteration[0696/0782] - Loss value: 0.25061362981796265\n",
      "Epoch[00/01] | Iteration[0697/0782] - Loss value: 0.024965329095721245\n",
      "Epoch[00/01] | Iteration[0698/0782] - Loss value: 0.05409358814358711\n",
      "Epoch[00/01] | Iteration[0699/0782] - Loss value: 0.05293740704655647\n",
      "Epoch[00/01] | Iteration[0700/0782] - Loss value: 0.2524379789829254\n",
      "Epoch[00/01] | Iteration[0701/0782] - Loss value: 0.13108058273792267\n",
      "Epoch[00/01] | Iteration[0702/0782] - Loss value: 0.0772286206483841\n",
      "Epoch[00/01] | Iteration[0703/0782] - Loss value: 0.2388674020767212\n",
      "Epoch[00/01] | Iteration[0704/0782] - Loss value: 0.1859937310218811\n",
      "Epoch[00/01] | Iteration[0705/0782] - Loss value: 0.1489984095096588\n",
      "Epoch[00/01] | Iteration[0706/0782] - Loss value: 0.16682258248329163\n",
      "Epoch[00/01] | Iteration[0707/0782] - Loss value: 0.3229762315750122\n",
      "Epoch[00/01] | Iteration[0708/0782] - Loss value: 0.03317345678806305\n",
      "Epoch[00/01] | Iteration[0709/0782] - Loss value: 0.06263188272714615\n",
      "Epoch[00/01] | Iteration[0710/0782] - Loss value: 0.053091082721948624\n",
      "Epoch[00/01] | Iteration[0711/0782] - Loss value: 0.13219855725765228\n",
      "Epoch[00/01] | Iteration[0712/0782] - Loss value: 0.18071727454662323\n",
      "Epoch[00/01] | Iteration[0713/0782] - Loss value: 0.06372172385454178\n",
      "Epoch[00/01] | Iteration[0714/0782] - Loss value: 0.3324926197528839\n",
      "Epoch[00/01] | Iteration[0715/0782] - Loss value: 0.0632210522890091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[00/01] | Iteration[0716/0782] - Loss value: 0.11861443519592285\n",
      "Epoch[00/01] | Iteration[0717/0782] - Loss value: 0.09619879722595215\n",
      "Epoch[00/01] | Iteration[0718/0782] - Loss value: 0.13540315628051758\n",
      "Epoch[00/01] | Iteration[0719/0782] - Loss value: 0.04214056581258774\n",
      "Epoch[00/01] | Iteration[0720/0782] - Loss value: 0.23444052040576935\n",
      "Epoch[00/01] | Iteration[0721/0782] - Loss value: 0.17221778631210327\n",
      "Epoch[00/01] | Iteration[0722/0782] - Loss value: 0.113728828728199\n",
      "Epoch[00/01] | Iteration[0723/0782] - Loss value: 0.13276627659797668\n",
      "Epoch[00/01] | Iteration[0724/0782] - Loss value: 0.14869478344917297\n",
      "Epoch[00/01] | Iteration[0725/0782] - Loss value: 0.1278013437986374\n",
      "Epoch[00/01] | Iteration[0726/0782] - Loss value: 0.04918580502271652\n",
      "Epoch[00/01] | Iteration[0727/0782] - Loss value: 0.13247427344322205\n",
      "Epoch[00/01] | Iteration[0728/0782] - Loss value: 0.029654240235686302\n",
      "Epoch[00/01] | Iteration[0729/0782] - Loss value: 0.06717336922883987\n",
      "Epoch[00/01] | Iteration[0730/0782] - Loss value: 0.05127768591046333\n",
      "Epoch[00/01] | Iteration[0731/0782] - Loss value: 0.180055171251297\n",
      "Epoch[00/01] | Iteration[0732/0782] - Loss value: 0.13856106996536255\n",
      "Epoch[00/01] | Iteration[0733/0782] - Loss value: 0.05027023330330849\n",
      "Epoch[00/01] | Iteration[0734/0782] - Loss value: 0.017410289496183395\n",
      "Epoch[00/01] | Iteration[0735/0782] - Loss value: 0.04381856694817543\n",
      "Epoch[00/01] | Iteration[0736/0782] - Loss value: 0.07990128546953201\n",
      "Epoch[00/01] | Iteration[0737/0782] - Loss value: 0.06306606531143188\n",
      "Epoch[00/01] | Iteration[0738/0782] - Loss value: 0.19121293723583221\n",
      "Epoch[00/01] | Iteration[0739/0782] - Loss value: 0.05636140704154968\n",
      "Epoch[00/01] | Iteration[0740/0782] - Loss value: 0.11063390970230103\n",
      "Epoch[00/01] | Iteration[0741/0782] - Loss value: 0.16425947844982147\n",
      "Epoch[00/01] | Iteration[0742/0782] - Loss value: 0.08623979985713959\n",
      "Epoch[00/01] | Iteration[0743/0782] - Loss value: 0.08175422251224518\n",
      "Epoch[00/01] | Iteration[0744/0782] - Loss value: 0.11498063802719116\n",
      "Epoch[00/01] | Iteration[0745/0782] - Loss value: 0.16539528965950012\n",
      "Epoch[00/01] | Iteration[0746/0782] - Loss value: 0.029225368052721024\n",
      "Epoch[00/01] | Iteration[0747/0782] - Loss value: 0.14457009732723236\n",
      "Epoch[00/01] | Iteration[0748/0782] - Loss value: 0.11244814097881317\n",
      "Epoch[00/01] | Iteration[0749/0782] - Loss value: 0.21774888038635254\n",
      "Epoch[00/01] | Iteration[0750/0782] - Loss value: 0.12449108064174652\n",
      "Epoch[00/01] | Iteration[0751/0782] - Loss value: 0.105756014585495\n",
      "Epoch[00/01] | Iteration[0752/0782] - Loss value: 0.07130839675664902\n",
      "Epoch[00/01] | Iteration[0753/0782] - Loss value: 0.15770870447158813\n",
      "Epoch[00/01] | Iteration[0754/0782] - Loss value: 0.14767374098300934\n",
      "Epoch[00/01] | Iteration[0755/0782] - Loss value: 0.12967920303344727\n",
      "Epoch[00/01] | Iteration[0756/0782] - Loss value: 0.20246638357639313\n",
      "Epoch[00/01] | Iteration[0757/0782] - Loss value: 0.20675912499427795\n",
      "Epoch[00/01] | Iteration[0758/0782] - Loss value: 0.10691691190004349\n",
      "Epoch[00/01] | Iteration[0759/0782] - Loss value: 0.10930056124925613\n",
      "Epoch[00/01] | Iteration[0760/0782] - Loss value: 0.09743513911962509\n",
      "Epoch[00/01] | Iteration[0761/0782] - Loss value: 0.10578911006450653\n",
      "Epoch[00/01] | Iteration[0762/0782] - Loss value: 0.05334937945008278\n",
      "Epoch[00/01] | Iteration[0763/0782] - Loss value: 0.11698321998119354\n",
      "Epoch[00/01] | Iteration[0764/0782] - Loss value: 0.13032984733581543\n",
      "Epoch[00/01] | Iteration[0765/0782] - Loss value: 0.10701809078454971\n",
      "Epoch[00/01] | Iteration[0766/0782] - Loss value: 0.14632047712802887\n",
      "Epoch[00/01] | Iteration[0767/0782] - Loss value: 0.07803232967853546\n",
      "Epoch[00/01] | Iteration[0768/0782] - Loss value: 0.04579413682222366\n",
      "Epoch[00/01] | Iteration[0769/0782] - Loss value: 0.05888306349515915\n",
      "Epoch[00/01] | Iteration[0770/0782] - Loss value: 0.11118098348379135\n",
      "Epoch[00/01] | Iteration[0771/0782] - Loss value: 0.1440977305173874\n",
      "Epoch[00/01] | Iteration[0772/0782] - Loss value: 0.25969198346138\n",
      "Epoch[00/01] | Iteration[0773/0782] - Loss value: 0.28415632247924805\n",
      "Epoch[00/01] | Iteration[0774/0782] - Loss value: 0.21893718838691711\n",
      "Epoch[00/01] | Iteration[0775/0782] - Loss value: 0.12255643308162689\n",
      "Epoch[00/01] | Iteration[0776/0782] - Loss value: 0.1016734316945076\n",
      "Epoch[00/01] | Iteration[0777/0782] - Loss value: 0.10804735869169235\n",
      "Epoch[00/01] | Iteration[0778/0782] - Loss value: 0.09683769196271896\n",
      "Epoch[00/01] | Iteration[0779/0782] - Loss value: 0.23322224617004395\n",
      "Epoch[00/01] | Iteration[0780/0782] - Loss value: 0.07895996421575546\n",
      "Epoch[00/01] | Iteration[0781/0782] - Loss value: 0.03498632460832596\n",
      "Epoch[00/01] - Validation accuracy:  87.87%\n"
     ]
    }
   ],
   "source": [
    "n_epochs=1\n",
    "fit_quantised(device, train_loader, valid_loader, pact_network, [pact_controller_linear, pact_controller_act], loss_fn, optimiser, lr_sched, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43467fdc",
   "metadata": {},
   "source": [
    "### Hands-on\n",
    "\n",
    "In this sub-section, you have the possibility of training your own PACT-quantised VGG convolutional network to solve the CIFAR-10 problem.\n",
    "\n",
    "A quick recap of the required steps:\n",
    "1. instantiate a floating-point network using the constructor of the `VGG` class;\n",
    "2. inspect the network's structure wrapping it inside a `LightweightGraph` and calling its method `show_nodes_list`: in this way, you can get a visual feeling of how the information flows through the network, and how you might want to quantise it;\n",
    "3. set the precision of different network nodes by creating a `patches` dictionary mapping node names to the number of quantisation levels that they should use (`n_levels`); we suggest that you select `n_levels` to take on power-of-two values (e.g., three bits amount to $2^{3} = 8$ quantisation levels): in this way;\n",
    "4. use the `all_pact_create_configs` and `all_ana_f2f_recipe` defined in previous cells to convert the original floating-point network to a fake-quantised PACT network (float-to-fake conversion);\n",
    "5. get handles on the PACT fake-quantised nodes and set their training hyper-parameters by instantiating `PACTController`s using the `all_pact_get_controllers` function defined in a previous cell;\n",
    "6. define the loss function, the optimiser (remember that PACT-trained networks require a custom `PACTOptimizer`), and (possibly) a learning rate scheduler;\n",
    "7. train the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3df3f0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd7d33df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def shiftandscale_tensor_01(tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Shift-and-scale an array so that its components fall in the range [0, 1].\"\"\"\n",
    "    if tensor.min() < tensor.max():  # the target 'tensor' is non-constant\n",
    "        tensor = (tensor - tensor.min()) / (tensor.max() - tensor.min())\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def show_2d_tensor(tensor: torch.Tensor) -> None:\n",
    "    \n",
    "    assert tensor.ndim == 2\n",
    "    out_channels, in_channels = tensor.shape\n",
    "    \n",
    "    tensor = tensor.detach().cpu()\n",
    "    tensor = shiftandscale_tensor_01(tensor)\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(16, 12)\n",
    "    ax = plt.imshow(tensor)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_3d_tensor(tensor: torch.Tensor) -> None:\n",
    "    \"\"\"Show a three-dimensional tensor by juxtaposing two-dimensional plots of its slices.\"\"\"\n",
    "    \n",
    "    assert tensor.ndim == 3\n",
    "    n_channels, h, w = tensor.shape\n",
    "    \n",
    "    tensor = tensor.detach().cpu()\n",
    "    tensor = shiftandscale_tensor_01(tensor)\n",
    "\n",
    "    # compute size of image grid (try to keep a square aspect ratio)\n",
    "    m = math.ceil(math.sqrt(n_channels))\n",
    "    n = (n_channels + (m - 1)) // m\n",
    "    \n",
    "    # it looks ugly to have empty 'Axes' objects in a grid of plots: https://matplotlib.org/3.2.2/gallery/lines_bars_and_markers/markevery_demo.html#sphx-glr-gallery-lines-bars-and-markers-markevery-demo-py\n",
    "    def trim_axs(axis_array, N):\n",
    "        \"\"\"Reduce 'axis_array' to 'N' 'Axes' objects. All further 'Axes' are removed from the figure.\"\"\"\n",
    "        axis_array = axis_array.flat\n",
    "        for ax in axis_array[N:]:\n",
    "            ax.remove()\n",
    "        return axis_array[:N]\n",
    "\n",
    "    fig, axis_array = plt.subplots(m, n)\n",
    "    fig.set_size_inches(16, 12)\n",
    "    axis_array = trim_axs(axis_array, n_channels)\n",
    "    for ax, slice_ in zip(axis_array, tensor):\n",
    "        ax.imshow(slice_, vmin=tensor.min(), vmax=tensor.max())  # explicitly setting 'vmin' and 'vmax' implies that colours have the same meaning across different sub-plots\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_tensor(device:   torch.device,\n",
    "                data_set: torch.utils.data.Dataset,\n",
    "                network:  nn.Module,\n",
    "                nodename: str) -> None:\n",
    "    \n",
    "    network.eval()\n",
    "    \n",
    "    name2module = {n.name: n.module for n in qlw.LightweightGraph(network).nodes_list}\n",
    "    assert nodename in name2module.keys()\n",
    "    \n",
    "    nodetype = name2module[nodename].__class__.__name__\n",
    "    node     = name2module[nodename]\n",
    "    \n",
    "    if nodetype in {'Conv2d', 'Linear'} | {'PACTConv2d', 'PACTLinear'}:\n",
    "        \n",
    "        tensor = node.weight_q if nodetype.startswith('PACT') else node.weight\n",
    "\n",
    "        if node.weight.ndim == 2:  # nodetype in {'Linear', 'PACTLinear'}\n",
    "            show_2d_tensor(tensor)\n",
    "            \n",
    "        else:  # nodetype in {'Conv2d', 'PACTConv2d'}\n",
    "            assert node.weight.ndim == 4\n",
    "            out_channels, in_channels, h, w = tensor.shape\n",
    "            show_3d_tensor(tensor[random.randrange(0, out_channels)])\n",
    "            \n",
    "    elif nodetype in {'ReLU'} | {'PACTUnsignedAct'}:\n",
    "        \n",
    "        # PyTorch uses dynamic graphs, so we do not have symbolic names to access feature 'Tensor's directly;\n",
    "        # instead, we need to attach callbacks to 'Module's ('register hook's in PyTorch jargon) which will intercept the required 'Tensor' when computation is triggered\n",
    "\n",
    "        def hook_show_features(self, input_, output):\n",
    "\n",
    "            if output.ndim == 2:\n",
    "                bs, n_channels = output.shape\n",
    "                show_2d_tensor(output[random.randrange(0, bs)].unsqueeze(0))\n",
    "            \n",
    "            else:\n",
    "                assert output.ndim == 4\n",
    "                bs, n_channels, h, w = output.shape\n",
    "                show_3d_tensor(output[random.randrange(0, bs)])\n",
    "            \n",
    "        handle = node.register_forward_hook(hook_show_features)\n",
    "        x, _   = data_set.__getitem__(random.randrange(0, len(data_set)))\n",
    "        x      = x.unsqueeze(0).to(device=device)\n",
    "        y_pr   = network(x)\n",
    "        handle.remove()\n",
    "        \n",
    "    else:\n",
    "        raise ValueError  # neither weights or features for this 'Module' can be visualised\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e3abe28c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x864 with 128 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_tensor(device, valid_data, pact_network, 'features.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbf7862",
   "metadata": {},
   "source": [
    "<a id='sec:fake2true'></a>\n",
    "## Exporting trained QNNs to integerised ONNX IRs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebdcb19",
   "metadata": {},
   "source": [
    "The network that we have trained is not an integerised program yet.\n",
    "In particular, not even its input data points have integer components.\n",
    "Therefore, as a first step along the integerisation pass we need to create `DataLoader`s that emit data points with integer components.\n",
    "\n",
    "As pointed out earlier, the ground truth files containing CIFAR-10 data points encode images using the RGB colour model.\n",
    "Each pixel is a triple of 8-bit bytes, each of which encodes the intensity of a different colour channel.\n",
    "However, `torchvision`'s importer objects scale each value down by a factor of $255$, returning arrays of floating-point numbers in the range $[0, 1]$.\n",
    "Then, we applied the shift-and-scale normalisation procedure.\n",
    "These operations involve integer-to-floating casting operations and floating-point operations that are not necessarily precisely invertible: therefore, retrieving the original integer pixel values from the pre-processed floating-point values is likely to be tedious and imprecise.\n",
    "\n",
    "Therefore, we opted for the following solution: instead of writing our own importer for CIFAR-10 images with integer components and computing analytically the quantum associated with our pre-processing (which would essentially amount to exactly inverting the pre-processing function), we simply add an additional pre-processing step that takes in input the cast-to-floating and shifted-and-scaled pixels, and outputs integerised versions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1e79525b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_range(data_loader: torch.utils.data.DataLoader) -> Tuple[float, float]:\n",
    "    \"\"\"Traverse the available data set to get the empirical range of input pixels.\"\"\"\n",
    "    min_ = 0.0\n",
    "    max_ = 0.0\n",
    "\n",
    "    for x, _ in data_loader:\n",
    "        min_ = min(min_, x.min().item())\n",
    "        max_ = max(max_, x.max().item())\n",
    "    \n",
    "    return min_, max_\n",
    "\n",
    "\n",
    "def add_quantisation_transform(data_loader: torch.utils.data.DataLoader, n_levels: int, min_: float, max_: float) -> float:\n",
    "    \n",
    "    quantiser = qa.pact.PACTAsymmetricAct(n_levels=n_levels, symm=True, learn_clip=False, init_clip='max', act_kind='identity')\n",
    "\n",
    "    clip_lo, clip_hi       = qa.pact.util.almost_symm_quant(torch.Tensor([max(abs(min_), abs(max_))]), n_levels)\n",
    "    quantiser.clip_lo.data = clip_lo\n",
    "    quantiser.clip_hi.data = clip_hi\n",
    "    \n",
    "    quantiser.started |= True\n",
    "    \n",
    "    transform_list  = []\n",
    "    transform_list += [data_loader.dataset.transform]\n",
    "    transform_list += [quantiser]\n",
    "    transform_list += [transforms.Lambda(lambda x: x / quantiser.get_eps())]\n",
    "    \n",
    "    data_loader.dataset.transform = transforms.Compose(transform_list)\n",
    "    \n",
    "    return quantiser.get_eps()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d338fb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input_levels = 2**8\n",
    "min_, max_     = get_input_range(valid_loader)\n",
    "\n",
    "input_eps = add_quantisation_transform(valid_loader, n_input_levels, min_, max_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959850ce",
   "metadata": {},
   "source": [
    "The components of the data points produce by the updated `DataLoader` are 8-bit signed integers (i.e., they are of the `INT8` data type).\n",
    "\n",
    "We can now proceed to the F2T conversion step.\n",
    "F2T conversion involves the application of elementary arithmetic properties (distributive, associative) and clever approximations; it also involves potentially complex pattern matching and graph rewriting rules.\n",
    "In `quantlib`, the functionalities to perform F2T conversions are implemented in two sub-packages:\n",
    "* `quantlib.editing.graphs`; it is based on PyTorch's graph tracing mechanism and on the Python NetworkX package for graph analysis and manipulation; this is the legacy method: it allows to define custom graph rewriting rules, but to master it the user must be familiar with pattern matching on graphs;\n",
    "* `quantlib.editing.fx`; in is based on PyTorch's `fx` package, which is dedicated to the manipulation of computational graphs; this is the new method: it allows to define pattern matching rules in a transparent way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f4dfddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import quantlib.editing.fx as qfx\n",
    "\n",
    "\n",
    "def f2t_convert(dataloader: torch.utils.data.DataLoader,\n",
    "                input_eps:  float,\n",
    "                network:    nn.Module) -> nn.Module:\n",
    "\n",
    "    network.eval()\n",
    "    network = network.to(device=torch.device('cpu'))\n",
    "    \n",
    "    x, _ = dataloader.dataset.__getitem__(0)\n",
    "    x    = x.unsqueeze(0)\n",
    "    \n",
    "    fake2true_converter = qfx.passes.pact.IntegerizePACTNetPass(shape_in=x.shape, eps_in=input_eps, D=2**19)\n",
    "    \n",
    "    return fake2true_converter(pact_network)\n",
    "\n",
    "#export_net(tq_pact_network,\n",
    "#           name='VGG_PACT',\n",
    "#           out_dir='ONNX', eps_in=CIFAR10_STATS_QUANTISE['eps'], integerize=False, D=2**19, in_data=dummy_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4ea66321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key <class 'torch.nn.modules.batchnorm.BatchNorm2d'> not found in _EPS_CONVERSIONS!\n",
      "Using identity epsilon propagation on node with op call_module, target pilot.1!\n",
      "key <class 'torch.nn.modules.batchnorm.BatchNorm2d'> not found in _EPS_CONVERSIONS!\n",
      "Using identity epsilon propagation on node with op call_module, target features.1!\n",
      "key <class 'torch.nn.modules.pooling.MaxPool2d'> not found in _EPS_CONVERSIONS!\n",
      "Using identity epsilon propagation on node with op call_module, target features.3!\n",
      "key <class 'torch.nn.modules.batchnorm.BatchNorm2d'> not found in _EPS_CONVERSIONS!\n",
      "Using identity epsilon propagation on node with op call_module, target features.5!\n",
      "key <class 'torch.nn.modules.batchnorm.BatchNorm2d'> not found in _EPS_CONVERSIONS!\n",
      "Using identity epsilon propagation on node with op call_module, target features.8!\n",
      "key <class 'torch.nn.modules.pooling.MaxPool2d'> not found in _EPS_CONVERSIONS!\n",
      "Using identity epsilon propagation on node with op call_module, target features.10!\n",
      "key <class 'torch.nn.modules.batchnorm.BatchNorm2d'> not found in _EPS_CONVERSIONS!\n",
      "Using identity epsilon propagation on node with op call_module, target features.12!\n",
      "key <class 'torch.nn.modules.batchnorm.BatchNorm2d'> not found in _EPS_CONVERSIONS!\n",
      "Using identity epsilon propagation on node with op call_module, target features.15!\n",
      "key <class 'torch.nn.modules.pooling.MaxPool2d'> not found in _EPS_CONVERSIONS!\n",
      "Using identity epsilon propagation on node with op call_module, target features.17!\n",
      "key <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'> not found in _EPS_CONVERSIONS!\n",
      "Using identity epsilon propagation on node with op call_module, target avgpool!\n",
      "key _CALL_METHOD_size not found in _EPS_CONVERSIONS!\n",
      "Using identity epsilon propagation on node with op call_method, target size!\n",
      "key _CALL_METHOD_view not found in _EPS_CONVERSIONS!\n",
      "Using identity epsilon propagation on node with op call_method, target view!\n",
      "key <class 'torch.nn.modules.batchnorm.BatchNorm1d'> not found in _EPS_CONVERSIONS!\n",
      "Using identity epsilon propagation on node with op call_module, target classifier.1!\n",
      "key <class 'torch.nn.modules.batchnorm.BatchNorm1d'> not found in _EPS_CONVERSIONS!\n",
      "Using identity epsilon propagation on node with op call_module, target classifier.4!\n",
      "key _OUTPUT_output not found in _EPS_CONVERSIONS!\n",
      "Using identity epsilon propagation on node with op output, target output!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spmatteo/anaconda3/envs/quantlab/lib/python3.8/site-packages/torch/fx/graph.py:606: UserWarning: Attempted to insert a call_module Node with no underlying reference in the owning GraphModule! Call GraphModule.add_submodule to add the necessary submodule\n",
      "  warnings.warn(\"Attempted to insert a call_module Node with \"\n"
     ]
    }
   ],
   "source": [
    "tq_pact_network = f2t_convert(valid_loader, input_eps, pact_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b16ad80",
   "metadata": {},
   "source": [
    "A sanity check: we compare the predictions of the fully-integerised program with the ground truth to verify that they are coherent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1440b9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True: Ship - Predicted: Ship\n"
     ]
    }
   ],
   "source": [
    "x, y_gt_int = valid_loader.dataset.__getitem__(random.randrange(0, len(valid_loader.dataset)))\n",
    "\n",
    "x = x.unsqueeze(0)\n",
    "y_pr = tq_pact_network(x)\n",
    "y_pr_int = y_pr.argmax(axis=1)\n",
    "\n",
    "print(\"True: {} - Predicted: {}\".format(int2label[y_gt_int], int2label[y_pr_int.item()]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3965e607",
   "metadata": {},
   "source": [
    "Once we have performed F2T conversion, we can export the resulting PyTorch `Module` to the ONNX format.\n",
    "Usually, the ONNX file output by `quantlib` will be taken in input by a platform-specific tool that will perform the deployment, a stage that includes:\n",
    "* platform-specific graph optimisations;\n",
    "* platform-specific code generation.\n",
    "\n",
    "To fulfill this job in an automated and effective way, the backend tool will require the ONNX to be annotated with details such as the precision (in bits) of each parameter and feature array.\n",
    "To this end, `quantlib` includes several backend-specific packages implementing export functions that can format the ONNX of the integerised network to make it a valid input for the platform-specific deployment tool.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a0d4a238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import quantlib.backends as qb\n",
    "\n",
    "\n",
    "def create_vgg_dir_export(dir_logs: os.PathLike) -> os.PathLike:\n",
    "    dir_export = os.path.join(dir_logs, 'onnx_exports')\n",
    "    if not os.path.isdir(dir_export):\n",
    "        os.makedirs(dir_export, exist_ok=True)\n",
    "    return dir_export\n",
    "\n",
    "def export_network(data_loader: torch.utils.data.DataLoader,\n",
    "                   input_eps:   float,\n",
    "                   network:     nn.Module,\n",
    "                   filename:    str,\n",
    "                   dir_export:  os.PathLike) -> None:\n",
    "    \n",
    "    network.eval()\n",
    "    \n",
    "    x, _ = data_loader.dataset.__getitem__(random.randrange(0, len(data_loader.dataset)))\n",
    "    x = x.unsqueeze(0)\n",
    "    \n",
    "    qb.dory.export_net(network,\n",
    "                       name=filename,\n",
    "                       out_dir=dir_export,\n",
    "                       eps_in=input_eps,\n",
    "                       integerize=False,\n",
    "                       D=2**19,\n",
    "                       in_data=x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d37038d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not permuting output of layer output...\n",
      "Not permuting output of layer classifier._QL_REPLACED__INTEGERIZE_BN1D_UNSIGNED_ACT_PASS_0...\n",
      "Not permuting output of layer classifier._QL_REPLACED__INTEGERIZE_BN1D_UNSIGNED_ACT_PASS_1...\n"
     ]
    }
   ],
   "source": [
    "dir_export = create_vgg_dir_export(dir_logs)\n",
    "export_network(valid_loader, input_eps, tq_pact_network, 'VGG_PACT_DORY.onnx', dir_export)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f9169e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
